<h1 id="目录">目录</h1>

- [1.指令微调数据制作发展路线](#1.指令微调数据制作发展路线)
- [2.指令微调数据构造有哪些指导原则？](#2.指令微调数据构造有哪些指导原则？)
- [3.简要介绍一下KV-Cache](#3.简要介绍一下KV-Cache)
- [4.简要介绍一下FLOPs](#4.简要介绍一下FLOPs)


<h1 id='1.指令微调数据制作发展路线'>1.指令微调数据制作发展路线</h1>

1. **Scaling law**：在指令微调数据较为匮乏的时期，收集更多的数据是提升性能的大力出奇迹办法。
2. **人工和启发式的数据多样性**：在数据量积累到一定规模后，数据混合配比成为新的研究话题。一些研究成果验证了合适的数据配比可以提升性能，但数据配比没有通用的万能钥匙。
3. **基于模型的多样性**：随着LLMs/MLLMs，可以让它们参与到数据生产和筛选流程中，例如用GPT3.5/4/4V生产数据，用其它LLMs作为数据质量筛选器。（GPT4/GPT4V为指令微调领域贡献了太多数据，这可能也是一种OpenAI吧）
4. **数据效率**：有了LLMs/MLLMs的加持，数据量似乎已经不成大问题。因此高质量数据的多样性、难度和复杂程度成为了关注焦点。满足上述要求的数据意味着用高质量的响应近似真实用户提示，LIMA论证了只要数据质量足够高，数据量会是次要因素。因此，需要自动化或半自动方案对数据进行过滤：
    1. 基于自然语言规则过滤；
    2. 用InsTag对指令微调数据打语义或意图的标签，从而做聚类分析；
    3. 利用GPT4等语言模型过滤噪声数据；
    4. 利用模型的loss等反馈数据对模型的影响，例如评估模型对指令的不确定性（Active Instruction Tuning）；
5. **数据治理、责任和其他问题**：开始关注数据商业条款、许可，版权等问题。


<h1 id='2.指令微调数据构造有哪些指导原则？'>2.指令微调数据构造有哪些指导原则？</h1>



<h1 id='3.简要介绍一下KV-Cache'>3.简要介绍一下KV-Cache</h1>


对于单个样本来说，生成式模型是next token prediction，随着序列变长，next token预测成本越来越高，FLOPs越来越大。但实际上它们重复计算了很多previous tokens。

KV-Cache的作用就是将计算过的token缓存起来不再重复计算。

假设没有KV-Cache，则next token prediction遵循如下伪代码。
```python
EOS_token = torch.tensor([198])
cur_tokens = torch.tensor(tokenizer.encode("WeThinkIn is"))
next_token = None
with torch.no_grad():
    while next_token != EOS_token:
        # cur_tokens会包含越来越多的重复计算
        logits, _ = model(cur_tokens)
        next_token = torch.argmax(logits[-1, :], dim=0, keepdim=True)
        # 每次得到next_token后需要和cur_tokens拼接
        cur_tokens = torch.cat((cur_tokens, next_token), 0)
```

```python
EOS_token = torch.tensor([198])
cur_tokens = torch.tensor(tokenizer.encode("WeThinkIn is"))
next_token = None
kv_cache = None
with torch.no_grad():
    while next_token != EOS_token:
        # 通过past_key_values实现
        logits, kv_cache = model(cur_tokens, past_key_values=kv_cache)
        next_token = torch.argmax(logits[-1, :], dim=0, keepdim=True)
        # 不再需要concate，因为需要重复计算的部分会不断增量缓存到kv_cache中，以空间换时间。
        cur_tokens = next_tokens
```

如果一个mini-batch内的样本共享相同的meta/system prompt或图像，则可以先统一做一次预填充，再通过past_key_value参数传入generate的方式实现不同样本间的KV-Cache。