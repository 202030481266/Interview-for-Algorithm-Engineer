# 目录

- [1.目前主流的AI视频技术框架有哪几种？](#1.目前主流的AI视频技术框架有哪几种？)
- [2.目前主流的AI视频大模型有哪些？](#2.目前主流的AI视频大模型有哪些？)
- [3.Sora有哪些创新点？](#3.Sora有哪些创新点？)
- [4.SVD（Stable-Video-Diffusion）有哪些创新点？](#4.SVD（Stable-Video-Diffusion）有哪些创新点？)
- [5.AIGC时代的主流AI视频生成流程有哪些？](#5.AIGC时代的主流AI视频生成流程有哪些？)
- [6.Sora在训练时是如何处理输入数据的？](#6.Sora在训练时是如何处理输入数据的？)
- [7.Sora是如何对视频数据进行标注的？](#7.Sora是如何对视频数据进行标注的？)
- [8.为什么说Sora等AI视频大模型具备世界模拟器（world simulator）的潜质？](#8.为什么说Sora等AI视频大模型具备世界模拟器（world-simulator）的潜质？)


<h2 id="1.目前主流的AI视频技术框架有哪几种？">1.目前主流的AI视频技术框架有哪几种？</h2>

Rocky梳理总结了AIGC时代到目前为止主流的AI视频技术框架，市面上的所有AI视频产品基本上都是基于以下这些框架：
1. 文本生成视频：输入文本，先生成图片或者直接生成视频。主要流程包括工作流前处理+扩散模型+运动模块+条件控制+工作流后处理。
2. 图像生成视频：输入图像，先生成前后帧图像，然后使用插帧与语义扩展持续生成前后序列帧图像，最后生成完整视频。主要流程包括工作流前处理+扩散模型+运动模块+条件控制+工作流后处理。
3. 视频生成视频：输入视频，提取关键帧，对关键帧进行转绘，然后再进行插帧，从而生成新的视频。主要流程包括工作流前处理+扩散模型+运动模块+条件控制+工作流后处理。


<h2 id="2.目前主流的AI视频大模型有哪些？">2.目前主流的AI视频大模型有哪些？</h2>

Rocky为大家梳理总结了AIGC时代到目前为主的主流AI视频大模型，如下所示：

1. Stable Video Diffusion（SVD）系列
2. Sora
3. 可灵AI
4. LUMA
5. Gen系列
6. Stable Diffusion系列 + Animatediff


<h2 id="3.Sora有哪些创新点？">3.Sora有哪些创新点？</h2>

OpenAI对Sora的定位不只是视频生成工具，而是希望在此基础上开发出能够让计算机理解真实世界的算法与技术——“作为世界模拟器的视频生成模型”。在这个宏大愿景下最具潜力的技术基底之一便是生成式模型 (generative model)。

下面是Sora的一些创新点：
1. 海量的数据：在Sora的技术报告中，关于数据量级是一句话都没有提。这就说明，Sora使用了海量的高质量视频数据用作训练，Rocky相信未来全互联网的视频数据都会被Sora用作训练，同时在视频数据领域的数据生成、数据增强将会有非常大的机会。
2. 灵活编码：在Sora中，借鉴了大语言模型的构建方式，使用video compression network（convolutional VAEs）将视频数据tokenizer化，获得visual patches，使得任何长度和内容的视频都能编码成AI视频模型可以直接处理（输入/输出）的embeddings。首先video compression network将输入视频的时间和空间两个维度同时进行压缩，编码成一个和视频大小成正比的3D visual patch矩阵，然后再将其展开成1D array of patches Embeddings，送入到后续的DiT model中。这样可以带来两个好处，分别是让Sora能够生成不同分辨率的视频分和生成的视频的边框更加合理。
3. DiT模型架构：Sora使用了DiT（Diffusion Transformer）作为核心架构，这让Transformer在AI领域的大一统更进一步。
4. 精细化数据标注：和DALL-E 3一样，OpenAI用内部标注工具（可能是GPT4-4o等）给视频数据进行详尽的描述标注，从而提升Sora模型生成视频与输入prompt的一致性、生成视频的质量和视频中正确显示文本的能力。Rocky认为数据工程是非常关键的一点，无论是传统深度学习时代还是AIGC时代，都是AI领域的杀手锏。
5. 让AI视频领域的Scaling Law成立：保证模型越大，数据越多，效果就越好。Sora也不例外。一句话概括Sora的贡献，便是：在足量的数据，优质的标注，灵活的编码下，scaling law 在 transformer + diffusion model 的架构上继续成立。


<h2 id="4.SVD（Stable-Video-Diffusion）有哪些创新点？">4.SVD（Stable-Video-Diffusion）有哪些创新点？</h2>

Rocky认为SVD（Stable Video Diffusion）模型非常有价值，其开源精神让我们动容，下面是SVD模型的主要创新点：
1. 基于Stable Diffusion 2.1模型架构
2. 海量数据集：StabilityAI使用了一个包含5.8亿个视频剪辑的巨大数据集，来训练SVD模型。为了筛选高质量数据，首先需要检测每个视频中的不同镜头和转场，并且评估每个镜头中的运动信息，然后为每个镜头自动生成描述文字和每个镜头的美学效果等。
3. 数据精细化处理：（1）级联切换检测：采用级联的切换检测方法识别视频中的场景转场。（2）运动信息提取:基于稠密光流估计每个视频片段的运动信息。（3）文本描述生成:为每个视频片段自动生成三种形式的文字描述。（4）质量评估:使用CLIP等方法评估每个片段的视觉质量、文本匹配度等。（5）过滤去噪:根据上述评估指标过滤掉质量较差的视频片段。经过层层筛选，最后保留了一个约1.5亿视频片段的超高质量数据集，为后续的SVD模型训练奠定重要基础。
4. 多阶段训练：SVD模型在模型训练方面也与传统方法不同，其采用了一个三层训练架构。第一阶段是进行图像预训练，初始化一个图像生成模型。第二阶段是在已经构建的大规模视频数据集上进行视频预训练，学习运动表征。第三阶段是在一个小规模的高质量视频数据集上进行微调。这种分阶段的训练策略可以让模型更好地生成高保真视频。


<h2 id="5.AIGC时代的主流AI视频生成流程有哪些？">5.AIGC时代的主流AI视频生成流程有哪些？</h2>

Rocky总结了如下图所示的AIGC时代主流AI视频生成流程，可以作为大家构建AI视频产品构架的基础底座：

![AIGC时代的主流AI视频生成流程](./imgs/AIGC时代的主流AI视频生成流程.jpg)


<h2 id="6.Sora在训练时是如何处理输入数据的？">6.Sora在训练时是如何处理输入数据的？</h2>

**Sora在处理输入数据的过程中引入了大语言模型标配的Tokenizer思想。**

在文本对话领域，Tokenizer可使任何长度和内容的文本编码成大语言模型可以直接处理（输入/输出）的Text Embeddings特征。在AI视频领域则是将视频数据进行编码获得**visual patches**，下图展示了Sore将输入视频转换成visual patches的过程：

![Sora对输入的视频数据进行编码](./imgs/Sora对输入的视频数据进行编码.png)

**其中先使用一个Visual Encoder模型将视频数据（空间和时间维度）压缩编码到Latent特征空间，获得一个3D visual patch array，接着将整个Latent特征分解成spacetime patches，最后再排列组合成为一个visual patches向量。**

为了能有一个高质量的视频数据压缩编码效果，OpenAI针对性训练了一个Video compression network作为Visual Encoder模型。同时也训练了一个Visual Decoder模型用于Sora的解码来获取生成的视频结果。

Sora通过对输入数据的压缩编码，**为AI视频的生成带来了很多帮助：**

1. **训练的数据分辨率获得了解放：** Sora能够训练任意的分辨率、时间长度和长宽比的视频/图像数据。
2. **灵活的生成分辨率：** Sora可以生成1920x1080像素（横屏）到1080x1920像素（竖屏）之间任意分辨率的视频。
3. **生成视频的边缘更加符合真实常理：** Sora尝试过固定分辨率进行训练，这种情况下就需要裁剪视频数据。这样数据的裁剪bias会被带入到模型中，导致Sora模型生成很多主要内容缺失的视频。


<h2 id="7.Sora是如何对视频数据进行标注的？">7.Sora是如何对视频数据进行标注的？</h2>

和AI绘画领域的DALL-E 3一样，**OpenAI使用内部的数据标注模型（大概率是GPT4-v等）给视频数据进行了详细的描述（Caption）**，从而提升了Sora模型生成的视频与输入Prompt的一致性，同时生成视频的质量与视频中文本的渲染能力也有一定的提升。

Rocky认为视频数据的精细化标注非常关键，是AI视频模型生成效果的关键一招，**因为视频生成的动态性非常高，是一个非常大的数据分布**。

与此同时，虽然在训练中使用详细的描述（Caption）作为数据标签能够增强AI视频模型的文本一致性能力，但是在用户使用时可能会出现bias，因为用户输入的描述一般都相对较短。**这里Sora和DALL-E 3一样，也使用GPT-4来对用户的输入描述进行扩充完善来解决这个问题，同时提高了用户的使用体验和视频生成的多样性**。


<h2 id="8.为什么说Sora等AI视频大模型具备世界模拟器（world-simulator）的潜质？">8.为什么说Sora等AI视频大模型具备世界模拟器（world simulator）的潜质？</h2>

Sora能够根据文本提示词生成逼真或富有创意的场景视频，展现出强大的模拟物理世界的潜力。

**OpenAI一直以来有一个宏大愿景：开发出能够让计算机理解我们物理世界的AI算法技术，目前最有可能的技术之一就是AIGC模型。**

也正因此，Sora的技术报告中对Sora的定位正是“作为世界模拟器的视频生成模型”。

当前世界的物理信息与数字信息都呈爆发式增长，如此庞大的信息量，**包括图像、视频、文本、语音、3D等多种模态**，**我们需要为AIGC时代之后即将到来的元宇宙初级阶段与AGI初级阶段构建一个AI模型基底**，来理解上述的多模态信息，能够有逻辑的分析表达这些信息。

因此Rocky也认为，AIGC模型是朝向这个目标迈进的最有希望的技术之一。正如理查德·费曼的一句名言：

> 我所无法创造的，我也不能理解。

**这句话的潜台词就是要真正理解一个事物，我们就需要去创造它。**

这正与AIGC时代的核心思想耦合，我们要训练一个AIGC模型，我们首先要构建海量的数据集库，然后配置充足算力去训练AIGC模型去创造生成无穷无尽的内容。

Sora等AI视频大模型是能够理解和模拟现实世界的模型基础，是AI技术向前迈进的重要一步，2024年也将成为AIGC时代早期的一个重要时间节点。

总的来说，**Rocky认同OpenAI乃至AI行业的这个愿景与观点，未来的元宇宙时代和AGI时代，以AI视频大模型为基座的多模态AI会给我们创造出无穷无尽的价值**。
