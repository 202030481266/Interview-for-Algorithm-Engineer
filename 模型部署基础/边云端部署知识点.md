# 目录

- [1.X86和ARM架构在深度学习侧的区别？](#user-content-1.x86和arm架构在深度学习侧的区别？)
- [2.为何在AI端侧设备一般不使用传统图像算法？](#user-content-2.为何在ai端侧设备一般不使用传统图像算法？)
- [3.端侧部署时整个解决方案的核心指标？](#user-content-3.端侧部署时整个解决方案的核心指标？)
- [4.主流AI端侧硬件平台有哪些？](#user-content-4.主流ai端侧硬件平台有哪些？)
- [5.主流AI端侧硬件平台一般包含哪些模块？](#user-content-5.主流ai端侧硬件平台一般包含哪些模块？)
- [6.算法工程师该如何看待硬件侧知识？](#user-content-6.算法工程师该如何看待硬件侧知识？)
- [7.优化模型端侧性能的一些方法](#user-content-7.优化模型端侧性能的一些方法)
- [8.目前主流的端侧算力芯片有哪些种类？](#user-content-8.目前主流的端侧算力芯片有哪些种类？)


<h2 id="1.x86和arm架构在深度学习侧的区别？">1.X86和ARM架构在深度学习侧的区别？</h2>
  
AI服务器与PC端一般都是使用X86架构，因为其<font color=DeepSkyBlue>高性能</font>；AI端侧设备（手机/端侧盒子等）一般使用ARM架构，因为需要<font color=DeepSkyBlue>低功耗</font>。

X86指令集中的指令是复杂的，一条很长指令就可以很多功能；而ARM指令集的指令是很精简的，需要几条精简的短指令完成很多功能。

X86的方向是高性能方向，因为它追求一条指令完成很多功能；而ARM的方向是面向低功耗，要求指令尽可能精简。


<h2 id="2.为何在ai端侧设备一般不使用传统图像算法？">2.为何在AI端侧设备一般不使用传统图像算法？</h2>
  
AI端侧设备多聚焦于深度学习算法模型的加速与赋能，而传统图像算法在没有加速算子赋能的情况下，在AI端侧设备无法发挥最优的性能。


<h2 id="3.端侧部署时整个解决方案的核心指标？">3.端侧部署时整个解决方案的核心指标？</h2>

1. 精度
2. 耗时
3. 内存占用
4. 功耗


<h2 id="4.主流ai端侧硬件平台有哪些？">4.主流AI端侧硬件平台有哪些？</h2>

1. 英伟达
2. 海思
3. 寒武纪
4. 比特大陆
5. 昇腾
6. 登临
7. 联咏
8. 安霸
9. 耐能
10. 爱芯
11. 瑞芯


<h2 id="5.主流ai端侧硬件平台一般包含哪些模块？">5.主流AI端侧硬件平台一般包含哪些模块？</h2>

1. 视频编解码模块
2. CPU核心处理模块
3. AI协处理器模块
4. GPU模块
5. DSP模块
6. DDR内存模块
7. 数字图像处理模块


<h2 id="6.算法工程师该如何看待硬件侧知识？">6.算法工程师该如何看待硬件侧知识？</h2>

GPU乃至硬件侧的整体逻辑，是CV算法工作中必不可少的组成部分，也是算法模型所依赖的重要物理载体。

<h3 id="gpu的相关知识">GPU的相关知识</h3>

现在AI行业有个共识，认为是数据的爆发和算力的突破开启了深度学习在计算机视觉领域的“乘风破浪”，而其中的算力，主要就是指以GPU为首的计算平台。

GPU（Graphical Processing Unit）从最初用来进行图形处理和渲染（玩游戏），到通过CUDA/OpenCL库以及相应的工程开发之后，成为深度学习模型在学术界和工业界的底层计算工具，其有以下的一些特征：

1. 异构计算：GPU能作为CPU的协处理器与CPU协同运算。
2. 单指令流多数据流（SIMD）架构：使得同一个指令（比如对图像数据的一些操作），可以同时在多个像素点上<font color=DeepSkyBlue>并行计算</font>，从而得到比较大的吞吐量，深度学习中大量的矩阵操作，让GPU成为一个非常适合的计算平台。
3. 多计算核心。比如Nvidia的GTX980GPU中，在和i7-5960CPU差不多的芯片面积上，有其128倍的运算速度。GTX980中有16个流处理单元，每个流处理单元中包含着128个CUDA计算核心，共有2048个GPU运算单元，与此同时i7-5960CPU只有16个类似的计算单元。
4. CUDA模块。作为GPU架构中的最小单元，它的设计和CPU有着非常类似的结构，其中包括了一个浮点运算单元，整型运算单元以及控制单元。一个流处理单元中的CUDA模块将执行同一个指令，但是会作用在不同的数据上。多CUDA模块意味着GPU有更加高的计算性能，但<font color=DeepSkyBlue>更重要的是在算法侧有没有高效地调度和使用</font>。
5. 计算核心频率。即时钟频率，代表每一秒内能进行同步脉冲次数。就核心频率而言，CPU要高于GPU。由于GPU采用了多核逻辑，即使提高一些频率，其实对于总体性能影响不会特别大。
6. 内存架构。GPU的多层内存架构包括全局内存，2级缓存，和芯片上的存储（包括寄存器，和1级缓存共用的共享内存，只读/纹理缓存和常量缓存）。

![](https://files.mdnice.com/user/33499/24cbb3b8-c530-4eec-a3f5-119b7a8c7ea6.png)

在使用GPU时，在命令行输入nvidia-smi命令时会打印出一张表格，其中包含了GPU当时状态的所有参数信息。

![](https://files.mdnice.com/user/33499/9772a00f-8006-4d93-ac17-13ca84043a3d.png)

CUDA/cuDNN/OpenCL科普小知识：

1. CUDA是NVIDIA推出的用于GPU的并行计算框架。
2. cuDNN是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。
3. OpenCL是由苹果（Apple）公司发起，业界众多著名厂商共同制作的面向异构系统通用目的并行编程的开放式、免费标准，也是一个统一的编程环境。

<h3 id="深度学习的端侧设备">深度学习的端侧设备</h3>

深度学习的端侧设备，又可以叫做边缘计算设备，深度学习特别是CV领域中，<font color=DeepSkyBlue>模型+端侧设备的组合能够加快业务的即时计算，决策和反馈能力，极大释放AI可能性</font>。

![](https://files.mdnice.com/user/33499/6863aa5d-a89b-4b08-a588-5b393cdd6191.png)

深度学习的端侧设备主要由ARM架构的CPU+ GPU/TPU/NPU等协处理器 + 整体功耗 + 外围接口 + 工具链等部分组成，也是算法侧对端侧设备进行选型要考虑的维度。

在实际业务中，根据公司的不同，算法工程师涉及到的硬件侧范围也会不一样。如果公司里硬件和算法由两个部门分别负责，那么算法工程师最多接触到的硬件侧知识就是<font color=DeepSkyBlue>硬件性能评估，模型转换与模型硬件侧验证，一些硬件高层API接口的开发与使用</font>；如果公司里没有这么细分的部门，那么算法工程师可能就会接触到端侧的视频编解码，模型推理加速，Opencv，FFmpeg，Tensor RT，工具链开发等角度的知识。

![](https://files.mdnice.com/user/33499/580db620-75b6-4254-a7c0-4200b9d32c62.png)

<h3 id="算法工程师该如何看待硬件侧">算法工程师该如何看待硬件侧</h3>

首先，整体上还是要将<font color=DeepSkyBlue>硬件侧工具化，把端侧设备当做算法模型的一个下游载体，会熟练的选型与性能评估更加重要</font>。

端侧设备是算法产品整体解决方案中一个非常重要的模块，<font color=DeepSkyBlue>算法+硬件</font>的范式将在未来的边缘计算与万物智能场景中持续发力。

在日常业务中，<font color=DeepSkyBlue>算法模型与端侧设备的适配性与兼容性</font>是必须要考虑的问题，端侧设备是否兼容一些特殊的网络结构？算法模型转化并部署后，精度是否下降？功耗与耗时能否达标？等等都让算法工程师的模型设计逻辑有更多的抓手。


<h2 id="7.优化模型端侧性能的一些方法">7.优化模型端侧性能的一些方法</h2>

1. 设计能最大限度挖掘AI协处理器性能的模型结构。
2. 多模型共享计算内存。
3. 减少模型分支结构，减少模型元素级操作。
4. 卷积层的输入和输出特征通道数相等时MAC最小，以提升模型Inference速度。


<h2 id="8.目前主流的端侧算力芯片有哪些种类？">8.目前主流的端侧算力芯片有哪些种类？</h2>

AI端侧算力设备（如NPU、TPU、VPU、FPGA等）目前正在快速发展，这些设备专门设计用于加速AIGC、传统深度学习、自动驾驶等领域任务。它们在性能和效率方面大大超过了传统的CPU和GPU。以下是Rocky对这些AI端侧算力的详细介绍：

### 1. NPU（Neural Processing Unit）
NPU，即神经处理单元，是一种专门用于加速神经网络计算的处理器。NPU通常集成在移动设备、物联网设备和其他嵌入式系统中，以提升AI应用的性能。

#### 特点与优势：
- **高效能耗比**：NPU在进行神经网络计算时具有高能效，适用于资源受限的设备。
- **专用硬件设计**：为了优化矩阵运算和卷积操作，NPU设计了专门的硬件加速器。
- **实时处理**：NPU能实现低延迟的实时AI推理，非常适合智能手机、摄像头等需要实时处理的设备。
- **集成性强**：NPU常与其他处理单元（如CPU、GPU）集成在同一个芯片上（如SoC），以提供全面的计算能力。

### 2. TPU（Tensor Processing Unit）
TPU，即张量处理单元，是Google开发的一种专用AI加速器，主要用于加速TensorFlow框架下的机器学习任务。

#### 特点与优势：
- **高性能**：TPU能够提供极高的计算能力，特别是在处理大规模矩阵运算和深度学习模型训练时。
- **定制化设计**：TPU为特定的AI工作负载（如矩阵乘法、卷积运算）进行了优化，显著提升了性能。
- **大规模部署**：TPU被广泛部署在Google的数据中心，用于支持Google的各项AI服务，如搜索、广告、翻译等。

#### 版本和架构：
- **TPU v1**：主要用于推理任务，每秒可执行92万亿次浮点运算（92 TFLOPS）。
- **TPU v2**和**TPU v3**：增强了训练能力，分别具有每秒180 TFLOPS和420 TFLOPS的计算能力。
- **TPU v4**：最新版本，进一步提升了性能和能效，适用于更大规模、更复杂的AI任务。


### 3. VPU（Vision Processing Unit）
VPU，即视觉处理单元，是一种专门设计用于计算机视觉和人工智能任务的处理器。VPUs的主要目标是以高效的能耗比处理复杂的视觉计算任务，适用于各种嵌入式和边缘设备。

####  特点与优势：
  - VPU专注于低功耗的计算机视觉任务，适用于嵌入式系统和边缘设备。
  - 提供高效的图像处理和神经网络推理能力。

### 4. FPGA（Field Programmable Gate Array）
FPGA，即现场可编程门阵列，是一种高度可编程的集成电路，可以根据特定应用的需求重新配置其硬件电路。FPGA在AI和机器学习中广泛应用于需要高灵活性和低延迟的任务。

####  特点与优势：
  - FPGA具有高度灵活性，可根据需求重新配置电路结构。
  - 提供较低的延迟和高效的能耗比，适用于特定AI任务的加速。
  - 能够实现高度并行计算，适用于实时处理应用。

### 5. Huawei Ascend
华为昇腾系列AI处理器包括适用于云端和边缘计算的多种型号，提供高性能的AI计算能力。
#### 特点与优势：
  - 华为的昇腾系列AI芯片，包括适用于云端和边缘计算的不同版本，如Ascend 910（高性能）和Ascend 310（边缘计算）。
  - 提供高度集成的AI计算能力，支持多种AI框架和模型。

### 6. Graphcore IPU（Intelligence Processing Unit）
Graphcore IPU是一种专门设计用于机器智能任务的处理器，采用全新的计算架构，优化了计算和内存访问。
#### 特点与优势：
  - IPU专为机器智能任务设计，采用了全新的计算架构，优化了计算和内存访问。
  - 能够高效处理稀疏计算和动态计算图，适用于复杂的AI模型。

### 总结
这些AI端侧设备显著提升了AIGC、传统深度学习、自动驾驶任务的性能和能效，推动了AI技术的快速发展和应用扩展。不同的加速器在设计上各有侧重，适用于不同的应用场景，满足了多样化的AI计算需求。