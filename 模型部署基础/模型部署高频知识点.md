# 目录
- [1.模型压缩的必要性与可行性？](#user-content-1.模型压缩的必要性与可行性？)
- [2.X86和ARM架构在深度学习侧的区别？](#user-content-2.x86和arm架构在深度学习侧的区别？)
- [3.FP32，FP16以及Int8的区别？](#user-content-3.fp32，fp16以及int8的区别？)
- [4.GPU显存占用和GPU利用率的定义](#user-content-4.gpu显存占用和gpu利用率的定义)
- [5.神经网络的显存占用分析](#user-content-5.神经网络的显存占用分析)
- [6.算法模型部署逻辑？](#user-content-6.算法模型部署逻辑？)
- [7.影响模型inference速度的因素？](#user-content-7.影响模型inference速度的因素？)
- [8.为何在AI端侧设备一般不使用传统图像算法？](#user-content-8.为何在ai端侧设备一般不使用传统图像算法？)
- [9.减小模型内存占用有哪些办法？](#user-content-9.减小模型内存占用有哪些办法？)
- [10.有哪些经典的轻量化网络？](#user-content-10.有哪些经典的轻量化网络？)
- [11.模型参数计算？](#user-content-11.模型参数计算？)
- [12.模型FLOPs怎么算？](#user-content-12.模型flops怎么算？)
- [13.什么是异构计算？](#user-content-13.什么是异构计算？)
- [14.端侧部署时整个解决方案的核心指标？](#user-content-14.端侧部署时整个解决方案的核心指标？)
- [15.什么是模型量化？](#user-content-15.什么是模型量化？)
- [16.什么是模型剪枝？](#user-content-16.什么是模型剪枝？)
- [17.主流AI端侧硬件平台有哪些？](#user-content-17.主流ai端侧硬件平台有哪些？)
- [18.主流AI端侧硬件平台一般包含哪些模块？](#user-content-18.主流ai端侧硬件平台一般包含哪些模块？)
- [19.算法工程师该如何看待硬件侧知识？](#user-content-19.算法工程师该如何看待硬件侧知识？)
- [20.现有的一些移动端开源框架？](#user-content-20.现有的一些移动端开源框架？)
- [21.端侧静态多Batch和动态多Batch的区别](#user-content-21.端侧静态多batch和动态多batch的区别)
- [22.优化模型端侧性能的一些方法](#user-content-22.优化模型端侧性能的一些方法)
- [23.ONNX的相关知识](#user-content-23.onnx的相关知识)
- [24.TensorRT的相关知识](#user-content-24.tensorrt的相关知识)
- [25.什么是模型蒸馏？](#user-content-25.什么是模型蒸馏？)
- [26.bfloat16精度和float16精度的区别？](#user-content-26.bfloat16精度和float16精度的区别？)
- [27.AI推理系统介绍](#user-content-27.AI推理系统介绍)
- [28.Nvidia 相关容器镜像使用](#user-content-28.Nvidia相关容器镜像使用)
- [29.常见推理框架介绍](#user-content-29.常见推理框架介绍)
- [30.大模型推理框架介绍](#user-content-30.大模型推理框架介绍)
- [31.TensorRT之trtexec的简单使用介绍](#user-content-31.TensorRT之trtexec的简单使用介绍)
- [32.TensorRT-llm简单介绍](#user-content-32.TensorRT-llm简单介绍)
- [33.PytorchJIT和TorchScript介绍](#user-content-33.PytorchJIT和TorchScript介绍)
- [34.ONNX模型转换及优化](#user-content-34.ONNX模型转换及优化)
- [35.onnxsim的介绍](#user-content-35.onnxsim的介绍)
- [36.TensorRT模型转换](#user-content-36.TensorRT模型转换)
- [37.什么是前端压缩技术和后端压缩技术？](#user-content-37.什么是前端压缩技术和后端压缩技术？)
- [38.在AI领域中模型一共有多少种主流部署形式？](#user-content-38.在AI领域中模型一共有多少种主流部署形式？)

<h2 id="1.模型压缩的必要性与可行性？">1.模型压缩的必要性与可行性？</h2>

模型压缩是指对算法模型进行精简，进而得到一个轻量且性能相当的小模型，压缩后的模型具有更小的结构和更少的参数，可以有效降低计算和存储开销，便于部署在端侧设备中。

随着AI技术的飞速发展，不管是移动端产品还是线上产品，进行AI赋能都成为了趋势。这种情况下，AI算法的实时性与减少内存占用都显得极为重要。AI模型的参数在一定程度上能够表达其复杂性，<font color=DeepSkyBlue>但并不是所有的参数都在模型中发挥作用</font>，部分参数作用有限，表达冗余，甚至会降低模型的性能。

<h2 id="2.x86和arm架构在深度学习侧的区别？">2.X86和ARM架构在深度学习侧的区别？</h2>
  
AI服务器与PC端一般都是使用X86架构，因为其<font color=DeepSkyBlue>高性能</font>；AI端侧设备（手机/端侧盒子等）一般使用ARM架构，因为需要<font color=DeepSkyBlue>低功耗</font>。

X86指令集中的指令是复杂的，一条很长指令就可以很多功能；而ARM指令集的指令是很精简的，需要几条精简的短指令完成很多功能。

X86的方向是高性能方向，因为它追求一条指令完成很多功能；而ARM的方向是面向低功耗，要求指令尽可能精简。

<h2 id="3.fp32，fp16以及int8的区别？">3.FP32，FP16以及Int8的区别？</h2>

常规精度一般使用<font color=OrangeRed>FP32</font>（32位浮点，单精度）占用4个字节，共32位；低精度则使用<font color=OrangeRed>FP16</font>（半精度浮点）占用2个字节，共16位，<font color=OrangeRed>INT8</font>（8位的定点整数）八位整型，占用1个字节等。

混合精度（Mixed precision）指使用FP32和FP16。 使用FP16 可以减少模型一半内存，但有些参数必须采用FP32才能保持模型性能。

虽然INT8精度低，但是数据量小、能耗低，计算速度相对更快，更符合端侧运算的特点。

<font color=DeepSkyBlue>不同精度进行量化的归程中，量化误差不可避免</font>。

在模型训练阶段，梯度的更新往往是很微小的，需要相对较高的精度，一般要用到FP32以上。在inference的阶段，精度要求没有那么高，一般F16或者INT8就足够了，精度影响不会很大。同时低精度的模型占用空间更小了，有利于部署在端侧设备中。

<h2 id="4.gpu显存占用和gpu利用率的定义">4.GPU显存占用和GPU利用率的定义</h2>
  
GPU在训练时有两个重要指标可以查看，即显存占用和GPU利用率。

显存指的是GPU的空间，即内存大小。显存可以用来放模型，数据等。

GPU 利用率主要的统计方式为：在采样周期内，GPU 上有 kernel 执行的时间百分比。可以简单理解为GPU计算单元的使用率。

<h2 id="5.神经网络的显存占用分析">5.神经网络的显存占用分析</h2>
  
Float32 是在深度学习中最常用的数值类型，称为单精度浮点数，每一个单精度浮点数占用4Byte的显存。

在整个神经网络训练周期中，在GPU上的显存占用主要包括：数据，模型参数，模型输出等。

数据侧：举个🌰，一个32*3*128*128的四维矩阵，其占用的显存 = 32*3*128*128*4 /1000 / 1000 = 6.3M

模型侧：占用显存的层包括卷积层，全连接层，BN层，梯度，优化器的参数等。

输出侧：占用的显存包括网络每一层计算出来的feature map以及对应的梯度等。

<h2 id="6.算法模型部署逻辑？">6.算法模型部署逻辑？</h2>

我在之前专门沉淀了一篇关于算法模型部署逻辑的文章，大家可以直接进行阅读取用：
  
[【CV算法上下游】系列之浅谈算法模型部署逻辑](https://mp.weixin.qq.com/s?__biz=Mzg4NDYwOTUwNA==&mid=2247483832&idx=1&sn=3eecfbd284fd9baa7215a7b152416ba8&chksm=cfb4d937f8c350219ecf5d91a95aaf717df77d80aa1c9ad923d59063b92a851abb1ad886df13&scene=21#wechat_redirect)

<h2 id="7.影响模型inference速度的因素？">7.影响模型inference速度的因素？</h2>

1. FLOPs（模型总的加乘运算）
2. MAC（内存访问成本）
3. 并行度（模型inference时操作的并行度越高，速度越快）
4. 计算平台（GPU，AI协处理器，CPU等）

<h2 id="8.为何在ai端侧设备一般不使用传统图像算法？">8.为何在AI端侧设备一般不使用传统图像算法？</h2>
  
AI端侧设备多聚焦于深度学习算法模型的加速与赋能，而传统图像算法在没有加速算子赋能的情况下，在AI端侧设备无法发挥最优的性能。

<h2 id="9.减小模型内存占用有哪些办法？">9.减小模型内存占用有哪些办法？</h2>

1. 模型剪枝
2. 模型蒸馏
3. 模型量化
4. 模型结构调整

<h2 id="10.有哪些经典的轻量化网络？">10.有哪些经典的轻量化网络？</h2>

1. SqueezeNet
2. MobileNet
3. ShuffleNet
4. Xception
5. GhostNet

<h2 id="11.模型参数计算？">11.模型参数计算？</h2>

首先，假设卷积核的尺寸是$K \times K$，有$C$个特征图作为输入，每个输出的特征图大小为$H \times W$，输出为$M$个特征图。

由于模型参数量主要由卷积，全连接层，BatchNorm层等部分组成，我们以卷积的参数量为例进行参数量的计算分析：
  
卷积核参数量：

$$M\times C\times K\times K$$

偏置参数量：

$$M$$

总体参数量：

$$M\times C\times K\times K + M$$

<h2 id="12.模型flops怎么算？">12.模型FLOPs怎么算？</h2>

同样，我们假设卷积核的尺寸是$K\times K$，有$C$个特征图作为输入，每个输出的特征图大小为$H \times W$，输出为$M$个特征图。

由于在模型中卷积一般占计算量的比重是最高的，我们依旧以卷积的计算量为例进行分析：

<font color=DeepSkyBlue>FLOPS（全大写）</font>：是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。

<font color=DeepSkyBlue>FLOPs（s小写）</font>：是floating point operations的缩写（s表示复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。

针对模型的计算量应该指的是FLOPs。

在上述情况下，卷积神经网络一次前向传播需要的乘法运算次数为：

$$H\times W\times M\times C\times K\times K$$

同时，所要进行的加法计算次数分为考虑偏置和不考虑偏置：

(1)考虑偏置的情况：

为了得到输出的特征图的一个未知的像素，我们需要进行
$$(C\times K\times K - 1) + (C - 1) + 1 = C \times K \times K$$
次加法操作，其中$K\times K$大小的卷积操作需要$K\times K - 1$次加法，由于有C个通道，所以需要将结果乘以C，每个通道间的数要相加，所以需要C - 1次加法，最后再加上偏置的1次加法。

所以总的加法计算量如下：

$$H\times W\times M\times C\times K\times K$$

所以总的卷积运算计算量（乘法+加法）：

$$2 \times H\times W\times M\times C\times K\times K$$

(2)不考虑偏置的情况：

总的卷积计算量：

$$H\times W\times M\times (2\times C\times K\times K - 1)$$

![卷积运算过程](https://files.mdnice.com/user/33499/ac4f6843-4108-4318-9821-639339f79c27.gif)

<h2 id="13.什么是异构计算？">13.什么是异构计算？</h2>

首先，<font color=DeepSkyBlue>异构现象</font>是指不同计算平台之间，由于硬件结构(包括计算核心和内存)，指令集和底层软件实现等方面的不同而有着不同的特性。

<font color=DeepSkyBlue>异构计算是指联合使用两个或者多个不同的计算平台，并进行协同运算</font>。比如CPU和GPU的异构计算，TPU和GPU的异构计算以及TPU/GPU/CPU的异构计算等等。

<h2 id="14.端侧部署时整个解决方案的核心指标？">14.端侧部署时整个解决方案的核心指标？</h2>

1. 精度
2. 耗时
3. 内存占用
4. 功耗

<h2 id="15.什么是模型量化？">15.什么是模型量化？</h2>

通常的深度学习模型参数是FP32浮点型的，而<font color=DeepSkyBlue>模型量化主要是使用FP16，INT8以及INT4等低精度类型来保存模型参数，从而有效的降低模型计算量和内存占用，并将精度损失限制在一个可接受的范围内</font>。

模型量化主要分在线量化和离线量化。在线量化在模型训练阶段采用量化方法进行量化。离线量化主要在模型离线工具（模型转换阶段）中采用量化方法进行量化。

<font color=DeepSkyBlue>工业界中主要使用离线量化作为通用模型量化的解决方案。</font>

<h2 id="16.什么是模型剪枝？">16.什么是模型剪枝？</h2>

<font color=DeepSkyBlue>模型剪枝按照剪枝粒度可分为突触剪枝、神经元剪枝、权重矩阵剪枝等，主要是将权重矩阵中不重要的参数设置为0，结合稀疏矩阵来进行存储和计算</font>。通常为了保证性能，需要逐步进行迭代剪枝，让精度损失限制在一个可接受的范围。

突触剪枝剪掉神经元之间的不重要的连接。对应到权重矩阵中，相当于将某个参数设置为0。

神经元剪枝则直接将某个节点直接裁剪。对应到权重矩阵中，相当于某一行和某一列置零。

除此之外，也可以将整个权重矩阵裁剪，每一层中只保留最重要的部分，这就是权重矩阵剪枝。相比突触剪枝和神经元剪枝，权重矩阵剪枝压缩率要大很多。

<h2 id="17.主流ai端侧硬件平台有哪些？">17.主流AI端侧硬件平台有哪些？</h2>

1. 英伟达
2. 海思
3. 寒武纪
4. 比特大陆
5. 昇腾
6. 登临
7. 联咏
8. 安霸
9. 耐能
10. 爱芯
11. 瑞芯

<h2 id="18.主流ai端侧硬件平台一般包含哪些模块？">18.主流AI端侧硬件平台一般包含哪些模块？</h2>

1. 视频编解码模块
2. CPU核心处理模块
3. AI协处理器模块
4. GPU模块
5. DSP模块
6. DDR内存模块
7. 数字图像处理模块

<h2 id="19.算法工程师该如何看待硬件侧知识？">19.算法工程师该如何看待硬件侧知识？</h2>

GPU乃至硬件侧的整体逻辑，是CV算法工作中必不可少的组成部分，也是算法模型所依赖的重要物理载体。

<h3 id="gpu的相关知识">GPU的相关知识</h3>

现在AI行业有个共识，认为是数据的爆发和算力的突破开启了深度学习在计算机视觉领域的“乘风破浪”，而其中的算力，主要就是指以GPU为首的计算平台。

GPU（Graphical Processing Unit）从最初用来进行图形处理和渲染（玩游戏），到通过CUDA/OpenCL库以及相应的工程开发之后，成为深度学习模型在学术界和工业界的底层计算工具，其有以下的一些特征：

1. 异构计算：GPU能作为CPU的协处理器与CPU协同运算。
2. 单指令流多数据流（SIMD）架构：使得同一个指令（比如对图像数据的一些操作），可以同时在多个像素点上<font color=DeepSkyBlue>并行计算</font>，从而得到比较大的吞吐量，深度学习中大量的矩阵操作，让GPU成为一个非常适合的计算平台。
3. 多计算核心。比如Nvidia的GTX980GPU中，在和i7-5960CPU差不多的芯片面积上，有其128倍的运算速度。GTX980中有16个流处理单元，每个流处理单元中包含着128个CUDA计算核心，共有2048个GPU运算单元，与此同时i7-5960CPU只有16个类似的计算单元。
4. CUDA模块。作为GPU架构中的最小单元，它的设计和CPU有着非常类似的结构，其中包括了一个浮点运算单元，整型运算单元以及控制单元。一个流处理单元中的CUDA模块将执行同一个指令，但是会作用在不同的数据上。多CUDA模块意味着GPU有更加高的计算性能，但<font color=DeepSkyBlue>更重要的是在算法侧有没有高效地调度和使用</font>。
5. 计算核心频率。即时钟频率，代表每一秒内能进行同步脉冲次数。就核心频率而言，CPU要高于GPU。由于GPU采用了多核逻辑，即使提高一些频率，其实对于总体性能影响不会特别大。
6. 内存架构。GPU的多层内存架构包括全局内存，2级缓存，和芯片上的存储（包括寄存器，和1级缓存共用的共享内存，只读/纹理缓存和常量缓存）。

![](https://files.mdnice.com/user/33499/24cbb3b8-c530-4eec-a3f5-119b7a8c7ea6.png)

在使用GPU时，在命令行输入nvidia-smi命令时会打印出一张表格，其中包含了GPU当时状态的所有参数信息。

![](https://files.mdnice.com/user/33499/9772a00f-8006-4d93-ac17-13ca84043a3d.png)

CUDA/cuDNN/OpenCL科普小知识：

1. CUDA是NVIDIA推出的用于GPU的并行计算框架。
2. cuDNN是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。
3. OpenCL是由苹果（Apple）公司发起，业界众多著名厂商共同制作的面向异构系统通用目的并行编程的开放式、免费标准，也是一个统一的编程环境。

<h3 id="深度学习的端侧设备">深度学习的端侧设备</h3>

深度学习的端侧设备，又可以叫做边缘计算设备，深度学习特别是CV领域中，<font color=DeepSkyBlue>模型+端侧设备的组合能够加快业务的即时计算，决策和反馈能力，极大释放AI可能性</font>。

![](https://files.mdnice.com/user/33499/6863aa5d-a89b-4b08-a588-5b393cdd6191.png)

深度学习的端侧设备主要由ARM架构的CPU+ GPU/TPU/NPU等协处理器 + 整体功耗 + 外围接口 + 工具链等部分组成，也是算法侧对端侧设备进行选型要考虑的维度。

在实际业务中，根据公司的不同，算法工程师涉及到的硬件侧范围也会不一样。如果公司里硬件和算法由两个部门分别负责，那么算法工程师最多接触到的硬件侧知识就是<font color=DeepSkyBlue>硬件性能评估，模型转换与模型硬件侧验证，一些硬件高层API接口的开发与使用</font>；如果公司里没有这么细分的部门，那么算法工程师可能就会接触到端侧的视频编解码，模型推理加速，Opencv，FFmpeg，Tensor RT，工具链开发等角度的知识。

![](https://files.mdnice.com/user/33499/580db620-75b6-4254-a7c0-4200b9d32c62.png)

<h3 id="算法工程师该如何看待硬件侧">算法工程师该如何看待硬件侧</h3>

首先，整体上还是要将<font color=DeepSkyBlue>硬件侧工具化，把端侧设备当做算法模型的一个下游载体，会熟练的选型与性能评估更加重要</font>。

端侧设备是算法产品整体解决方案中一个非常重要的模块，<font color=DeepSkyBlue>算法+硬件</font>的范式将在未来的边缘计算与万物智能场景中持续发力。

在日常业务中，<font color=DeepSkyBlue>算法模型与端侧设备的适配性与兼容性</font>是必须要考虑的问题，端侧设备是否兼容一些特殊的网络结构？算法模型转化并部署后，精度是否下降？功耗与耗时能否达标？等等都让算法工程师的模型设计逻辑有更多的抓手。

<h2 id="20.现有的一些移动端开源框架？">20.现有的一些移动端开源框架？</h2>

1. NCNN，其GitHub地址：https://github.com/Tencent/ncnn
2. Paddle Lite，其GitHub地址：https://github.com/PaddlePaddle/paddle-mobile
3. MACE（ Mobile AI Compute Engine），其GitHub地址：https://github.com/XiaoMi/mace
4. TensorFlow Lite，其官网地址：https://www.tensorflow.org/lite?hl=zh-cn
5. PocketFlow，其GitHub地址：https://github.com/Tencent/PocketFlow
6. 等等。。。

<h2 id="21.端侧静态多batch和动态多batch的区别">21.端侧静态多Batch和动态多Batch的区别</h2>

当设置静态多Batch后，如Batch=6，那么之后不管是输入2Batch还是4Batch，都会按照6Batch的预设开始申请资源。

而动态多Batch不用预设Batch数，会根据实际场景中的真实输入Batch数来优化资源的申请，提高端侧实际效率。

由于动态多Batch的高性能，通常Inference耗时和内存占用会比静态多Batch时要大。

<h2 id="22.优化模型端侧性能的一些方法">22.优化模型端侧性能的一些方法</h2>

1. 设计能最大限度挖掘AI协处理器性能的模型结构。
2. 多模型共享计算内存。
3. 减少模型分支结构，减少模型元素级操作。
4. 卷积层的输入和输出特征通道数相等时MAC最小，以提升模型Inference速度。

<h2 id="23.onnx的相关知识">23.ONNX的相关知识</h2>

ONNX是一种神经网络模型的框架，其最经典的作用是作为不同框架之间的中间件，成为模型表达的一个通用架构，来增加不同框架之间的交互性。
  
<font color=DeepSkyBlue>ONNX的优势</font>：
1. ONNX的模型格式有极佳的细粒度。
2. ONNX是模型表达的一个通用架构，主流框架都可以兼容。
3. ONNX可以实现不同框架之间的互相转化。

<h2 id="24.tensorrt的相关知识">24.TensorRT的相关知识</h2>
  
TensorRT是一个高性能的深度学习前向Inference的优化器和运行的引擎。
  
<font color=DeepSkyBlue>TensorRT的核心</font>：将现有的模型编译成一个engine，类似于C++的编译过程。在编译engine过程中，会为每一层的计算操作找寻最优的算子方法，将模型结构和参数以及相应kernel计算方法都编译成一个二进制engine，因此在部署之后大大加快了推理速度。

我们需要给TensorRT填充模型结构和参数，也就是解析我们自己的模型结构和参数文件，获取数据放到其中。官方给了三种主流框架模型格式的解析器（parser），分别是：ONNX，Caffe以及TensorFlow。
  
<font color=DeepSkyBlue>TensorRT的优势</font>：

1. 把一些网络层进行了合并。具体🌰如下图所示。
2. 取消一些不必要的操作。比如不用专门做concat的操作等。
3. TensorRT会针对不同的硬件都相应的优化，得到优化后的engine。
4. TensorRT支持INT8和FP16的计算，通过在减少计算量和保持精度之间达到一个理想的trade-off。
  
![TensorRT对网络结构进行重构](https://files.mdnice.com/user/33499/b2b6037e-b4ef-46ba-9fa8-fac46e18e96c.png)

<h2 id="25.什么是模型蒸馏？">25.什么是模型蒸馏？</h2>

模型蒸馏（Model Distillation）是一种模型压缩技术，旨在将一个**大型复杂模型（通常称为“教师模型”）**的知识转移到一个**小型简单模型（称为“学生模型”）**中。

模型蒸馏技术最开始由Hinton等人于2015年提出，主要用于改进小型模型的性能，使其在保持较低计算成本的同时，能够逼近大型模型的性能。

### 模型蒸馏的基本原理

模型蒸馏的基本思想是使用大型模型的输出（软标签）来训练小型模型。

大型模型的输出通常包含了关于类别概率的更多信息，这些信息比硬标签（即实际的类别标签）更能表达不同类别之间的相对关系。通过训练小型模型去学习逼近这些软标签，小型模型可以学习到更细致的决策边界。

### 模型蒸馏的步骤

1. **训练教师模型**：
   教师模型通常是一个大型深度网络，能够在AI细分任务上达到SOTA精度。

2. **生成软标签**：
   使用教师模型对训练数据集进行预测，记录输出的类别概率（软标签）。这些概率不仅表示最可能的类别，还提供了对其他类别的预测概率，包含了更丰富的信息。

3. **训练学生模型**：
   学生模型的结构比教师模型简单，其训练过程不仅使用真实的标签（硬标签），还使用教师模型生成的软标签。通常，训练过程中会使用一个**温度参数（T）**来调整软标签的"软化"程度。损失函数是硬标签的损失和软标签的损失的加权和。

4. **评估学生模型**：
   在独立的测试数据上评估学生模型的性能，验证其是否成功学习到了教师模型的知识。

### 温度调整（Temperature Scaling）

在蒸馏过程中，温度参数`T`用于控制软标签的平滑程度。较高的温度会使得概率分布更加平滑，使得学生模型能够从教师模型的预测中学到更细微的差别。损失函数通常是交叉熵损失，计算学生模型预测和软标签之间的差异。

### 模型蒸馏的优势

- **效率提升**：学生模型通常比教师模型更小、更快，适合部署在资源受限的环境中。
- **泛化能力**：学生模型通过学习教师模型的软标签，通常可以获得比直接训练更好的泛化能力。

### 实际应用

模型蒸馏已被广泛应用于多种任务，如AI绘画、图像分类、图像分割、目标检测、语音识别和自然语言处理等，特别是在需要模型部署到移动设备或需要实时处理的场景中。

总之，模型蒸馏是提高模型部署效率的有效技术，特别适合于需要在保持模型性能的同时减小模型大小和提升计算速度的应用场景。

<h2 id="26.bfloat16精度和float16精度的区别？">26.bfloat16精度和float16精度的区别？</h2>

### BFLOAT16

**定义与结构**：
BFLOAT16是一种16位宽的浮点数格式，由Google针对Tensor Processing Units (TPUs)开发，特别适用于AI算法应用。它的结构如下：
- 1位符号位
- 8位指数
- 7位尾数

这种格式与标准的32位浮点数（FP32）共享相同的指数范围，但尾数精度较低。这意味着BFLOAT16在表示大范围数值时与FP32相近，但在表示精度上有所折衷。

**优点**：
- **较大的动态范围**：与Float16相比，BFLOAT16能够表示更大范围的数值，这归功于它更大的指数范围。这对于深度学习中的梯度和归一化运算非常重要，因为这些操作可能涉及广泛的数值范围。
- **与FP32兼容性好**：由于指数位与FP32相同，BFLOAT16可以无损地转换为FP32，这简化了混合精度训练的实现，同时减少了转换过程中的精度损失。

**缺点**：
- **较低的数值精度**：因为尾数位只有7位，相比于Float16的10位尾数，BFLOAT16在表示精确数值时的能力较弱。

### Float16(FP16)

**定义与结构**：
Float16是IEEE定义的16位浮点数标准，结构如下：
- 1位符号位
- 5位指数
- 10位尾数

Float16提供了较FP32更低的精度和较小的数值范围，但在存储和计算效率上具有优势。

**优点**：
- **较高的数值精度**：Float16的10位尾数提供了比BFLOAT16更高的精度，适合需要高精度计算的应用。
- **计算加速**：在支持Float16运算的硬件上（如GPU），使用Float16可以显著加速计算过程，尤其是在AI模型训练和推理中。

**缺点**：
- **较小的动态范围**：Float16的5位指数提供的动态范围较小，可能导致在一些AI训练场景中出现梯度消失或爆炸问题。
- **兼容性问题**：Float16到FP32的转换可能会涉及更复杂的数值调整，可能导致精度损失。

### 应用场景

- **BFLOAT16**：由于其较大的动态范围和与FP32的良好兼容性，非常适合用于AI模型的训练和推理，尤其是在Google的TPU上。
- **Float16**：适用于对计算精度要求较高的场景，并且在NVIDIA及其他厂商的GPU上得到了广泛支持，尤其适合于需要加速的AI算法任务。
<h2 id="27.AI推理系统介绍">27.AI推理系统介绍</h2>

### 推理系统简述
推理系统是用于部署人工智能模型，执行推理预测任务的人工智能系统，类似传统 Web 服务或移动端应用系统的作用。通过推理系统，可以将深度学习模型部署到云端或者边缘端，并服务和处理用户的请求。模型训练过程好比是传统软件工程中的代码开发的过程，而开发完的代码势必要打包，部署给用户使用，那么推理系统就负责应对模型部署和服务生命周期中遇到的挑战和问题。<br>
当推理系统将完成训练的模型进行部署和服务时，需要考虑设计和提供模型压缩（轻量化，剪枝、量化、蒸馏），负载均衡，请求调度，加速优化，多副本和生命周期管理等支持。相比深度学习框架等为训练而设计的系统，推理系统不仅关注低延迟，高吞吐，可靠性等设计目标，同时受到资源，服务等级协议，功耗等约束。
推理阶段不同于训练阶段，只需要执行前向传播过程，将输入样本计算为输出标签。<br>
### 推理系统的特点
1) 模型被部署为长期运行的服务
服务有明确对请求的低延迟（Low Latency）和高吞吐（High Throughput）需求。例如，互联网服务一般都有明确的响应时间延迟约束，保证用户体验，同时需要应对爆发增长的用户产生的高吞吐请求的响应需求。
2) 推理有更苛刻的资源约束
更小的内存，更低的功耗等。例如，手机的资源要远小于数据中心的商用服务器。
3) 推理不需要反向传播梯度下降
可以牺牲一定的数据精度。例如，模型本身不再被更新，可以通过量化，稀疏性等手段牺牲一定精度换取一定的性能提升。
4) 部署的设备型号更加多样
需要多样的定制化的优化。例如，相比服务器端可以通过 Docker 等手段解决环境问题。移动端显得更为棘手，手机有多种多样的平台与操作系统，IOT 设备有不同的芯片和上层软件栈，需要工具与系统提供编译以减少用户适配代价。
<h2 id="28.Nvidia相关容器镜像使用">28.Nvidia相关容器镜像使用</h2>

通过拉取镜像创建容器，可以免去繁琐的繁琐的环境安装步骤，只需要宿主机上安装Nvidia驱动即可，拉取的镜像中已经配置好cuda，tensorrt，tritonserver等相关需要的环境。以tritonserver为例介绍如何使用镜像，以及介绍triton框架的使用
## 环境搭建
1) 镜像拉取
```sh
docker pull nvcr.io/nvidia/tritonserver:23.01-py3
```
在拉取镜像的时候，需要检查镜像版本和NVIDIA驱动是否匹配，镜像的版本号和驱动版本存在对应关系,参考文档：<https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/index.html>

2) 启动镜像
```sh
docker run -dt --gpus=all -p 1237:22 --name triton -v /home/xxiao/code:/workspace/code nvcr.io/nvidia/tritonserver:23.01-py3
```
--gpus该参数在下文config中详细讲解
报错解决：
!["图片自定义高度" height="" width=""](https://assets.che300.com/wiki/2023-10-17/16975149812428621.png)
运行docker的时候添加参数：
```sh
--cap-add=SYS_ADMIN --security-opt seccomp=unconfined
```

3) 进去docker镜像后，执行
```sh
tritonserver --model-repository=./model_repository
```
--model-repository：该参数指定模型路径，必须要有
--help：查看其他参数
日志相关：--log-verbose=1 --log-info=true
目录结构如下
!["图片自定义高度" height="" width=""](https://assets.che300.com/wiki/2023-10-10/16969066644328920.png)
triton server会加载该目录下的所有模型，在资源足够的情况下，可以通过一个triton server启动所有的推理服务
config.pbtxt内容如下
```
name: "sam_embedding_onnx"
platform: "onnxruntime_onnx"
max_batch_size : 0
input [
  {
    name: "images"
    data_type: TYPE_FP32
    dims: [ 1, 3, 1024, 1024 ]
  }
]
output [
  {
    name: "image_embeddings"
    data_type: TYPE_FP32
    dims: [ 1, 256, 64, 64 ]
  },
  {
    name: "interm_embeddings"
    data_type: TYPE_FP32
    dims: [ 1, 32, 256, 256 ]
  }
]
instance_group [
    {
      count: 1
      kind: KIND_GPU
      gpus: [1]
    }
  ]
```
各个字段含义
```txt
name：model-repository目录下的模型名称，需要保持一致
platform：推理框架选择，实例选择的是onnx模型，对应的为onnxruntime_onnx
常见平台有以下：
    1、tensorflow_savedmodel: TensorFlow SavedModel 格式。
    2、tensorflow_graphdef: TensorFlow GraphDef 格式。
    3、tensorflow_cc: TensorFlow C++ 库。
    4、pytorch_libtorch: PyTorch 的 LibTorch C++ 库。
    5、onnxruntime_onnx: ONNX 运行时（ONNX Runtime）。
    6、tensorrt_plan: NVIDIA TensorRT 的计划模型（plan file）。
    7、custom: 自定义平台，允许用户自行实现推理后端。
max_batch_size：当定义为0时，动态输入
input：定义模型的输入节点，（目前看来必须满足NCHW和NHWC格式），
    name：输入节点名称
    data_type：根据实际数据精度填写
    dims：输入shape
output：模型输出节点，同输入信息定义
instance_group：设备信息定义
    count：启动实例个数
    kind：设备类型（CPU、GPU，可能NPU）
    gpus：模型运行在那个GPU上。
triton server服务是以多进程的方式运行，--gpus指定了多少块GPU就会启动多少个进程，模型会运行在instance_group/gpus指定的gpu上，若不指定gpu，则在所有的gpu上运行
```
启动triton server服务成功后，会开启三个端口供访问，
!["图片自定义高度" height="" width=""](https://assets.che300.com/wiki/2023-10-10/16969085830307693.png)
8000：HTTPService
8001：GRPCInferenceService
8002：Metrics Service

客户端访问实例：
```python
import numpy as np
import tritonclient.http as httpclient
triton_client = httpclient.InferenceServerClient(url="localhost:8000", verbose=False)
model_name = "sam_embedding_onnx"
inputs = []
inputs.append(httpclient.InferInput('images',[1, 3, 1024, 1024], "FP32"))
inputs[0].set_data_from_numpy(np.random.randn(1, 3, 1024, 1024).astype(np.float32),binary_data=False)

outputs = []
outputs.append(httpclient.InferRequestedOutput('image_embeddings'))
outputs.append(httpclient.InferRequestedOutput('interm_embeddings'))
results = triton_client.infer(model_name,inputs, outputs=outputs)
print(results.as_numpy('interm_embeddings').shape)
print(results.as_numpy('image_embeddings').shape)
```
triton server只负责模型加载和推理，有关具体模型的前后处理，需要在客户端代码中实现
### 参考项目
https://github.com/yuxiaoranyu/stable_diffusion_trt_triton
该项目使用tensorrt和triton部署stable_diffusion 图生图模块

<h2 id="29.常见推理框架介绍">29.常见推理框架介绍</h2>

### onnxruntime
onnxruntime是微软推出的一款推理框架，用户可以非常便利的用其运行一个onnx模型。onnxruntime支持多种运行后端，包括CPU、GPU、TensorRT、DML等。
### TensorRT
TensorRT是一个高性能的深度学习推理优化器，可以为深度学习应用提供低延迟、高吞吐率的模型部署。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现在已经能支持TensorFlow、caffe、mxnet、pytorch等几乎所有的深度学习框架，将TensorRT和Nvidia的GPU结合起来，能在几乎所有的框架中进行快速和高效的部署推理。
### OpenVino
OpenVino是英特尔针对自家硬件平台开发的一套深度学习工具库，包含推理库、模型优化等一些列与深度学习模型部署相关的功能。OpenVino是一个比较成熟且仍在快速发展的推理库，提供的demo和sample都很充足，上手比较容易，可以用来快速部署开发，尤其是Intel的硬件平台上性能超过了大部分的开源库。
### Tengine
Tengine是OPEN AI LAB（开放智能）推理的AI推理框架，致力于解决AIoT应用场景下多厂家多种类的边缘AI芯片与多样的训练框架、算法模型之间的相互兼容适配，同时提升算法在芯片上的运行性能，将从云端完成训练后的算法高效迁移到异构的边缘智能芯片上执行，缩短AI应用开发与部署周期，助力加速AI产业化落地。
### NCNN
NCNN是一个为手机端极致优化的高性能神经网络前向计算框架。NCNN从设计之初深刻考虑手机端的部署和应用。无第三方依赖、跨平台，手机端cpu的速度快于目前所有已知的开源框架。目前已在腾讯多款应用中使用，如QQ、Qzone、微信等。
### MNN
MNN是一个高效、轻量的深度学习框架。它支持深度模型推理与训练，尤其在端侧的推理与训练性能在业界处于领先地位。目前MNN已经在阿里巴巴的手机淘宝、天猫、优酷、钉钉、闲鱼等20多个app中使用，覆盖直播、短视频、搜多推荐、商品图像搜索、互动营销、券已发放、安全风控等70多个场景。
### TFLite
TensorFlowLite是Google在2017年5月推出的轻量级机器学习解决方案，主要针对移动端设备和嵌入式设备。针对移动端设备特点，TensorFlow Lite是用来诸多技术对内核进行了定制优化，预熔激活，量子化内核。
<h2 id="30.常见推理框架介绍">30.常见推理框架介绍</h2>

### vLLM
vLLM全称Virtual Large Language Model，由Nvidia开源，旨在降低大模型推理的显存占用。其核心思想是将模型的一部分保存在CPU内存或硬盘上，只将当前计算所需的部分加载到GPU显存中，从而打破GPU显存限制。<br>
vLLM支持PyTorch和FasterTransformer后端，可无缝适配现有模型。使用vLLM，在配备96GB内存+440GB A100的服务器上可运行1750亿参数模型，在配备1.5TB内存+880GB A100的服务器上可运行6万亿参数模型。
### TensorRT-LLM
Tensorrt-LLM是Nvidia在TensorRT推理引擎基础上，针对Transformer类大模型推理优化的框架。主要特性包括：
1) 支持多种优化技术，如kernel融合、矩阵乘优化、量化感知训练等，可提升推理性能
2) 支持多GPU多节点部署，可扩展到万亿规模参数
3) 提供Python和C++ API，易于集成和部署
在Nvidia测试中，基于OPT-30B在A100上的推理，Tensorrt-LLM可实现最高32倍加速。
### DeepSpeed
DeepSpeed是微软开源的大模型训练加速库，最新的DeepSpeed-Inference也提供了推理加速能力，主要特点包括：
1) 通过内存优化、计算优化、通信优化，降低推理延迟和提升吞吐
2) 支持多GPU横向扩展，单卡可推理数百亿参数模型
3) 提供Transformer、GPT、BERT等模型的推理示例
4) 集成Hugging Face transformers库，使用简单
在GPT-NeoX测试中，基于DeepSpeed的推理相比原生PyTorch可实现7.7倍加速。
### Text Generation Inference
Text Generation Inference(简称TextGen)是Hugging Face主导的开源推理框架，旨在为自然语言生成模型如GPT、OPT等提供高性能推理。主要特点包括：
1) 高度优化的核心代码，支持FP16、int8等多种精度
2) 支持多GPU多节点扩展，可推理万亿规模参数
3) 良好的用户体验，提供Python高层API，简化开发
4) 支持Hugging Face生态中的模型，如GPT2、GPT-Neo、BLOOM等

在OPT-175B基准测试中，TextGen可实现最高17倍推理加速。

<h2 id="31.TensorRT之trtexec的简单使用介绍">31.TensorRT之trtexec的简单使用介绍</h2>

### 简介
trtexec是一种无需开发自己的应用程序即可快速使用 TensorRT 的工具。trtexec工具有三个主要用途：
1) 它对于在随机或用户提供的输入数据上对网络进行基准测试很有用。
2) 它对于从模型生成序列化引擎很有用。
3) 它对于从构建器生成序列化时序缓存很有用。
### 转换模型（onnx为例）
1) 将ONNX模型转换为静态batchsize的TensorRT模型，启动所有精度以达到最佳性能，工作区大小设置为1024M
```
trtexec --onnx=mnist.onnx --explicitBatch --saveEngine=mnist.trt --workspace=1024 --best
```
2) 将ONNX模型转换为动态batchsize的TensorRT模型，启动所有精度以达到最佳性能，工作区大小设置为1024M
```
trtexec --onnx=mnist.onnx --minShapes=input:<shape_of_min_batch> --optShapes=input:<shape_of_opt_batch> --maxShapes=input:<shape_of_max_batch> --saveEngine=mnist.trt --best --workspace=1024 --best
```
–minShapes，–optShapes ，–maxShapes必须全部设置，设置的形式为：NCHW

### 运行模型
1) 在具有静态输入形状的全维模式下运行 ONNX 模型
```
trtexec --onnx=model.onnx --shapes=input:32x3x224x224
```
2) 使用给定的输入形状在全维模式下运行 ONNX 模型
```
trtexec --onnx=model.onnx --shapes=input:32x3x224x224
```
3) 使用一系列可能的输入形状对 ONNX 模型进行基准测试
```
trtexec --onnx=model.onnx --minShapes=input:1x3x224x224 --optShapes=input:16x3x224x224 --maxShapes=input:32x3x224x224 --shapes=input:5x3x224x224
```
### 网络性能测试
1) 加载转换后的TensorRT模型进行性能测试，指定batch大小
```
trtexec --loadEngine=mnist16.trt --batch=1
```
2) 收集和打印时序跟踪信息
```
trtexec --deploy=data/AlexNet/AlexNet_N2.prototxt --output=prob --exportTimes=trace.json
```
3) 使用多流调整吞吐量<br>调整吞吐量可能需要运行多个并发执行流。例如，当实现的延迟完全在所需阈值内时，我们可以增加吞吐量，即使以一些延迟为代价。例如，为批量大小 1 和 2 保存引擎并假设两者都在 2ms 内执行，延迟阈值：
```
trtexec --deploy=GoogleNet_N2.prototxt --output=prob --batch=1 --saveEngine=g1.trt --int8 --buildOnly
trtexec --deploy=GoogleNet_N2.prototxt --output=prob --batch=2 --saveEngine=g2.trt --int8 --buildOnly
```
保存的引擎可以尝试找到低于 2 ms 的组合批次/流，以最大化吞吐量：
```
trtexec --loadEngine=g1.trt --batch=1 --streams=2
trtexec --loadEngine=g1.trt --batch=1 --streams=3
trtexec --loadEngine=g1.trt --batch=1 --streams=4
trtexec --loadEngine=g2.trt --batch=2 --streams=2
```
### 参考文档
<https://github.com/NVIDIA/TensorRT/tree/main/samples/trtexec>

<h2 id="32.TensorRT-llm简单介绍">32.TensorRT-llm简单介绍</h2>

### 简介
TensorRT-LLM（NVIDIA官方支持）用于在NVIDIA GPU平台做大模型推理部署工作。<br>
TRT-LLM基于TensorRT来将LLM构建为engine模型

TRT-LLM目前支持多种大模型，可以直接使用，在example中，而且还在以非常快的速度支持新的模型

TRT-LLM支持单机单卡、单机多卡（NCCL）、多机多卡，支持量化（8/4bit）

TRT-LLM的runtime支持chat和stream两种模式

TRT-LLM当前支持python和cpp（可以直接使用cpp，也可以使用cpp的bybind接口）两种模式的runtime

通过example下的各个模型的build.py来构建离线模型，通过example下的run.py（不同的业务适配一下run.py中的逻辑即可）来运行模型

TRT-LLM默认支持kv-cache，支持PagedAttention，支持flashattention，支持MHA/MQA/GQA等

### 安装使用
docker编译安装
```
// docker方式编译
step1: 安装操作系统匹配的docker，参考docker安装方式即可
step2: 下载 tensorrt-llm代码

# TensorRT-LLM uses git-lfs, which needs to be installed in advance.
apt-get update && apt-get -y install git git-lfs

git clone https://github.com/NVIDIA/TensorRT-LLM.git
cd TensorRT-LLM
git submodule update --init --recursive
git lfs install
git lfs pull
// 上述每步都需要执行成功，由于网络问题，可能会失败，失败后重复执行，直到成功位置
// git lfs 这两步会将 tensorrt-llm/cpp/tensort-llm/batch_manager 下面的静态库 下载下来，后来编译会用到
batch_manager/
├── aarch64-linux-gnu
│   ├── libtensorrt_llm_batch_manager_static.a
│   ├── libtensorrt_llm_batch_manager_static.pre_cxx11.a
│   └── version.txt
├── x86_64-linux-gnu
│   ├── libtensorrt_llm_batch_manager_static.a
│   └── libtensorrt_llm_batch_manager_static.pre_cxx11.a
└── x86_64-windows-msvc
    └── tensorrt_llm_batch_manager_static.lib

step3：编译llm，提供了两种方式
方式一：一步到位的编译方式，推荐这种
make -C docker release_build  // 编译，此处cuda/tensorrt/cudnn/nccl等版本都是采用编译脚本中默认设置的
                              // 编译成功后，为一个docker镜像，大概有20多G，另外，docker方式编译对磁盘空间大小有要求
                              // 目前估计需要50G左右，如果docker的根目录空间不够，编译也会失败，可以通过给docker根目
                              //  扩容或者修改根目录来实现，保证编译空间的足够

make -C docker release_run // 运行编译成功的镜像, 此处需要有gpu办卡，如果在没有gpu的环境上，可以编译成功，但是执行会失败

方式二：逐步进行编译，编译结果和上述一致
```
编译有2种包，一种是仅包含cpp的代码包，一种是cpp+python的wheel包
```
//  仅cpp的代码包 ： 仅编译 TensorRT-LLM/cpp 下面的c++和cuda代码
// cpp + python的包： 编译 TensorRT-LLM/cpp 和 TensorRT-LLM/tensortrt-llm 下面的c++ cuda python代码
```
### 参考文档
<https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/docs/source/installation.md>
<h2 id="33.PytorchJIT和TorchScript介绍">33.PytorchJIT和TorchScript介绍</h2>

### 简介
PyTorch支持两种模式：eager模式和script模式。eager模式主要用于模型的编写、训练和调试，script模式主要是针对部署的，其包含PytorchJIT和TorchScript  <br>
script模式使用torch.jit.trace和torch.jit.script创建一个PyTorch eager module的中间表示（intermediate representation, IR），IR 经过内部优化，并在运行时使用 PyTorch JIT 编译。PyTorch JIT 编译器使用运行时信息来优化 IR。该 IR 与 Python 运行时是解耦的。
PyTorch JIT（Just-In-Time Compilation）是 PyTorch 中的即时编译器。

它允许你将模型转化为 TorchScript 格式，从而提高模型的性能和部署效率。
JIT 允许你在动态图和静态图之间无缝切换。你可以在 Python 中以动态图的方式构建和调试模型，然后将模型编译为 TorchScript 以进行优化和部署。
JIT 允许你在不同的深度学习框架之间进行模型转换，例如将 PyTorch 模型转换为 ONNX 格式，从而可以在其他框架中运行。
TorchScript 是 PyTorch 提供的一种将模型序列化以便在其他环境中运行的机制。它将 PyTorch 模型编译成一种中间表示形式，可以在没有 Python 解释器的环境中运行。这使得模型可以在 C++ 等其他语言中运行，也可以在嵌入式设备等资源受限的环境中实现高效的推理。


TorchScript 的特性和用途：
1) 静态图表示形式：TorchScript 是一种静态图表示形式，它在模型构建阶段对计算图进行编译和优化，而不是在运行时动态构建。这可以提高模型的执行效率。
2) 模型导出：TorchScript 允许将 PyTorch 模型导出到一个独立的文件中，然后可以在没有 Python 环境的设备上运行。
3) 跨平台部署：TorchScript 允许在不同的深度学习框架之间进行模型转换，例如将 PyTorch 模型转换为 ONNX 格式，从而可以在其他框架中运行。
4) 模型优化和量化：通过 TorchScript，你可以使用各种技术（如量化）对模型进行优化，从而减小模型的内存占用和计算资源消耗。
5) 融合和集成：TorchScript 可以帮助你将多个模型整合到一个整体流程中，从而提高系统的整体性能。
6) 嵌入式设备：对于资源受限的嵌入式设备，TorchScript 可以帮助你优化模型以适应这些环境。

为什么要用script模式呢？

1) 可以脱离python GIL以及python runtime的限制来运行模型，比如通过LibTorch通过C++来运行模型。这样方便了模型部署，例如可以在IoT等平台上运行。例如这个tutorial，使用C++来运行pytorch的model。
2) PyTorch JIT是用于pytorch的优化的JIT编译器，它使用运行时信息来优化 TorchScript modules，可以自动进行层融合、量化、稀疏化等优化。因此，相比pytorch model，TorchScript的性能会更高。
### 使用
```
import torch

# 假设已经存在模型model
# 创建模型输入tensor
input_tensor = torch.rand(1, 3, 640, 640)
jit_model = torch.jit.trace(model, input_tensor)
torch.jit.save(jit_model, "model.pt")
```
<h2 id="34.ONNX模型转换及优化">34.ONNX模型转换及优化</h2>

### 转换
```
import torch

# 假设已经存在模型model
# 创建模型输入tensor
input_tensor = torch.rand(1, 3, 640, 640)
input_names = ["input"]
output_names = ["output"]
# 设置动态shape
dynamic_axes = {'input': {0: 'batch_size'}, 
                'output': {0: 'batch_size'}}
onnx_model = "model.onnx"
torch.onnx.export(model, input_tensor, 'model.onnx', input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes, opset_version=13)
```
当模型较大时，需要进行一定优化，如stable diffusers中的unet模型
### 优化
```
import onnx
import onnx_graphsurgeon as gs
import torch
from onnx import shape_inference
from polygraphy.backend.onnx.loader import fold_constants
from torch.onnx import export

class Optimizer:
    def __init__(self, onnx_graph, verbose=False):
        self.graph = gs.import_onnx(onnx_graph)
        self.verbose = verbose

    def info(self, prefix):
        if self.verbose:
            print(
                f"{prefix} .. {len(self.graph.nodes)} nodes, {len(self.graph.tensors().keys())} tensors, {len(self.graph.inputs)} inputs, {len(self.graph.outputs)} outputs"
            )

    def cleanup(self, return_onnx=False):
        self.graph.cleanup().toposort()
        if return_onnx:
            return gs.export_onnx(self.graph)

    def select_outputs(self, keep, names=None):
        self.graph.outputs = [self.graph.outputs[o] for o in keep]
        if names:
            for i, name in enumerate(names):
                self.graph.outputs[i].name = name

    def fold_constants(self, return_onnx=False):
        onnx_graph = fold_constants(gs.export_onnx(self.graph), allow_onnxruntime_shape_inference=True)
        self.graph = gs.import_onnx(onnx_graph)
        if return_onnx:
            return onnx_graph

    def infer_shapes(self, return_onnx=False):
        onnx_graph = gs.export_onnx(self.graph)
        if onnx_graph.ByteSize() > 2147483648:
            raise TypeError("ERROR: model size exceeds supported 2GB limit")
        else:
            onnx_graph = shape_inference.infer_shapes(onnx_graph)

        self.graph = gs.import_onnx(onnx_graph)
        if return_onnx:
            return onnx_graph

def optimize(onnx_graph, name, verbose):
    opt = Optimizer(onnx_graph, verbose=verbose)
    opt.info(name + ": original")
    opt.cleanup()
    opt.info(name + ": cleanup")
    opt.fold_constants()
    opt.info(name + ": fold constants")
    opt.infer_shapes()
    opt.info(name + ': shape inference')
    onnx_opt_graph = opt.cleanup(return_onnx=True)
    opt.info(name + ": finished")
    return onnx_opt_graph

model_path = "model.onnx"
shape_inference.infer_shapes_path(model_path, model_path)
model_opt_graph = optimize(onnx.load(model_path), name="model", verbose=True)
```

<h2 id="35.onnxsim的介绍">35.onnxsim的介绍</h2>

### 简介
ONNX-Simplifier（简称onnxsim）是一个开源工具，用于简化ONNX（Open Neural Network Exchange）模型。它通过合并模型中的冗余节点和优化操作来减少模型的大小和复杂性，从而提高模型的执行效率。这个工具是由华为诺亚方舟实验室开发的，并且是作为Python包发布的。

onnxsim的主要功能包括：

1) 节点融合：将多个操作融合为一个操作，减少节点数量，从而减少模型的大小和提高推理速度。
2) 冗余操作消除：移除模型中的冗余操作，如恒等操作或对结果没有影响的操作。
3) 常数折叠：在模型中直接计算可导出为常数的表达式，减少推理时的计算量。
4) 优化形状和类型：优化模型中的张量形状和类型，以减少内存使用和提高效率。
### 使用
```
import onnx
from onnxsim import simplify

# 加载ONNX模型
model_path = 'model.onnx'
onnx_model = onnx.load(model_path)
# 简化模型
model_simplified, check = simplify(onnx_model)
# 检查简化是否成功
assert check, "Simplified ONNX model could not be validated"
# 保存简化后的模型
onnx.save(model_simplified, 'path/to/simplified/model.onnx')
```
命令行使用
```
python -m onnxsim model.onnx model_sim.onnx

```
<h2 id="36.TensorRT模型转换">36.TensorRT模型转换</h2>

假设已经存在onnx模型
```
import sys
import tensorrt as trt

def convert_models(onnx_path: str, output_path: str, fp16: bool = False):

    # 初始化配置
    TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)
    TRT_BUILDER = trt.Builder(TRT_LOGGER)
    TRT_RUNTIME = trt.Runtime(TRT_LOGGER)
    # 创建一个网络定义，并设置EXPLICIT_BATCH标志以支持批处理大小。
    network = TRT_BUILDER.create_network(
        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    onnx_parser = trt.OnnxParser(network, TRT_LOGGER)
    print("onnx_path: ", onnx_path)
    parse_success = onnx_parser.parse_from_file(onnx_path)
    for idx in range(onnx_parser.num_errors):
        print(onnx_parser.get_error(idx))
    if not parse_success:
        sys.exit("ONNX model parsing failed")
    print("Load Onnx model done")

    # 获取网络的输入和输出信息
    inputs = [network.get_input(i) for i in range(network.num_inputs)]
    outputs = [network.get_output(i) for i in range(network.num_outputs)]
    for inp in inputs:
        print(f'input "{inp.name}" with shape{inp.shape} {inp.dtype}')
    for out in outputs:
        print(f'output "{out.name}" with shape{out.shape} {out.dtype}')
    # 创建一个优化配置文件profile，并设置输入节点的动态形状范围。
    profile = TRT_BUILDER.create_optimization_profile()

    profile.set_shape("input", (1, 3, 224, 224), (1, 3, 640, 640), (1, 3, 640, 640))
    
    # 创建一个构建器配置config，并将优化配置文件添加到其中。如果需要，还可以设置FP16精度模式。
    config = TRT_BUILDER.create_builder_config()
    config.add_optimization_profile(profile)
    config.set_preview_feature(trt.PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805, True)
    if fp16:
        config.set_flag(trt.BuilderFlag.FP16)

    # 构建TensorRT engine，并将其序列化
    plan = TRT_BUILDER.build_serialized_network(network, config)
    if plan is None:
        sys.exit("Failed building engine")
    print("Succeeded building engine")
    # 反序列化engine，并保存
    engine = TRT_RUNTIME.deserialize_cuda_engine(plan)

    # save TRT engine
    with open(output_path, "wb") as f:
        f.write(engine.serialize())

if __name__ == "__main__":
    onnx_path = "model.onnx"
    output_path = "model.plan"
    fp16 = True
    convert_models(onnx_path, output_path, fp16)

```

<h2 id="37.什么是前端压缩技术和后端压缩技术？">37.什么是前端压缩技术和后端压缩技术？</h2>

在AI领域，模型压缩技术用于减少模型的大小和计算量，从而使模型更易于部署在资源受限的环境，如移动设备和嵌入式系统中。模型压缩可以在训练的不同阶段实施，通常分为前端压缩（前期压缩）和后端压缩（后期压缩）两种技术。下面Rocky详细介绍这两种技术的特点和应用。

### 模型前端压缩技术（前期压缩）

**前端压缩主要指在模型训练过程中或训练开始前使用的模型压缩技术**。这些技术的目标是减少训练过程中模型的计算和存储需求，或优化模型结构以便更有效地训练。常见的模型前端压缩技术包括：

1. **网络剪枝（Network Pruning）**：在训练初期或训练过程中移除模型中的冗余参数（如权重接近零的神经元）。这可以是结构化剪枝（如移除整个卷积核或神经网络层）或非结构化剪枝（如随机移除单个权重）。

2. **知识蒸馏（Knowledge Distillation）**：在训练过程中，使用一个大的、已训练好的教师模型来指导一个结构更简单的学生模型。通过这种方式，学生模型学习到教师模型的输出行为，而具有更少的参数和计算需求。

3. **低秩分解（Low-Rank Factorization）**：在模型训练之前，将大的权重矩阵分解为几个小的矩阵的乘积，这样可以减少模型参数的数量并降低存储和计算成本。**低秩分解技术在AIGC时代持续繁荣，成为LoRA系列模型的核心思想**。

4. **设计轻量化网络**：从头开始设计轻量级的网络架构，如MobileNet、ShuffleNet等，这些网络结构特别适用于移动和嵌入式系统。

### 模型后端压缩技术（后期压缩）

**后端压缩主要发生在模型训练完成后，目的是减少模型部署时的资源需求**。这些技术使得模型更适合部署在资源受限的设备上。常见的后端压缩技术包括：

1. **量化（Quantization）**：将模型中的浮点数权重转换为低精度的格式（如从32位浮点数转换为8位整数）。这可以显著减少模型的内存占用，并可能加速模型的推理速度。

2. **进一步的网络剪枝**：在模型训练完成后，进行额外的剪枝操作，进一步移除不重要的权重或神经元，以优化模型大小和推理速度。

3. **二值化或三值化（Binarization/Ternarization）**：将权重量化为最极端的低比特形式，即所有权重只有1位（二值）或2位（三值），这极大地减少了模型的大小并提升了运算效率。

4. **编码和压缩**：应用编码技术，如霍夫曼编码，减少模型文件的实际大小，便于存储和传输。

### 结论

前端压缩和后端压缩技术各有优势和应用场景。前端压缩有助于在模型设计和训练阶段建立高效的网络结构，而后端压缩则更专注于模型部署前的优化。结合使用这两种技术可以有效地缩减模型大小，提升运算效率，并使深度学习模型更易于在各种计算场景。

<h2 id="38.在AI领域中模型一共有多少种主流部署形式？">38.在AI领域中模型一共有多少种主流部署形式？</h2>

**AI领域发展至今，Rocky认为目前主要有三大核心方向，包括AIGC、传统深度学习以及自动驾驶**。以下是这些领域中主流的模型部署形式：

### 1. AIGC领域的模型部署

**a. 云端部署**
- **特点**：模型部署在云服务器上，用户通过API调用服务进行内容生成。
- **优势**：可扩展，易于更新和维护，适合处理大量并发请求。
- **应用示例**：OpenAI的GPT-4模型作为API服务提供，用户可以通过互联网调用模型生成文本。

**b. 客户端部署**
- **特点**：模型部署在用户的设备上，如PC或移动设备。
- **优势**：保护用户数据隐私，减少网络延迟。
- **应用示例**：Adobe Photoshop的内容感知工具，可在用户设备上直接运行，不需数据上传到云端。

### 2. 深度学习领域的模型部署

**a. 云端部署**
- **特点**：模型部署在云服务器上，利用云计算的强大计算能力进行数据处理和模型推理。
- **优势**：提供强大的计算资源，易于扩展，便于管理。
- **应用示例**：在图像识别、语音识别等需要大规模数据处理的应用中，云端部署可以快速处理来自全球的请求。

**b. 边缘部署**
- **特点**：模型直接部署在本地设备上，如智能手机、IoT设备或本地服务器。
- **优势**：减少数据传输时间，提高响应速度，增强隐私保护。
- **应用示例**：在移动应用中，如实时语言翻译和增强现实（AR），模型在本地设备上运行，以实现低延迟和离线功能。

### 3. 自动驾驶领域的模型部署

**a. 车载部署**
- **特点**：模型直接部署在车辆的计算平台上，处理来自车载传感器的数据。
- **优势**：实时处理传感器数据，快速做出驾驶决策。
- **应用示例**：自动驾驶汽车中的决策系统，如特斯拉Autopilot，需要在车辆内部快速处理来自摄像头、雷达和其他传感器的数据。

**b. 云端部署**
- **特点**：某些控制功能和监控系统部署在云端控制中心。
- **优势**：可以进行更复杂的数据分析，实时监控和调整车辆行为。
- **应用示例**：远程监控和调度系统，可用于管理自动驾驶车队，例如在无人配送服务中进行调度和监控。

总之，AI模型的部署形式应根据具体的应用需求、数据敏感性、可用资源和所需的响应时间来决定。不同的部署策略具有各自的优势和限制，理解这些可以帮助我们选择最适合特定需求的部署方法。
