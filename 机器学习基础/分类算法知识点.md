# 目录

- [1. 什么是K最近邻算法介绍（K-nearest neighbor， KNN）](#user-content-1. 什么是K最近邻算法介绍（K-nearest neighbor， KNN）)
- [2. 朴素贝叶斯](#user-content-2. 朴素贝叶斯)


<h2 id="1. 什么是K最近邻算法介绍（K-nearest neighbor， KNN）">1. 什么是K最近邻算法介绍（K-nearest neighbor， KNN）</h2>

### （1）K最近邻算法介绍（K-nearest neighbor， KNN）
KNN算法是一种用于分类任务的非参数统计方法。
- 核心思想：当预测一个新样本的标签时，**根据它距离最近的 $k$ 个样本点是什么标签来判断该新样本属于哪个标签**（多数投票）。
- 输入输出：输入为特征空间中的一个点，输出为该点所对应的类别标签。

### （2）KNN的算法流程
 假设一个样本数据集${(x_i, y_i)}$, $x_i$是一个多维向量，$y_i$是该向量的标签，对于未知向量$x_j$，预测其对应的标签$y_j$
 - 计算$x_j$到每一个$x_i$的距离；
 - 对距离进行排序；
 - 选择最接近$x_j$的$k$个样本（也可通过kd树搜索）；
 - 根据多数投票原则，预测$x_j$的标签。
 
### （3）核心参数
#### 距离度量
两个向量$x_i= (x_i^1,x_i^2,x_i^3...x_i^n)$，$x_j=(x_j^1,x_j^2,x_j^3...x_j^n)$的距离 → 两个向量的相似程度，其公式为：
$$L_p(x_i,x_j)=(\sum_{j=1}^{n}{|x_{i}^{l}-x_{j}^{l}|^p})^{\frac{1}{p}}$$
当p=1，为曼哈顿距离；
当p=2，为欧氏距离；
当p=$\infty$，为向量分量的最大距离差。

#### $k$值选取
- 过小的$k$值分类器：未知样本对邻近的样本十分敏感，易受到噪声干扰；
- 过大的$k$值分类器：未知样本易被预测为占比较大的标签类型。

**常用的方法**：  
（1）从$k=1$开始，使用交叉验证法从样本数据集中分出检验集估计分类器的误差率。  
（2）重复该过程，每次$k$增值1，允许增加一个近邻。  
（3）选取产生最小误差率的$k$。
（4）一般$k$值不超过20，上限为$n$的开方。

####  分类决策规则
- **KNN的分类决策规则**：对未知样本的最邻近$k$个样本进行标签统计，采用多数投票进行分类预测。

### （4）Python实现
使用**sklearn.neighbors.KNeighborsClassifier**即可创建KNN分类器，参数包括：
- **n_neighbors**：设定k值，默认为5；
- **weights**：设定k个邻近样本对型统计的权重，默认为平均权重；
- **algorithm**：设定搜索邻近样本的方法，包括ball tree， kd tree和 brute。

### （5）算法优劣
- **优点**：简单易用，无需训练；对异常值不敏感。
- **缺点**：惰性算法，计算量大。

### （6）算法应用场景
- 人脸识别，文字识别，医学图像处理等。 （毋雪雁,王水花,张煜东.K最近邻算法理论与应用综述[J].计算机工程与应用,2017,53(21):1-7.）

### （7）KNN用于回归问题
- 对于k个邻近样本的标签，采用平均值作为未知样本标签的预测值。



<h2 id="2. 朴素贝叶斯">2. 朴素贝叶斯</h2>

### （1） 贝叶斯定理
**先验概率** - Prior probability
- **定义**：在观测数据前，表达不确定量的不确定性的概率分布，记为$P(A)$。
- **释义**：根据已知的经验和分析得到的概率，即由因求果。

**后验概率** - Posterior probability
- **定义**： 考虑和给出相关证据或数据后所得到的条件概率，记为$P(B|A)$。
- **释义**：依据得到的结果所计算出的事件发生的概率，即由果溯因。

**联合概率** - Joint probability
- **定义**：两个事件共同发生的概率，记为$P(AB)$。

**条件概率** - Conditional probability
- **定义**：事件$A$在事件$B$发生的条件下发生的概率。
- **公式**：
$$P(A|B)=\frac{P(AB)}{P(B)}$$

**全概率公式** - Law of total probability
- **定义**：将一复杂事件$A$的概率求解问题转化为不同独立条件（$P({B_1}\bigcap{B_2}...)=0$ & $P({B_1}\bigcup{B_2}...)=1$）下发生的事件概率的求和问题。
- **公式**：
$$P(A)=\sum_{i=1}^n{{P(A|B_i)}\times{P(B_i)}} $$

**贝叶斯定理** - Bayes' theorem
- **定义**：描述条件概率和后验概率的乘法关系。
- **公式**:
$$P(B_j|A)=\frac{{P(A|B_j)}\times{P(B_j)}}{P(A)} $$
$$P(B_j|A)=\frac{{P(A|B_j)}\times{P(B_j)}}{\sum_{i=1}^n{{P(A|B_i)}\times{P(B_i)}}} （独立事件）$$

**后验概率与条件概率的联系**
- 后验概率是一种条件概率，用于描述在给定观测结果的情况下，因变量取某个值的概率。

### （2）案例释义（以信号发射为例）
**案例**：有一个信号的发射端和接收端。发射端只发射A、B两种信号，其中发射信号A的概率为0.6，发射信号B的概率为0.4。当发射信号A时，接收端接收到信号A的概率是0.9，接收到信号B的概率是0.1。当发射信号B时，接收端接收到信号B的概率为0.8，接收到信号A的概率为0.2。求当接收到信号A时，发射信号为A的概率。

**概率**
- **先验概率**：
-$P(sendA)$ = 0.6，$P(sendB)$ = 0.4
- **条件概率**：
-$P(receiveA|sendA)$ = 0.9，$P(receiveB|sendB)$ = 0.8，$P(receiveA|sendB)$ = 0.2
- **后验概率**：
$P(sendA|receiveA)$ = ? 

### （3）朴素贝叶斯 - Naive bayes
**基本假设**
 - 给定数据样本的每个特征相互独立。

**简要推导**
给定训练数据集，其中每个样本$x$都包含$n$维特征，即$x=(x_1, x_2 ..., x_n)$，标签集合有$k$种标签，即$y=(y_1, y_2 ..., y_k)$。对于新样本$x$，判断其标签$y$，根据贝叶斯定理，可以到$x$属于$y_k$标签的概率为：
$$P(y_k|x)=\frac{{P(x|y_k)}\times{P(y_k)}}{\sum_{i=1}^k{{P(x|y_i)}\times{P(y_i)}}}$$
- 后验概率最大的标签则为对新样本$x$的预测标签。

由于朴素贝叶斯的基本假设，所以条件概率$P(x|y_k)$可以转化为：
$$P(x|y_k)=P(x_1,x_2 ..., x_n|y_k)=\prod_{j=1}^n{P(x_j|y_k)}$$
整合上式，朴素贝叶斯算法可以表示为：
$$f(x)={\mathop{argmax}\limits_{y_1, y_2 ..., y_k}}{\frac{\prod_{j=1}^n{P(x_j|y_k)}{P(y_k)}}{\sum_{i=1}^k{{\prod_{j=1}^n{P(x_j|y_i)}}\times{P(y_i)}}}}$$
 由于对于所有的标签，分母一样，因此忽略分母部分，将朴素贝叶斯化简为：
 $$f(x)={\mathop{argmax}\limits_{y_1, y_2 ..., y_k}}{\prod_{j=1}^n{P(x_j|y_k)}{P(y_k)}}$$
 
### （4）细分模型（以文本分类为例）
**高斯模型**
- **连续型变量特征**
- **条件概率**
$$P(x_i|y_k) = \frac{1}{\sqrt{2\pi\sigma_{y_k,i}^2}}\times{e^{-\frac{(x_i-u_{y_k,i})^2}{2\sigma_{y_k,i}^2}}}$$

<smaller> $u_{y_k,i}$ - $y_k$类中，第i维特征的均值；
$\sigma_{y_k,i}^2$ - $y_k$类中，第i维特征的方差。

在文本分类场景下，样本$x$就是文档，特征$x_i$就是单词，标签就$y$就是文档类别，对于新样本$x$判断其类别。

**多项式模型**
- 以**单词的频次**参与统计计算。
- **先验概率**：
$$P(y_k)=\frac{y_k类文档的所有单词}{所有文档的所有单词}$$
- **条件概率**
$$P(x_i|y_k)=\frac{单词x_i在y_k类文档中出现的次数之和+1}{y_k类文档的所有单词+所有文档的单词种类}$$

**伯努利模型**
- 以**是否出现**参与统计计算。
- **先验概率**：
$$P(y_k)=\frac{y_k类文档的个数}{所有文档的个数}$$
- **条件概率**
$$P(x_i|y_k)=\frac{y_k类文档中包含单词x_i的文档个数+1}{y_k类文档的个数+2}$$

### （5）拉普拉斯平滑

由于$P(y_k)\times{\prod_{j=1}^n{P(x_j|y_k)}{P(y_k)}}$为多项连乘，其中一项为0，则整个公式为0(**零概率事件**)。因此，假定训练样本很大时，每个特征$x_i$和样本$x$的计数加1造成的估计概率变化可以忽略不计，但可以方便有效的避免零概率问题。

### （6）Python代码
	sklearn.naive_bayes.MultinomialNB()


### （7）算法优劣

**优点**
- 分类稳定，可以处理多分类任务；
-  对确实数据不敏感，且可以进行增量训练。

**缺点**
- 需要知道事件发生的先验概率。



