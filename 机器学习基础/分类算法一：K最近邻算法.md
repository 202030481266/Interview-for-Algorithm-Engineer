### 1. K最近邻算法介绍（K-nearest neighbor， KNN）
KNN算法是一种用于分类任务的非参数统计方法。
- 核心思想：当预测一个新样本的标签时，**根据它距离最近的 $k$ 个样本点是什么标签来判断该新样本属于哪个标签**（多数投票）。
- 输入输出：输入为特征空间中的一个点，输出为该点所对应的类别标签。

### 2. KNN的算法流程
 假设一个样本数据集${(x_i, y_i)}$, $x_i$是一个多维向量，$y_i$是该向量的标签，对于未知向量$x_j$，预测其对应的标签$y_j$
 - 计算$x_j$到每一个$x_i$的距离；
 - 对距离进行排序；
 - 选择最接近$x_j$的$k$个样本（也可通过kd树搜索）；
 - 根据多数投票原则，预测$x_j$的标签。
 
### 3. 核心参数
#### 3.1 距离度量
两个向量$x_i= (x_i^1,x_i^2,x_i^3...x_i^n)$，$x_j=(x_j^1,x_j^2,x_j^3...x_j^n)$的距离 → 两个向量的相似程度，其公式为：
$$L_p(x_i,x_j)=(\sum_{j=1}^{n}{|x_{i}^{l}-x_{j}^{l}|^p})^{\frac{1}{p}}$$
当p=1，为曼哈顿距离；
当p=2，为欧氏距离；
当p=$\infty$，为向量分量的最大距离差。

####  32 $k$值选取
- 过小的$k$值分类器：未知样本对邻近的样本十分敏感，易受到噪声干扰；
- 过大的$k$值分类器：未知样本易被预测为占比较大的标签类型。

**常用的方法**：  
（1）从$k=1$开始，使用交叉验证法从样本数据集中分出检验集估计分类器的误差率。  
（2）重复该过程，每次$k$增值1，允许增加一个近邻。  
（3）选取产生最小误差率的$k$。
（4）一般$k$值不超过20，上限为$n$的开方。

####  3.3 分类决策规则
- **KNN的分类决策规则**：对未知样本的最邻近$k$个样本进行标签统计，采用多数投票进行分类预测。

### 4. Python实现
使用**sklearn.neighbors.KNeighborsClassifier**即可创建KNN分类器，参数包括：
- **n_neighbors**：设定k值，默认为5；
- **weights**：设定k个邻近样本对型统计的权重，默认为平均权重；
- **algorithm**：设定搜索邻近样本的方法，包括ball tree， kd tree和 brute。

### 5. 算法优劣
- **优点**：简单易用，无需训练；对异常值不敏感。
- **缺点**：惰性算法，计算量大。

### 6. 算法应用场景
- 人脸识别，文字识别，医学图像处理等。 （毋雪雁,王水花,张煜东.K最近邻算法理论与应用综述[J].计算机工程与应用,2017,53(21):1-7.）

### 7. KNN用于回归问题
- 对于k个邻近样本的标签，采用平均值作为未知样本标签的预测值。


