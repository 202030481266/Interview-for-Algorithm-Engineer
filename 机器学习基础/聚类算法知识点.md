# 目录

- [1.常用的距离度量方法](#user-content-1.常用的距离度量方法)

<h2 id="1.常用的距离度量方法">1.常用的距离度量方法</h2>

1. 欧式距离

2. 闵可夫斯基距离

3. 马氏距离

4. 互信息

5. 余弦距离

6. 皮尔逊相关系数

7. Jaccard相关系数

8. 曼哈顿距离
  
![](https://files.mdnice.com/user/33499/d17199ff-84e7-49e1-bbf5-19a2b12e6686.png)


<h2 id="2.K-Means聚类算法">2.K-Means聚类算法</h2>

2.1. K-means 的算法步骤为：
1. 初始化：选择k 个样本作为初始聚类中心；
2. 针对数据集中每个样本，计算它到 k 个聚类中心的距离并将其分配到与它距离最小的聚类中心所对应的簇中；
3. 针对每个簇，重新计算它的聚类中心，即该簇的所有样本的质心；
4. 重复上面 2、3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）。

+ 手撕k-means
```
import numpy as np

def K_means(X,K=3,max_iters=100,eps=1e-8):

    centers = X[np.random.choice(X.shape[0], K, replace=False)]
    
    for _ in range(max_iters):
        
        distances = np.linalg.norm(X[:, np.newaxis, :] - centers, axis=2)
        labels = np.argmin(distances, axis=1)
        new_centers = np.array([X[labels == k].mean(axis=0) for k in range(K)])
        
        if np.linalg.norm(centers - new_centers, axis=1).mean()<eps:
            break
    
        centers = new_centers

    return labels
 
```

2.2. 复杂度
+ 时间复杂度：$O(tknm)$，其中，t 为迭代次数，k 为簇的数目，n 为样本点数，m 为样本点特征维度。
+ 空间复杂度：$O(knm)$，其中，k 为簇的数目，m 为样本点特征维度，n 为样本点数。

2.3. 优缺点
1. 优点
+ 容易理解，聚类效果不错（局部最优）；
+ 可以用于处理大数据集；
+ 数据近似高斯分布时，效果很好；
+ 算法复杂度低。
2. 缺点
+ K 值需要人定；
+ 对初始簇中心敏感；
+ 对异常值敏感；
+ 一个样本只能聚类为一个簇；
+ 对于太离散的数据、样本类别不平衡的数据、非凸形状的数据，聚类效果较差。

2.4. 调优与改进
1. 为了使基于欧氏距离的聚类效果更好，需要对数据先去除异常点，再进行数据归一化（使数据零均值，单位方差）、标准化（每维特征的单位相同等）。
2. 合理选择 K 值。
+ 使用手肘法人工选取。
画出聚类簇数K（X轴）-误差平方和SSE（Y轴）的曲线。当 K 小于真实聚类簇数时，K 增大会增加每个簇的紧致程度，SSE下降幅度大；当 K 接近真实聚类簇数时，增加 K 导致的簇紧致程度影响会迅速变小，SSE 下降幅度骤减，随着 K 值继续增大而趋于平缓。SSE 和 K 的关系图是一个手肘形状，这个肘部对应的 K 值就是数据的真实聚类簇数K。
+ Gap statistic 方法。
基于公式 $Gap(K) = E(logD_k) - logD_k$。$D_k$为聚类簇数是K时的损失函数(即误差平方和)，$E(logD_k)$指的是$logD_k$的期望。通过蒙特卡洛模拟计算$E(logD_k)$：在样本里所在的区域中按照均匀分布随机产生和原始样本数一样多的随机样本，并对这些随机样本做 K-Means，得到一个$D_k$。一般模拟 20 次，得到 20 个 $logD_k$。对这 20 个数值求平均值得到$E(logD_k)$的近似值。最终计算 Gap Statisitc $Gap(K)$，$Gap(K)$ 取得最大值所对应的 K 就是最佳的 K。
3. 采用核函数处理非近似高斯分布的数据。
+ 利用核函数的非线性映射特性。
4. K-means++。
+ 仅在初始化簇中心的方式上做了改进，其它地方同K-means聚类算法一样。它在初始化簇中心时的改进为：逐个选取K个簇中心，且离现有簇中心越远的样本点越有可能被选为下一个簇中心。从数据集中随机选取一个样本点作为第一个初始聚类中心；计算每个样本与当前已有聚类中心之间的最短距离$d_i$，然后计算每个样本点被选为下一个聚类中心的概率（$p(i)=\frac{d_i}{\sum_i d_i}$），选择最大概率值所对应的样本点作为下一个簇中心；重复选取直到选择出K个聚类中心。
5. K-means II。
+ 改变K-means++中初始化簇中心时的取样策略：每次取样 k 个最大概率值所对应的样本点，重复该取样过程 $log(n)$（一般4到5次就够了）次，则得到 $Klog(n)$个样本点，然后从这些点中选取 k 个作为簇中心。
6. 迭代自组织数据分析法（ISODATA）。
+ 无需 人为的设定K 的值。它在K-means算法的基础上，增加对聚类结果的合并和分裂两个操作，当聚类结果某一类中样本数太少，或两个类间的距离太近，或样本类别远大于设定类别数时，进行合并，当聚类结果某一类中样本数太多，或某个类内方差太大，或样本类别远小于设定类别数时，进行分裂。



