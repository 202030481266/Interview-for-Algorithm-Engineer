### 1. 贝叶斯定理
**先验概率** - Prior probability
- **定义**：在观测数据前，表达不确定量的不确定性的概率分布，记为$P(A)$。
- **释义**：根据已知的经验和分析得到的概率，即由因求果。

**后验概率** - Posterior probability
- **定义**： 考虑和给出相关证据或数据后所得到的条件概率，记为$P(B|A)$。
- **释义**：依据得到的结果所计算出的事件发生的概率，即由果溯因。

**联合概率** - Joint probability
- **定义**：两个事件共同发生的概率，记为$P(AB)$。

**条件概率** - Conditional probability
- **定义**：事件$A$在事件$B$发生的条件下发生的概率。
- **公式**：
$$P(A|B)=\frac{P(AB)}{P(B)}$$

**全概率公式** - Law of total probability
- **定义**：将一复杂事件$A$的概率求解问题转化为不同独立条件（$P({B_1}\bigcap{B_2}...)=0$ & $P({B_1}\bigcup{B_2}...)=1$）下发生的事件概率的求和问题。
- **公式**：
$$P(A)=\sum_{i=1}^n{{P(A|B_i)}\times{P(B_i)}} $$

**贝叶斯定理** - Bayes' theorem
- **定义**：描述条件概率和后验概率的乘法关系。
- **公式**:
$$P(B_j|A)=\frac{{P(A|B_j)}\times{P(B_j)}}{P(A)} $$
$$P(B_j|A)=\frac{{P(A|B_j)}\times{P(B_j)}}{\sum_{i=1}^n{{P(A|B_i)}\times{P(B_i)}}} （独立事件）$$

**后验概率与条件概率的联系**
- 后验概率是一种条件概率，用于描述在给定观测结果的情况下，因变量取某个值的概率。

### 2. 案例释义（以信号发射为例）
**案例**：有一个信号的发射端和接收端。发射端只发射A、B两种信号，其中发射信号A的概率为0.6，发射信号B的概率为0.4。当发射信号A时，接收端接收到信号A的概率是0.9，接收到信号B的概率是0.1。当发射信号B时，接收端接收到信号B的概率为0.8，接收到信号A的概率为0.2。求当接收到信号A时，发射信号为A的概率。

**概率**
- **先验概率**：
-$P(sendA)$ = 0.6，$P(sendB)$ = 0.4
- **条件概率**：
-$P(receiveA|sendA)$ = 0.9，$P(receiveB|sendB)$ = 0.8，$P(receiveA|sendB)$ = 0.2
- **后验概率**：
$P(sendA|receiveA)$ = ? 

### 3. 朴素贝叶斯 - Naive bayes
**基本假设**
 - 给定数据样本的每个特征相互独立。

**简要推导**
给定训练数据集，其中每个样本$x$都包含$n$维特征，即$x=(x_1, x_2 ..., x_n)$，标签集合有$k$种标签，即$y=(y_1, y_2 ..., y_k)$。对于新样本$x$，判断其标签$y$，根据贝叶斯定理，可以到$x$属于$y_k$标签的概率为：
$$P(y_k|x)=\frac{{P(x|y_k)}\times{P(y_k)}}{\sum_{i=1}^k{{P(x|y_i)}\times{P(y_i)}}}$$
- 后验概率最大的标签则为对新样本$x$的预测标签。

由于朴素贝叶斯的基本假设，所以条件概率$P(x|y_k)$可以转化为：
$$P(x|y_k)=P(x_1,x_2 ..., x_n|y_k)=\prod_{j=1}^n{P(x_j|y_k)}$$
整合上式，朴素贝叶斯算法可以表示为：
$$f(x)={\mathop{argmax}\limits_{y_1, y_2 ..., y_k}}{\frac{\prod_{j=1}^n{P(x_j|y_k)}{P(y_k)}}{\sum_{i=1}^k{{\prod_{j=1}^n{P(x_j|y_i)}}\times{P(y_i)}}}}$$
 由于对于所有的标签，分母一样，因此忽略分母部分，将朴素贝叶斯化简为：
 $$f(x)={\mathop{argmax}\limits_{y_1, y_2 ..., y_k}}{\prod_{j=1}^n{P(x_j|y_k)}{P(y_k)}}$$
 
### 4. 细分模型（以文本分类为例）
**高斯模型**
- **连续型变量特征**
- **条件概率**
$$P(x_i|y_k) = \frac{1}{\sqrt{2\pi\sigma_{y_k,i}^2}}\times{e^{-\frac{(x_i-u_{y_k,i})^2}{2\sigma_{y_k,i}^2}}}$$

<smaller> $u_{y_k,i}$ - $y_k$类中，第i维特征的均值；
$\sigma_{y_k,i}^2$ - $y_k$类中，第i维特征的方差。

在文本分类场景下，样本$x$就是文档，特征$x_i$就是单词，标签就$y$就是文档类别，对于新样本$x$判断其类别。

**多项式模型**
- 以**单词的频次**参与统计计算。
- **先验概率**：
$$P(y_k)=\frac{y_k类文档的所有单词}{所有文档的所有单词}$$
- **条件概率**
$$P(x_i|y_k)=\frac{单词x_i在y_k类文档中出现的次数之和+1}{y_k类文档的所有单词+所有文档的单词种类}$$

**伯努利模型**
- 以**是否出现**参与统计计算。
- **先验概率**：
$$P(y_k)=\frac{y_k类文档的个数}{所有文档的个数}$$
- **条件概率**
$$P(x_i|y_k)=\frac{y_k类文档中包含单词x_i的文档个数+1}{y_k类文档的个数+2}$$

### 5. 拉普拉斯平滑

由于$P(y_k)\times{\prod_{j=1}^n{P(x_j|y_k)}{P(y_k)}}$为多项连乘，其中一项为0，则整个公式为0(**零概率事件**)。因此，假定训练样本很大时，每个特征$x_i$和样本$x$的计数加1造成的估计概率变化可以忽略不计，但可以方便有效的避免零概率问题。

### 6. Python代码
	sklearn.naive_bayes.MultinomialNB()


### 7. 算法优劣

**优点**
- 分类稳定，可以处理多分类任务；
-  对确实数据不敏感，且可以进行增量训练。

**缺点**
- 需要知道事件发生的先验概率。

