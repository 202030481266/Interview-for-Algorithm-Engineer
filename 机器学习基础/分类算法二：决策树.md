### 1. 决策树（Decision Tree）
决策树是一种树形结构模型，由一个根节点，若干个内部节点和叶节点组成。其中，根节点和内部结点表示一个特征或属性，叶结点表示一个类别。西瓜分类的一颗决策树如下所示：

>![输入图片描述](imgs/DT-1.png)

### 2. 基本流程
 决策树一个由根到叶的递归过程，通过根节点和内部节点划分属性，直到只剩单一类型/无可用属性后停止。其伪代码如下所示：
>![输入图片描述](imgs/DT-2.png)

**具体停止条件**
 - 当前节点包含的样本属于同一类别，视为叶节点，无需划分；
 - 无可用属性进行后续划分，视为同一类别（叶节点），无需划分；
 - 当点节点不包含样本，删除该节点，回退至父节点重新划分。
 
### 3. 算法分类
- **ID3**： 在决策树的内部节点和根节点上，使用信息增益方法作为划分属性的选择标准，信息增益越大越好。
- **C4.5**：使用信息增益率来选择节点属性，增益率越高越好。
-  **CART**：使用基尼指数选择划分属性，基尼指数越小越好。

### 4. 划分属性的选择标准
#### 4.1 信息增益
假定离散属性$a$有$V$个可能的取值${a^1, a^2, ..., a^n}$，若使用$a$来对样本集$D$进行划分，则会产生$V$个内部节点，其中第$v$个内部节点包含了$D$中所有在属性$a$上取值为${a^n}$的样本，记为$D_v$，根据式（1）计算出$D_v$的信息熵，并给予该节点权重$\frac{|D^v|}{|D|}$。再考虑到其他内部节点，计算出属性$a$对样本集进行划分所获得的“信息增益”，如式（2）所示。
$$Ent(D) = - \sum_1^n{p_klog_2p_k}$$
 - <smaller> $Ent(D)$ - 信息熵；
 - $n$ - 类别数目；
 - $p_k$ - 样本集中第$k$类样本所占的比例
 $$Gain(D,a) = Ent(D) - \sum_1^V\frac{|D^v|}{|D|}Ent(D^v)$$
 - $Gain(D, a)$ - 信息增益。
 - 
#### 4.2 信息增益率
$$Gain_ratio(D) =\frac{Gain(D,a)}{-\sum_1^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}}$$

#### 4.3 基尼指数
$$Gini(D) = - \sum_{k_1=1}^n\sum_{k_2!=k_1}{p_{k_1}}{p_{k_2}}$$
$$Gini_index(D,a) =  \sum_{1}^V\frac{|D^v|}{|D|}Gini(D^v)$$

### 5. Python代码
	from sklearn import tree #导入需要的模块
	clf = tree.DecisionTreeClassifier() #实例化模型对象
	class sklearn.tree.DecisionTreeClassifier (criterion=’gini’/'entropy', splitter=’best’, max_depth=None,min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None,random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,class_weight=None, presort=False)



### 6. 算法优劣
**优点**

- 易于理解和实现；
-   数据准备简单；

**缺点**
- 类别过多时，分类困难且时间较长。

