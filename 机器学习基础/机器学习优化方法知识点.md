# 目录

- [1.数据类别不平衡怎么处理？](#user-content-1.数据类别不平衡怎么处理？)
- [2.什么是过拟合，解决过拟合的方法有哪些？](#user-content-2.什么是过拟合，解决过拟合的方法有哪些？)
- [3.什么是欠拟合，解决欠拟合的方法有哪些？](#user-content-3.什么是欠拟合，解决欠拟合的方法有哪些？)
- [4.正则化的本质以及常用正则化手段？](#user-content-4.正则化的本质以及常用正则化手段？)
- [5.L范数的作用？](#user-content-5.l范数的作用？)
- [6.Dropout的作用？](#user-content-6.dropout的作用？)
- [7.如何找到让F1最高的分类阈值？](#user-content-7.如何找到让f1最高的分类阈值？)
- [8.L1正则为什么比L2正则更容易产生稀疏解?](#user-content-8.l1正则为什么比l2正则更容易产生稀疏解)
- [9.梯度爆炸和梯度消失产生的原因及解决方法?](#user-content-9.梯度爆炸和梯度消失产生的原因及解决方法)
- [10.数据EDA逻辑（Exploratory Data Analysis）？](#user-content-10.数据eda逻辑（exploratory-data-analysis）？)
- [11.什么是PEFT技术？](#user-content-11.什么是PEFT技术？)
- [12.什么是哈达玛积，在AI领域有哪些典型应用？](#user-content-12.什么是哈达玛积，在AI领域有哪些典型应用？)
- [13.什么是机器学习中的鞍点？](#user-content-13.什么是机器学习中的鞍点？)


<h2 id="1.数据类别不平衡怎么处理？">1.数据类别不平衡怎么处理？</h2>
  
1. 数据增强。

2. 对少数类别数据做过采样，多数类别数据做欠采样。

3. 损失函数的权重均衡。（不同类别的loss权重不一样，最佳参数需要手动调节）

4. 采集更多少数类别的数据。

5. 转化问题定义，将问题转化为异常点检测或变化趋势检测问题。 异常点检测即是对那些罕见事件进行识别，变化趋势检测区别于异常点检测，其通过检测不寻常的变化趋势来进行识别。

6. 使用新的评价指标。

7. 阈值调整，将原本默认为0.5的阈值调整到：较少类别/（较少类别+较多类别）。

<h2 id="2.什么是过拟合，解决过拟合的方法有哪些？">2.什么是过拟合，解决过拟合的方法有哪些？</h2>
  
<font color=DeepSkyBlue>过拟合</font>：模型在训练集上拟合的很好，但是模型连噪声数据的特征都学习了，丧失了对测试集的泛化能力。

<font color=DeepSkyBlue>解决过拟合的方法</font>：

1. 重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清洗数据或重新选择数据。
  
2. 增加训练样本数量。使用更多的训练数据是解决过拟合最有效的手段。我们可以通过一定的规则来扩充训练数据，比如在图像分类问题上，可以通过图像的平移、旋转、缩放、加噪声等方式扩充数据;也可以用GAN网络来合成大量的新训练数据。
3. 降低模型复杂程度。适当降低模型复杂度可以避免模型拟合过多的噪声数据。在神经网络中减少网络层数、神经元个数等。
4. 加入正则化方法，增大正则项系数。给模型的参数加上一定的正则约束，比如将权值的大小加入到损失函数中。
5. 采用dropout方法，dropout方法就是在训练的时候让神经元以一定的概率失活。
6. 提前截断（early stopping），减少迭代次数。
7. 增大学习率。
8. 集成学习方法。集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险，如Bagging方法。

<h2 id="3.什么是欠拟合，解决欠拟合的方法有哪些？">3.什么是欠拟合，解决欠拟合的方法有哪些？</h2>
  
<font color=DeepSkyBlue>欠拟合</font>：模型在训练集和测试集上效果均不好，其根本原因是模型没有学习好数据集的特征。

<font color=DeepSkyBlue>解决欠拟合的方法</font>：

1. 可以增加模型复杂度。对于神经网络可以增加网络层数或者神经元数量。

2. 减小正则化系数。正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要有针对性地减小正则化系数。

3. Boosting。

<h2 id="4.正则化的本质以及常用正则化手段？">4.正则化的本质以及常用正则化手段？</h2>
  
正则化是机器学习的核心主题之一。<font color=DeepSkyBlue>正则化本质是对某一问题加以先验的限制或约束以达到某种特定目的的一种操作</font>。在机器学习中我们通过使用正则化方法，防止其过拟合，降低其泛化误差。

常用的正则化手段：

1. 数据增强
  
2. 使用L范数约束
3. dropout
4. early stopping
5. 对抗训练

<h2 id="5.l范数的作用？">5.L范数的作用？</h2>
  
L范数主要起到了正则化（<font color=DeepSkyBlue>即用一些先验知识约束或者限制某一抽象问题</font>）的作用，而正则化主要是防止模型过拟合。

范数主要用来表征高维空间中的距离，故在一些生成任务中也直接用L范数来度量生成图像与原图像之间的差别。

下面列出深度学习中的范数：
  
![](https://files.mdnice.com/user/33499/bca9e394-0548-4882-8e7f-47607bebbba7.png)

![](https://files.mdnice.com/user/33499/5c98ba3a-d4d1-4078-9ee9-5146990e3363.png)


<h2 id="6.dropout的作用？">6.Dropout的作用？</h2>

Dropout是在训练过程中以一定的概率使神经元失活，也就是输出等于0。从而提高模型的泛化能力，减少过拟合。
  
![使用Dropout](https://files.mdnice.com/user/33499/5f3b5d37-facd-4817-a765-990e6833b4a7.png)

我们可以从<font color=DeepSkyBlue>两个方面去直观地理解Dropout的正则化效果</font>：1）在Dropout每一轮训练过程中随机丢失神经元的操作相当于多个模型进行取平均，因此用于预测时具有vote的效果。2）减少神经元之间复杂的共适应性。当隐藏层神经元被随机删除之后，使得全连接网络具有了一定的稀疏化，从而有效地减轻了不同特征的协同效应。也就是说，有些特征可能会依赖于固定关系的隐含节点的共同作用，而通过Dropout的话，就有效地避免了某些特征在其他特征存在下才有效果的情况，增加了神经网络的鲁棒性。

<font color=DeepSkyBlue>Dropout在训练和测试时的区别</font>：Dropout只在训练时产生作用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合风险。而在测试时，应该用整个训练好的模型，因此不需要Dropout。

<h2 id="7.如何找到让f1最高的分类阈值？">7.如何找到让F1最高的分类阈值？</h2>
  
首先，这个问题只存在于二分类问题中，对于多分类问题，只需要概率最高的那个预测标签作为输出结果即可。

F1值是综合了精准率和召回率两个指标对模型进行评价：
  
![](https://files.mdnice.com/user/33499/9ed8d70a-11fe-4e11-9bf1-6f7666965316.png)

一般设0.5作为二分类的默认阈值，但一般不是最优阈值。想要精准率高，一般使用高阈值，而想要召回率高，一般使用低阈值。在这种情况下，我们通常可以通过P-R曲线去寻找最优的阈值点或者阈值范围。

<h2 id="8.l1正则为什么比l2正则更容易产生稀疏解">8.L1正则为什么比L2正则更容易产生稀疏解?</h2>

我们首先可以设目标函数为 $L$ ，目标函数中的权值参数为 $w$ ，那么目标函数和权值参数的关系如下所示：

![](https://img-blog.csdnimg.cn/20200817112906402.png)

如上图所示，最优的 $w$ 在绿色的点处，而且 $w$ 非零。

我们首先可以使用L2正则进行优化，新的目标函数： $L + CW^{2}$ ，示意图如下蓝线所示：

![](https://img-blog.csdnimg.cn/20200817113043873.png)

我们可以看到，最优的 $w$ 出现在黄点处， $w$ 的绝对值减小了，更靠近横坐标轴，但是依然是非零的。

<font color=DeepSkyBlue>为什么是非零的呢？</font>

我们可以对L2正则下的目标函数求导：

![](https://files.mdnice.com/user/33499/6fff569d-11fa-4c0d-93e7-604f6aed387e.png)

我们发现，权重 $w$ 每次乘上的是小于1的倍数进行收敛，而且其导数在 $w=0$ 时没有办法做到左右两边导数异号，所以L2正则使得整个训练过程稳定平滑，但是没有产生稀疏性。

接下来我们使用L1正则，新的目标函数： $L + C|w|$ ，示意图如下粉线所示：

![](https://img-blog.csdnimg.cn/20200817115050210.png)

这里最优的 $w$ 就变成了0。因为保证使用L1正则后 $x=0$ 处左右两个导数异号，就能满足极小值点形成的条件。

我们来看看这次目标函数求导的式子：

![](https://img-blog.csdnimg.cn/20200817115308997.png)

可以看出L1正则的惩罚很大， $w$ 每次都是减去一个常数的线性收敛，所以L1比L2更容易收敛到比较小的值，而如果 $C > |f^{'}(0)|$ ，就能保证 $w = 0$ 处取得极小值。

上面只是一个权值参数 $w$ 。在深层网路中，L1会使得大量的 $w$ 最优值变成0，从而使得整个模型有了稀疏性。

<h2 id="9.梯度爆炸和梯度消失产生的原因及解决方法">9.梯度爆炸和梯度消失产生的原因及解决方法?</h2>

### 梯度爆炸和梯度消失问题

一般在深层神经网络中，我们需要预防梯度爆炸和梯度消失的情况。

梯度消失（gradient vanishing problem）和梯度爆炸（gradient exploding problem）一般随着网络层数的增加会变得越来越明显。

例如下面所示的含有三个隐藏层的神经网络，梯度消失问题发生时，接近输出层的hiden layer3的权重更新比较正常，但是前面的hidden layer1的权重更新会变得很慢，导致前面的权重几乎不变，仍然接近初始化的权重，<font color=DeepSkyBlue>这相当于hidden layer1没有学到任何东西，此时深层网络只有后面的几层网络在学习，而且网络在实际上也等价变成了浅层网络</font>。

![](https://img-blog.csdnimg.cn/2020071110042155.png)

### 产生梯度爆炸和梯度消失问题的原因

我们来看看看反向传播的过程：

（假设网络每一层只有一个神经元，并且对于每一层 $y_{i} = \sigma(z_{i}) = \sigma(w_{i}x_{i} + b_{i})$ ）

![](https://img-blog.csdnimg.cn/20200711101713569.png)

可以推导出：

![](https://img-blog.csdnimg.cn/20200711101729614.png)

而sigmoid的导数 $\sigma^{'}(x)$ 如下图所示：

![](https://img-blog.csdnimg.cn/20200711101845385.png)

可以知道， $\sigma^{'}(x)$的最大值是$\frac{1}{4}$ ，而我们初始化的权重 $|w|$ 通常都小于1，因此 $\sigma^{'}(x)|w| <= \frac{1}{4}$ ，而且链式求导层数非常多，不断相乘的话，最后的结果越来越小，趋向于0，就会出现梯度消失的情况。

梯度爆炸则相反， $\sigma^{'}(x)|w| > 1$ 时，不断相乘结果变得很大。

<font color=DeepSkyBlue>梯度爆炸和梯度消失问题都是因为网络太深，网络权重更新不稳定造成的，本质上是梯度方向传播的连乘效应。</font>

### 梯度爆炸和梯度消失的解决方法

1. 使用预训练加微调策略。
2. 进行梯度截断。
3. 使用ReLU、LeakyReLU等激活函数。
4. 引入BN层。
5. 使用残差结构。
6. 使用LSTM思想。

<h2 id="10.数据eda逻辑（exploratory-data-analysis）？">10.数据EDA逻辑（Exploratory Data Analysis）？</h2>

1. 导入相应的Modules（numpy，pandas，matplotlib，PIL等）
2. 阅读了解所有的数据文件（图片数据，类别文件，辅助文件等）
3. 数据类别特征分析（数据类别总数，数据类别的平衡度，数据尺寸，噪声数据等）
4. 数据可视化二次分析（直观了解不同类别的区别）


<h2 id="11.什么是PEFT技术？">11.什么是PEFT技术？</h2>

PEFT（Parameter-Efficient Fine-Tuning，参数高效微调）是一种在AI模型上进行微调的技术，旨在提高模型在小样本数据上的性能，同时减少训练和推理的计算资源消耗。PEFT技术通过仅更新模型的一部分参数或通过使用更高效的参数更新策略来达到这些目标。以下是Rocky对PEFT技术的详细讲解：

### 我们为什么需要PEFT技术

在实际应用中，深度学习模型往往需要针对特定任务或数据集进行微调。然而，微调整个模型的所有参数不仅计算成本高，而且容易导致过拟合，特别是在训练数据有限的情况下。PEFT 通过优化参数更新策略，可以在保持高性能的同时显著减少计算资源需求。

### PEFT的主要方法

PEFT包括多种具体的方法和策略，以下是一些常见的PEFT技术：

#### 1. 微调部分层（Layer-wise Fine-Tuning）

**概述**：仅微调模型的某些特定层，而保持其他层的参数不变。

**方法**：
- 冻结部分层：在训练过程中保持某些层的参数固定，只更新特定层的参数。
- 选择性微调：根据任务的需求和模型的特性，选择重要或高层次的层进行微调。

**优点**：减少计算量和过拟合风险，同时保留模型的预训练知识。

#### 2. 低秩分解（Low-Rank Factorization）

**概述**：将模型参数矩阵分解为几个低秩矩阵，只更新低秩矩阵的参数。

**方法**：
- 使用奇异值分解（SVD）等分解技术，将高维参数矩阵分解为低秩矩阵。
- 在训练过程中，只更新这些低秩矩阵的参数。

**优点**：减少参数数量和计算复杂度，提高训练效率。

#### 3. 子网络选择（Subnetwork Selection）

**概述**：从预训练模型中选择一个子网络进行微调，而不改变整个模型。

**方法**：
- Lottery Ticket Hypothesis：通过训练，发现预训练模型中存在一个子网络（Lottery Ticket），能够在小样本情况下表现良好。
- 激活特定子网络：在训练和推理过程中，仅激活并微调模型的某些子网络。

**优点**：充分利用预训练模型的潜力，减少计算资源需求。

#### 4. 权重共享和参数重用（Weight Sharing and Parameter Reuse）

**概述**：在模型中共享权重或重用参数，以减少训练和推理的计算量。

**方法**：
- 在多任务学习中，多个任务共享部分模型参数。
- 使用权重共享技术，如Transformer中的共享注意力头。

**优点**：提高参数利用率，减少模型参数数量。

### PEFT的应用场景
1. AIGC
2. 传统深度学习
3. 自动驾驶

### PEFT 的优势和挑战

#### 优势

1. **减少计算资源需求**：通过只更新一部分参数，显著减少训练和推理的计算资源需求。
2. **提高训练效率**：通过参数高效更新策略，加快模型的训练速度。
3. **减少过拟合风险**：在小样本数据上训练时，减少过拟合的风险，提高模型的泛化能力。

#### 挑战

1. **选择合适的策略**：需要根据具体任务和数据特点，选择合适的PEFT策略。
2. **模型复杂性**：某些PEFT方法可能会增加模型的实现复杂性，需要更多的工程实践。
3. **参数调优**：需要精细调优模型参数和训练超参数，以达到最佳效果。

### PEFT技术实际案例

以下是一个简单的示例，展示我们如何应用PEFT技术进行模型微调：

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models

# 加载预训练模型
model = models.resnet50(pretrained=True)

# 冻结所有层的参数
for param in model.parameters():
    param.requires_grad = False

# 替换最后的全连接层
model.fc = nn.Linear(model.fc.in_features, 10)  # 假设有10个分类

# 仅微调最后的全连接层
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# 加载数据
transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)

# 训练模型
model.train()
for epoch in range(10):  # 假设训练10个epoch
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

print("微调完成")
```

在这个示例中，我们加载了一个预训练的ResNet 50模型，仅微调了最后的全连接层以适应新的分类任务（CIFAR-10数据集）。通过冻结其他层的参数，我们实现了PEFT，提高了训练效率并减少了过拟合风险。

### 总结

PEFT（参数高效微调）技术通过优化参数更新策略，在保持模型性能的同时，显著减少了计算资源需求和过拟合风险。PEFT在AIGC、传统深度学习、自动驾驶等领域具有广泛应用前景。选择合适的PEFT策略并进行精细调优，可以让我们在小样本数据上实现高效且高性能的模型微调。


<h2 id="12.什么是哈达玛积，在AI领域有哪些典型应用？">12.什么是哈达玛积，在AI领域有哪些典型应用？</h2>

### 哈达玛积（Hadamard Product）

哈达玛积（Hadamard Product），又称逐元素乘积（element-wise product），是线性代数中的一种矩阵运算。它与标准矩阵乘法不同，哈达玛积是对两个相同大小的矩阵的对应元素进行乘积运算。

哈达玛积通过逐元素乘积，它能够有效地进行特征组合、权重计算和信息传播，增强模型的表达能力和计算效率。

### 数学定义

给定两个相同大小的矩阵 $ A $ 和 $ B $，它们的哈达玛积 $ C $ 定义如下：

$$ C = A \circ B $$

其中 $ C $ 的每个元素 $ c_{ij} $ 计算为：

$$ c_{ij} = a_{ij} \times b_{ij} $$

例如，假设有以下两个矩阵 $ A $ 和 $ B $：

$$ A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, \quad B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} $$

它们的哈达玛积 $ C $ 为：

$$ C = A \circ B = \begin{bmatrix} 1 \times 5 & 2 \times 6 \\ 3 \times 7 & 4 \times 8 \end{bmatrix} = \begin{bmatrix} 5 & 12 \\ 21 & 32 \end{bmatrix} $$

### 哈达玛积在深度学习中的应用

在AI行业中，哈达玛积有多种重要的应用，特别是在AIGC、传统深度学习、自动驾驶领域中。下面是一些典型的应用和作用：

1. **注意力机制**：
   在注意力机制（Attention Mechanism）中，哈达玛积被用来计算注意力权重。例如，在自注意力（Self-Attention）机制中，通过计算输入向量之间的相似度，利用哈达玛积对这些相似度进行加权，从而突出输入序列中重要的部分。

2. **门控机制**：
   在循环神经网络（RNN）和长短期记忆网络（LSTM）中，哈达玛积用于门控机制。例如，LSTM中的输入门、遗忘门和输出门的计算都涉及到哈达玛积，这些门控制了信息的流动。

3. **卷积神经网络（CNN）**：
   在CNN中，哈达玛积用于元素级别的操作，例如在深度可分离卷积（Depthwise Separable Convolutions）中，通过对每个通道进行逐元素乘积来实现卷积操作，减少了计算量和参数量。

4. **图神经网络（GNN）**：
   在图神经网络中，哈达玛积用于节点特征的组合。例如，在图卷积网络（GCN）中，通过逐元素乘积来聚合邻居节点的特征，从而更新节点的表示。


<h2 id="13.什么是机器学习中的鞍点？">13.什么是机器学习中的鞍点？</h2>

在机器学习的算法优化中，鞍点（saddle point）是一个非常重要的概念，特别是在训练深度学习模型时。理解鞍点对于理解优化过程的困难性和改进优化算法至关重要。

### 鞍点的定义

在数学和优化领域，**鞍点是一个既不是局部最小值也不是局部最大值的点，但在某些方向上表现为最小值，而在其他方向上表现为最大值**。

鞍点正式的定义如下：
- 对于函数 $f(x)$ ，点 $x^*$ 是鞍点，如果在 $x^*$ 点，函数在某些方向上的二次导数为正（表示该方向上的局部最小值），而在其他方向上的二次导数为负（表示该方向上的局部最大值）。

### 鞍点的几何解释

从几何角度上看，鞍点类似于马鞍的形状。在二维空间中，**可以将鞍点想象为一个山谷和山脊交汇的地方**。沿着一个方向，它是山谷（局部最小值），沿着另一个方向，它是山脊（局部最大值）。

### 鞍点对机器学习模型的影响

在训练机器学习模型时，损失函数往往是高维非凸函数，具有许多局部最小值、鞍点和平坦区域。鞍点对优化算法（如梯度下降）造成了以下几个影响：

1. **训练困难**：
   - 梯度下降法在鞍点附近的收敛速度会变得非常慢，因为梯度在鞍点处为零，导致更新步长变小，优化过程停滞。

2. **鞍点数量众多**：
   - 高维空间中的鞍点数量比局部最小值多得多，因此优化算法更容易遇到鞍点而非局部最小值。

3. **逃离鞍点**：
   - 使用优化算法如随机梯度下降（SGD），通过引入随机噪声，能够帮助算法逃离鞍点。这是因为噪声可以帮助突破平坦区域，继续朝着下降方向前进。

### 鞍点与局部最小值的区别

局部最小值是指在一个小范围内，函数值小于或等于该范围内其他点的值。鞍点虽然在某些方向上是最小值，但在其他方向上是最大值，因此既不是纯粹的最小值也不是最大值。

### 实例与图示

#### 简单的二维示例

以函数 $f(x, y) = x^2 - y^2$ 为例。

- 函数在原点 $(0, 0)$ 处有一个鞍点。
- 沿 $x$ 方向，函数表现为 $x^2$ ，在 $x=0$ 时是最小值。
- 沿 $y$ 方向，函数表现为 $-y^2$ ，在 $y=0$ 时是最大值。

#### 三维图示

在三维空间中，函数 $f(x, y) = x^2 - y^2$ 的图形如下所示：

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

x = np.linspace(-2, 2, 400)
y = np.linspace(-2, 2, 400)
X, Y = np.meshgrid(x, y)
Z = X**2 - Y**2

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z, cmap='viridis')

ax.set_xlabel('X axis')
ax.set_ylabel('Y axis')
ax.set_zlabel('Z axis')
ax.set_title('Saddle Point Example')

plt.show()
```

![机器学习中的鞍点示意图](./机器学习中的鞍点示意图.png)

在这个图中，$(0, 0)$ 处的点即为鞍点。我们可以看到，在 $x$ 方向是一个上凸的曲线（局部最小值），在 $y$ 方向是一个下凸的曲线（局部最大值）。

