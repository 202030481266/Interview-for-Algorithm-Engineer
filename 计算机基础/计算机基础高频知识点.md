# 目录

- [1.深度学习中常用的Linux命令汇总](#user-content-1.深度学习中常用的linux命令汇总)
- [2.计算机多线程和多进程的区别？](#user-content-2.计算机多线程和多进程的区别？)
- [3.TCP/IP四层模型的相关概念](#user-content-3.tcpip四层模型的相关概念)
- [4.OSI七层模型的相关概念](#user-content-4.osi七层模型的相关概念)
- [5.Linux中的进程状态种类](#user-content-5.linux中的进程状态种类)
- [6.Linux中ps aux指令与grep指令配合管理进程](#user-content-6.linux中ps-aux指令与grep指令配合管理进程)
- [7.Git，GitLab，SVN的相关知识](#user-content-7.git，gitlab，svn的相关知识)
- [8.协程的相关概念](#user-content-8.协程的相关概念)
- [9.Linux系统的相关概念](#user-content-9.linux系统的相关概念)
- [10.Linux系统和Windows系统的区别？](#user-content-10.linux系统和windows系统的区别？)
- [11.POC验证测试的概念](#user-content-11.poc验证测试的概念)
- [12.Docker的相关概念及常用命令](#user-content-12.docker的相关概念及常用命令)
- [13.深度学习中常用的文件格式汇总](#user-content-13.深度学习中常用的文件格式汇总)
- [14.TCP和UDP的区别？](#user-content-14.tcp和udp的区别？)
- [15.conda创建管理虚拟环境的命令大全](#user-content-15.conda创建管理虚拟环境的命令大全)
- [16.如何让两台服务器之间ssh免密通信？](#user-content-16.如何让两台服务器之间ssh免密通信？)
- [17.什么是主机ip和BMC信息？](#user-content-17.什么是主机ip和BMC信息？)
- [18.Linux中的find命令使用大全](#user-content-18.Linux中的find命令使用大全)
- [19.通过Dockerfile构建镜像](#user-content-19.通过Dockerfile构建镜像)
- [20.docker-compose的使用](#user-content-20.docker-compose的使用)
- [21.CPU和GPU的区别？](#user-content-21.CPU和GPU的区别？)
- [22.两个Linux服务器之间数据传输的命令有哪些？](#user-content-22.两个Linux服务器之间数据传输的命令有哪些？)
- [23.计算机中同步调用和异步调用有哪些区别？](#user-content-23.计算机中同步调用和异步调用有哪些区别？)
- [24.计算机中串行操作和并行操作有哪些区别？](#user-content-24.计算机中串行操作和并行操作有哪些区别？)
- [25.Linux中的tail命令使用大全](#user-content-25.Linux中的tail命令使用大全)
- [26.Linux中有哪些常用的查看文件夹占用空间的命令？](#user-content-26.Linux中有哪些常用的查看文件夹占用空间的命令？)
- [27.为什么有符号INT数据类型可表示的负数比正数多1？](#27.为什么有符号INT数据类型可表示的负数比正数多1？)
- [28.介绍一下Linux系统中的Shell脚本](#28.介绍一下Linux系统中的Shell脚本)
- [29.介绍一下AI行业中规范的AI服务启动Shell脚本](#29.介绍一下AI行业中规范的AI服务启动Shell脚本)
- [30.如何使用Git将AI项目切换到特定分支？](#30.如何使用Git将AI项目切换到特定分支？)
- [31.在AI项目中，从云端读取传输信息失败，显示网络不可用，一般可以怎么解决呢？](#31.在AI项目中，从云端读取传输信息失败，显示网络不可用，一般可以怎么解决呢？)
- [32.docker中如何使用gpu？](#32.docker中如何使用gpu？)
- [33.docker中如何使用摄像头并显示画面？](#docker中如何使用摄像头并显示画面？)

<h2 id="1.深度学习中常用的linux命令汇总">1.深度学习中常用的Linux命令汇总</h2>
  
```
1.man：man command，可以查看某个命令的帮助文档，按q退出帮助文档
2.cd：用于切换目录，cd - 可以在最近两次目录之间来回切换
3.touch：touch file创建文件。
4.ls：ls -lh可以列出当前目录下文件的详细信息。
5.pwd：pwd命令以绝对路径的方式显示用户当前的工作目录
6.cat：cat file显示文件内容。
7.mkdir：mkdir dir可以创建一个目录；mkdir -p dir/xxx/xxx可以递归创建目录。
8.cat：cat file显示文件内容，按q退出。
9.more：more file显示文件内容，按q退出。
10.grep：筛选命令，比如我想查找当前目录下的py文件（ls -lh | grep .py）
11.whereis：可以查找含有制定关键字的文件，如whereis python3
重定向 > 和 >>：Linux 允许将命令执行结果重定向到一个文件，将本应显示在终端上的内容输出／追加到指定文件中。其中>表示输出，会覆盖原有文件；>>表示追加，会将内容追加到已有文件的末尾。
12.cp：cp dst1 dst2复制文件；cp -r dst1 dst2复制文件夹。
13.mv：mv dst1 dst2可以移动文件、目录，也可以给文件或目录重命名。
14.zip：zip file.zip file压缩文件；zip dir.zip -r dir压缩文件夹。
15.unzip：unzip file.zip解压由zip命令压缩的.zip文件。
16.tar：
    tar -cvf file.tar dir打包文件夹
    tar -xvf file.tar解包
    tar -czvf file.tar.gz dir压缩文件夹
    tar -zxvf file.tar.gz解压
17.chmod：chmod -R 777 data将整个data文件夹修改为任何人可读写。
18.ps：ps aux列出所有进程的详细信息。
19.kill：kill PID根据PID杀死进程。
20.df：df -h 查看磁盘空间。
21.du：du -h dir查看文件夹大小。
22.top：实时查看系统的运行状态，如 CPU、内存、进程的信息。
23wget：wget url从指定url下载文件。
24.ln：ln -s dst1 dst2建立文件的软链接，类似于windows的快捷方式；ln dst1 dst2建立文件的硬链接。无论哪种链接，dst1都最好使用绝对路径。
25.top：我们可以使用top命令实时的对系统处理器的状态进行监视。
26.apt-get：用于安装，升级和清理包。
27.vim：对文件内容进行编辑。
28.nvidia-smi：对GPU使用情况进行查看。
29.nohup sh test.sh &：程序后台运行且不挂断。
30.find：这个命令用于查找文件，功能强大。find . -name "*.c"表示查找当前目录及其子目录下所有扩展名是.c的文件。
```

<h2 id="2.计算机多线程和多进程的区别？">2.计算机多线程和多进程的区别？</h2>

<font color=DeepSkyBlue>进程和线程的基本概念</font>：

进程：是并发执行的程序在执行过程中分配和管理资源的基本单位，是一个动态的概念，竞争计算机系统资源的基本单位。

线程：是进程的一个执行单元，是进程内的调度实体。比进程更小的独立运行的基本单位。线程也被称为轻量级进程。

<font color=OrangeRed>一个程序至少一个进程，一个进程至少一个线程</font>。

<font color=DeepSkyBlue>线程的意义</font>：

每个进程都有自己的地址空间，即进程空间，在网络环境下，一个服务器通常需要接收不确定数量用户的并发请求，为每一个请求都创建一个进程显然行不通（系统开销大响应用户请求效率低），因此操作系统中线程概念被引进。线程的执行过程是线性的，尽管中间会发生中断或者暂停，但是进程所拥有的资源只为该线状执行过程服务，一旦发生线程切换，这些资源需要被保护起来。<font color=OrangeRed>进程分为单线程进程和多线程进程</font>，单线程进程宏观来看也是线性执行过程，微观上只有单一的执行过程。多线程进程宏观是线性的，微观上多个执行操作。

<font color=DeepSkyBlue>进程和线程的区别</font>：

<font color=OrangeRed>地址空间</font>：同一进程中的线程共享本进程的地址空间，而进程之间则是独立的地址空间。

<font color=OrangeRed>资源拥有</font>：同一进程内的线程共享进程的资源如内存、I/O、CPU等，但是进程之间的资源是独立的。（一个进程崩溃后，在保护模式下不会对其他进程产生影响，但是一个线程崩溃可能导致整个进程都死掉。<u>所以多进程比多线程健壮</u>。进程切换时，消耗的资源大、效率差。所以涉及到频繁的切换时，使用线程要好于进程。同样如果要求同时进行并且又要共享某些变量的并发操作，只能用线程不能用进程。）

<font color=OrangeRed>执行过程</font>：每个独立的线程都有一个程序运行的入口、顺序执行序列和程序出口。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。（线程是基于进程的）

<font color=OrangeRed>线程是处理器调度的基本单元，但进程不是</font>。

<font color=OrangeRed>两者均可并发执行</font>。

<font color=DeepSkyBlue>进程和线程的优缺点</font>：

线程执行开销小，但是不利于资源的管理和保护。

进程执行开销大，但是能够很好的进行资源管理和保护。

<font color=DeepSkyBlue>何时使用多进程，何时使用多线程</font>:

对资源的管理和保护要求高，不限制开销和效率时，使用多进程。（CPU密集型任务）

要求效率高，频繁切换时，资源的保护管理要求不是很高时，使用多线程。（I/O密集型任务）

<h2 id="3.tcpip四层模型的相关概念">3.TCP/IP四层模型的相关概念</h2>
  
<font color=DeepSkyBlue>TCP/IP四层模型</font>：

1. 应用层：负责各种不同应用之间的协议，如文件传输协议（FTP），远程登陆协议（Telnet），电子邮件协议（SMTP），网络文件服务协议（NFS），网络管理协议（SNMP）等。

2. 传输层：负责可靠传输的TCP协议、高效传输的UDP协议。

3. 网络层：负责寻址（准确找到对方设备）的IP，ICMP，ARP，RARP等协议。

4. 数据链路层：负责将数字信号在物理通道（网线）中准确传输。

<font color=DeepSkyBlue>四层模型逻辑</font>：

发送端是由上至下，把上层来的数据在头部加上各层协议的数据（部首）再下发给下层。

接受端则由下而上，把从下层接收到的数据进行解密和去掉头部的部首后再发送给上层。

层层加密和解密后，应用层最终拿到了需要的数据。
  
<h2 id="4.osi七层模型的相关概念">4.OSI七层模型的相关概念</h2>


![](https://files.mdnice.com/user/33499/d2286ff0-c54b-4905-88e4-cfb6dc4c4c54.png)

<h2 id="5.linux中的进程状态种类">5.Linux中的进程状态种类</h2>

1. 运行（正在运行或在运行队列中等待）
2. 中断（休眠中，受阻，在等待某个条件的形成或等待接受到信号）
3. 不可中断（收到信号不唤醒和不可运行，进程必须等待直到有中断发生）
4. 僵死（进程已终止，但进程描述符存在，直到父进程调用wait4()系统调用后释放）
5. 停止（进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行）

<h2 id="6.linux中ps-aux指令与grep指令配合管理进程">6.Linux中ps aux指令与grep指令配合管理进程</h2>

<h3 id="ps相关指令">ps相关指令</h3>

ps命令（Process Status）是最基本同时也是非常强大的进程查看命令。

- ps a 显示现行终端机下的所有程序，包括其他用户的程序。
- ps -A   显示所有程序。
- ps c    列出程序时，显示每个程序真正的指令名称，而不包含路径，参数或常驻服务的标示。
- ps -e  此参数的效果和指定"A"参数相同。
- ps e   列出程序时，显示每个程序所使用的环境变量。
- ps f    用ASCII字符显示树状结构，表达程序间的相互关系。
- ps -H    显示树状结构，表示程序间的相互关系。
- ps -N   显示所有的程序，除了执行ps指令终端机下的程序之外。
- ps s     采用程序信号的格式显示程序状况。
- ps S     列出程序时，包括已中断的子程序资料。
- ps -t <终端机编号> 　指定终端机编号，并列出属于该终端机的程序的状况。
- ps u 　 以用户为主的格式来显示程序状况。
- ps x 　 显示所有程序，不以终端机来区分。

<h3 id="ps-aux--more-指令">ps aux | more 指令</h3>

这个指令可以显示进程详细的状态。

参数解释：

- USER：进程的所有者。
- PID：进程的ID。
- PPID：父进程。
- %CPU：进程占用的CPU百分比。
- %MEM：进程占用的内存百分比。
- NI：进程的NICE值，数值越大，表示占用的CPU时间越少。
- VSZ：该进程使用的虚拟内存量（KB）。
- RSS：该进程占用的固定内存量（KB）。
- TTY：该进程在哪个终端上运行，若与终端无关，则显示？。若为pts/0等，则表示由网络连接主机进程。
- WCHAN：查看当前进程是否在运行，若为-表示正在运行。
- START：该进程被触发启动时间。
- TIME：该进程实际使用CPU运行的时间。
- COMMAND：命令的名称和参数。
- STAT状态位常见的状态字符：
D 无法中断的休眠状态（通常 IO 的进程）；
R 正在运行可中在队列中可过行的；
S 处于休眠状态；
T 停止或被追踪；
W 进入内存交换  （从内核2.6开始无效）；
X 死掉的进程   （基本很少見）；
Z 僵尸进程；
< 优先级高的进程
N 优先级较低的进程
L 有些页被锁进内存；
s 进程的领导者（在它之下有子进程）；
l 多进程的（使用 CLONE_THREAD, 类似 NPTL pthreads）；+ 位于后台的进程组；

<h3 id="ps-aux--grep-xxx命令">ps aux | grep xxx命令</h3>

如果直接用ps命令，会显示所有进程的状态，通常结合grep命令查看某进程的状态。

grep （global search regular expression(RE) and print out the line,全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。

例如我想要查看Python 的所有进程，可以在终端输入如下命令：

```bash
ps aux | grep python
```

便可以把Python相关的进程全部都打印到终端供我们查看。相关参数和之前的ps aux | more一致。

<h3 id="进程结束命令">进程结束命令</h3>

我们可以使用kill命令来结束进程。

如下面的指令所示：

```bash
kill   PID  //杀掉进程
kill  -9 PID //强制杀死进程
```

<h2 id="7.git，gitlab，svn的相关知识">7.Git，GitLab，SVN的相关知识</h2>

<h3 id="git">Git</h3>

Git是当前主流的一种<font color=DeepSkyBlue>开源分布式版本控制系统，可以有效、快速的进行项目版本管理</font>。

Git没有中央服务器，不同于SVN这种需要中央服务器的集中式版本控制系统。

Git的功能：版本控制（版本管理，远程仓库，分支协作）

Git的工作流程：

![Git工作流程](https://files.mdnice.com/user/33499/b3f94684-959d-4e9d-a842-57e19f4c7381.png)

Git的常用命令：

```
git init 创建仓库

git clone 克隆github上的项目到本地

git add  添加文件到缓存区

git commit 将缓存区内容添加到仓库中
```

<h3 id="gitlab">GitLab</h3>

GitLab是一个基于Git实现的在线代码仓库软件，可以基于GitLab搭建一个类似于GitHub的仓库，<font color=DeepSkyBlue>但是GitLab有完善的管理界面和权限控制，有较高的安全性，可用于企业和学校等场景</font>。

<h3 id="svn">SVN</h3>

SVN全名Subversion，是一个开源的版本控制系统。<font color=DeepSkyBlue>不同于Git，SVN是集中式版本控制系统</font>。

SVN只有一个集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。

SVN的特点是<font color=DeepSkyBlue>安全，效率，资源共享</font>。

SVN的常用操作：

```
Checkout 检出代码

Update 更新代码

Commit 提交代码

Add 提交新增文件

Revert to this version + commit 撤销已经提交的代码
```

<h2 id="8.协程的相关概念">8.协程的相关概念</h2>

协程（Coroutine，又称微线程）<font color=DeepSkyBlue>运行在线程之上，更加轻量级，协程并没有增加线程总数，只是在线程的基础之上通过分时复用的方式运行多个协程，大大提高工程效率</font>。

协程的特点：

1. 协程类似于子程序，但执行过程中，协程内部可中断，然后转而执行其他的协程，在适当的时候再返回来接着执行。协程之间的切换不需要涉及任何系统调用或任何阻塞调用。
2. 协程只在一个线程中执行，发生在用户态上的一个逻辑。并且是协程之间的切换并不是线程切换，而是由程序自身控制，协程相比线程节省线程创建和切换的开销。
3. 协程中不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。

![](https://files.mdnice.com/user/33499/4e041c47-28e1-4b92-9cc0-f922888775c1.png)

协程适用于有大量I/O操作业务的场景，可以到达很好的效果，一是降低了系统内存，二是减少了系统切换开销，因此系统的性能也会提升。

在协程中尽量不要调用阻塞I/O的方法，比如打印，读取文件等，除非改为异步调用的方式，并且协程只有在I/O密集型的任务中才会发挥作用。

<h2 id="9.linux系统的相关概念">9.Linux系统的相关概念</h2>

Linux系统是一种操作系统（Operating System简称OS），它是软件的一部分，是硬件基础上的第一层软件，即硬件和应用软件沟通的桥梁。

Linux系统系统会控制其他程序运行，管理系统资源，提供最基本的计算功能，如管理及配置内存、决定系统资源供需的优先次序等，同时还提供一些基本的服务程序。<font color=DeepSkyBlue>Linux系统内核</font>指的是提供硬件抽象层、硬盘及文件系统控制及多任务功能的系统核心程序。<font color=DeepSkyBlue>Linux发行套件系统</font>是由 Linux系统内核与各种常用应用软件的集合产品。

<font color=DeepSkyBlue>在Linux系统中一切都是文件</font>。在linux系统中，目录、字符设备、块设备、套接字、打印机等都被抽象成了文件，Linux系统中的一切文件都是从“根(/)”目录开始的，并按照树形结构来存放文件，且定义了常见目录的用途，文件和目录名称严格区分大小写。

<h3 id="linux系统的文件目录结构">Linux系统的文件目录结构</h3>

- /usr：这是一个非常重要的目录，包含绝大多数的（多）用户工具和应用程序，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。
- /lib：存放着系统开机时会用到的函数库，以及在/bin和/sbin下命令会调用的函数库，几乎所有的应用程序都需要用到这些共享库。
- /var：存放不断扩充的内容，如经常被修改的目录、文件（包括各种日志文件）等。
- /boot：存放启动Linux时所需的一些核心文件（linux内核文件），包括一些引导程序文件、链接文件、镜像文件等。
- /home：用户的主目录，在Linux中，每个用户都有一个自己的目录，该目录名一般以用户账号命名，包含保存的文件、个人设置等。
- /sbin：s就是Super User的意思，这里存放的是系统管理员使用的系统管理命令。
- /bin：这个存放的是当前用户的系统管理命令（cat、cp、ps等）。
- /etc：存放所有的系统管理所需的配置文件和子目录（例如人员的帐号密码文件，各种服务的起始文件等）。
- /tmp：存放一些临时文件，在系统重启时临时文件将被删除。
- /snap：Ubuntu 16.04及之后版本引入了snap包管理器，与之相关的目录、文件(包括安装文件)位于/snap中。
- /lost+found：该目录一般情况下是空的，当系统非法关机后会在该目录生成一些遗失的片段。
- /media：linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到该目录下。
- /srv：该目录存放一些服务启动之后需要提取的数据。
- /root：该目录为系统管理员用户主目录。
- /opt：该目录存放安装的第三方软件，如Oracle数据库就可以安装到该目录下。
- /mnt：挂载其他的文件系统(含硬盘分区)的目录。
- /lib64:类似lib目录，存放64位库文件。
- /srv：可以视作service的缩写，是一些网络服务启动后，这些服务需要取用的数据目录，常见的服务例如www,ftp等。
- /proc：这个目录本身是一个虚拟文件系统，它放置的数据都是在内存当中，不占用硬盘的容量。
- /sys：这个目录其实跟/proc非常的相似，也是一个虚拟的文件系统主要也是记录与内核相关的信息，不占用硬盘容量。
- /dev：在linux中任何的设备和接口设备都是以文件的形式存在于这个目录当中。你只要到通过访问这个目录下的某个文件就相当于访问某个设备。

<h3 id="linux系统种类">Linux系统种类</h3>

- 红帽企业版Linux：RedHat是全世界内使用最广泛的Linux系统。它具有极强的性能与稳定性，是众多生成环境中使用的（收费的）系统。
- Fedora：由红帽公司发布的桌面版系统套件，用户可以免费体验到最新的技术或工具，这些技术或工具在成熟后会被加入到RedHat系统中，因此Fedora也成为RedHat系统的试验版本。
- CentOS：通过把RedHat系统重新编译并发布给用户免费使用的Linux系统，具有广泛的使用人群。
- Deepin：在中国发行，对优秀的开源成品进行集成和配置。
- Debian：稳定性、安全性强，提供了免费的基础支持，在国外拥有很高的认可度和使用率。
- Ubuntu：是一款派生自Debian的操作系统，<font color=DeepSkyBlue>对新款硬件具有极强的兼容能力。Ubuntu与Fedora都是极其出色的Linux桌面系统，而且Ubuntu也可用于服务器领域</font>。
  
<h2 id="10.linux系统和windows系统的区别？">10.Linux系统和Windows系统的区别？</h2>

- Linux系统更稳定且有效率。
- Linux系统是免费（或少许费用），而Windows系统是商业化主导。
- Linux系统漏洞少且快速修补。
- Linux系统支持多用户同时使用计算机。
- Linux系统有更加安全的用户与文件权限策略。
- Linux系统可以访问源代码并根据用户的需要修改代码，而Windows系统不能访问源代码。
- Linux系统更能支持多种深度学习配套软件，但是windows系统能支持大量的视频游戏软件。

<h2 id="11.poc验证测试的概念">11.POC验证测试的概念</h2>

**POC（Proof of Concept）**，即概念验证。通常是企业进行产品选型时或开展外部实施项目前，进行的一种产品或供应商能力验证工作。主要验证内容：

1. 产品的功能。产品功能由企业提供，企业可以根据自己的需求提供功能清单，也可以通过与多家供应商交流后，列出自己所需要的功能。
2. 产品的性能。性能指标也是由企业提供，并建议提供具体性能指标所应用的环境及硬件设备等测试环境要求。
3. 产品的API适用性。
4. 产品相关技术文档的规范性、完整性。
5. 涉及到自定义功能研发的，还需要验证API开放性，供应商实施能力。
6. 企业资质规模及企业实施案例等。

<font color=DeepSkyBlue>验证内容归根结底，就是证明企业选择的产品或供应商能够满足企业提出的需求，并且提供的信息准确可靠</font>。

**POC测试工作的前提：**

1. 前期调研充分，并已经对产品或供应商有了比较深入的沟通了解。
2. 企业对自己的产品需求比较清晰。

**POC测试工作参与者：**

使用用户代表、业务负责人、项目负责人、技术架构师、测试工程师、商务经理等。

**POC测试工作准备文档：**

1. POC测试工作说明文档。内容包括测试内容、测试要求（如私有化部署）、测试标准、时间安排等。
2. 功能测试用例。主要确认功能可靠性，准确性。内容包括功能名称、功能描述等。
3. 场景测试用例。主要测试企业团队实施响应速度、实施能力、集成能力。这部分通常按照企业需求而定，不建议太复杂，毕竟需要供应商实施，拖的太久企业耐性受到影响，时间也会被拉长。
4. 技术测评方案。主要验证产品的性能、功能覆盖情况、集成效率、技术文档的质量。
5. 商务测评方案。主要包括企业实力、企业技术人才能力、版权验证、市场背景、产品报价等。

**POC测试工作的主要流程：**

**第一阶段：工作启动**

由商务或者对外代表对供应商发布正式邀请并附POC测试工作说明。

建立POC协同群。以满足快速沟通，应答。

涉及到私有化部署的，需要收集供应商部署环境要求，并与供应商一起进行部署工作，同时企业参与人员对部署工作情况做好记录。

**第二阶段：产品宣讲及现场集中测试**

供应商根据企业提供的POC测试工作说明及相应测试模块的用例或方案进行产品现场测试论证。

企业参与人员参与功能测试，并填写记录和意见。此阶段供应商往往需进行现场操作指导。

**第三阶段：技术测评**

供应商根据企业提供的技术要求给出相关支持技术文档，企业进行现场对比，根据实际情况进行统计记录。并保留供应商提供的资料和对比记录。

涉及到场景demo设计的，建议企业对实施人员能力、实施时长、实施准确性进行对比。

**第四阶段：间歇性测试工作**

该阶段是在第一阶段启动时，就可以开始了。测试功能外，还包括关键用户使用的体验心得、易用性评价。该部分允许企业用户主观评价，建议可以扩大范围组织间歇性测试，并做好测试用户记录。间歇时间1天或者多天根据实际情况安排。

**第五阶段：商务验证**

供应商根据企业提供的商务测评方案，积极配合工作。涉及到客户核实的，还需要企业进行考证。该部分工作也是从第一阶段启动时，就可以开始了。

**第六阶段：背书归档、分析总结**

每个阶段的工作都需要记录好参与人、时间、工作时间，并将测试过程中企业的、供应商的文档分类归档。对每个阶段进行分析对比，总结评价。最后进行整体工作分析总结。

<font color=DeepSkyBlue>POC工作按照不同企业和程度，测试的方式和投入力度不一样。但是目的都是相同的——验证产品或供应商能力是否满足企业需求</font>。
  
<h2 id="12.docker的相关概念及常用命令">12.Docker的相关概念及常用命令</h2>

<h3 id="docker简介">Docker简介</h3>

Docker是一个开源的应用容器引擎，基于Go语言并遵从Apache2.0协议开源。

Docker可以打包代码以及相关的依赖到一个轻量级、可移植的容器中，然后发布到任何流行的Linux机器上，也可以实现虚拟化。

容器完全使用沙箱机制，相互之间不会有任何接口（类似iPhone的app），更重要的是容器性能开销极低。

![Docker漫画](https://img-blog.csdnimg.cn/2020062418053921.png#pic_center)

Docker的应用场景：
1. Web应用的自动化打包和发布。
2. 自动化测试和持续集成、发布。
3. 在服务器环境中部署/调整数据库或其它的后台应用。

<h3 id="docker架构">Docker架构</h3>

Docker包括三个基本单元:

1. 镜像（Image）：Docker镜像（Image），就相当于是一个root文件系统。比如官方镜像ubuntu:16.04就包含了完整的一套Ubuntu16.04最简系统的root文件系统。
2. 容器（Container）：镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的类和实例一样，<font color=DeepSkyBlue>镜像是静态的定义，容器是镜像运行时的实体</font>。容器可以被创建、启动、停止、删除、暂停等。
3. 仓库（Repository）：仓库可看成一个代码控制中心，用来保存镜像。

<h3 id="docker容器使用">Docker容器使用</h3>

**Docker客户端**

Docker客户端非常简单，我们可以直接输入docker命令来查看到Docker客户端的所有命令选项。也可以通过命令docker command --help更深入的了解指定的Docker命令使用方法。

```bash
docker
```

**容器使用**

获取本地没有的镜像。如果我们本地没有我们想要的镜像，我们可以使用 docker pull 命令来载入镜像：

```bash
docker pull 镜像
```

启动容器。以下命令使用ubuntu镜像启动一个容器，参数为以命令行模式进入该容器：

```bash
docker run -it 镜像 /bin/bash
```
参数解释：

- -i：允许你对容器内的标准输入 (STDIN) 进行交互。
- -t：在新容器内指定一个伪终端或终端。
- /bin/bash：放在镜像名后的是命令，这里我们希望有个交互式Shell。

我们可以通过 <仓库名>:<标签> 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以latest作为默认标签。

要退出终端，直接输入exit或者CTRL+D。

启动已经停止运行的容器。查看所有的容器的命令如下：

```bash
docker ps -a
```

我们也可以用docker ps命令查看正在运行的容器。

```bash
docker ps
```

我们可以使用 docker start 启动一个已停止的容器：

```bash
docker start 容器
```

想要后台运行容器，我们可以过 -d 指定容器的运行模式：

```bash
docker run -itd --name 指定创建的容器名 容器 /bin/bash
```

加了 -d 参数默认不会进入容器，想要进入容器需要使用下面的指令进入容器：

- docker attach
- docker exec：推荐大家使用 docker exec 命令，因为使用此命令退出容器终端，不会导致容器的停止。

```bash
docker attach 容器  //如果从这个容器退出，会导致容器的停止。

docker exec -it 容器 /bin/bash   //如果从这个容器退出，不会导致容器的停止。
```

想要停止容器，其命令如下：

```bash
docker stop 容器ID
```

停止的容器重启命令：

```bash
docker restart 容器ID
```

删除容器：

```bash
docker rm -f 容器ID
```

<h3 id="docker镜像使用">Docker镜像使用</h3>

列出镜像列表。我们可以使用 docker images 来列出本地主机上的镜像。

```bash
docker images
```

各个参数解释：

- REPOSITORY：表示镜像的仓库源
- TAG：镜像的标签
- IMAGE ID：镜像ID
- CREATED：镜像创建时间
- SIZE：镜像大小

查找镜像：

```bash
docker search 镜像
```

各个参数解释：

- NAME: 镜像仓库源的名称
- DESCRIPTION: 镜像的描述
- OFFICIAL: 是否 docker 官方发布
- stars: 类似 Github 里面的 star，表示点赞、喜欢的意思。
- AUTOMATED: 自动构建。

删除镜像：

```bash
docker rmi 镜像
```

<h3 id="docker镜像的修改和自定义">Docker镜像的修改和自定义</h3>

**docker镜像的更新**

在启动docker镜像后，写入一些文件、代码、更新软件等等操作后，退出docker镜像，之后在终端输入如下命令：

```bash
docker commit -m="..." -a= "..." 容器ID 指定要创建的目标镜像名称
```

参数解释：

- commit：固定格式
- -m：提交的描述信息
- -a：指定镜像作者

接着可以用docker images查看镜像是否更新成功。（注意：不要创建名称已存在的镜像，这样会使存在的镜像名称为none，从而无法使用）

**镜像名称修改和添加新标签**

更改镜像名称（REPOSITORY）：

```bash
docker tag 容器ID 新名称
```

更改镜像tag，不修改名称：

```bash
docker tag IMAGEID(镜像id) REPOSITORY:TAG（仓库：标签）
```

<h3 id="docker容器和本机之间的文件传输">Docker容器和本机之间的文件传输</h3>

主机和容器之间传输文件的话需要用到容器的ID全称。

从本地传输到容器中：

```bash
docker cp 本地文件路径 容器name：/root/（容器路径）
```

从容器传输到本地上：

```bash
docker cp 容器name：/root/（容器路径） 本地文件路径
```

<h3 id="docker挂载宿主机文件目录">Docker挂载宿主机文件目录</h3>

docker可以支持把一个宿主机上的目录挂载到镜像的目录中。

在启动docker镜像时，输入如下命令：

```bash
docker run -it -v /宿主机绝对路径:/镜像内挂载绝对路径 容器REPOSITORY /bin/bash
```

通过-v参数，冒号前为宿主机目录，冒号后为镜像内挂载的路径，必须为绝对路径。

如果宿主机目录不存在，则会自动生成，镜像里也是同理。

默认挂载的路径权限为读写。如果指定为只读可以用：ro

```bash
docker run -it -v /宿主机绝对路径:/镜像内挂载绝对路径:ro 容器REPOSITORY /bin/bash
```

<h2 id="13.深度学习中常用的文件格式汇总">13.深度学习中常用的文件格式汇总</h2>

1. csv：可用于方便的存储数据与标签。
2. txt：最常见的文件格式，可用于存储数据路径与数据label。
3. Json：是一种轻量级的数据交换格式，常用于保存数据label。
4. Yaml：是一种数据序列化语言，通常用于编写配置文件，比如将网络模型配置参数与训练参数解耦至Yaml文件中，方便训练与优化。
5. Cfg：Darknet中经典的保存网络模型结构的文件格式。
6. Protobuf：是一个高效的结构化数据存储格式，可以存储神经网络的权重信息，在caffe中经常出现它的身影。
  
<h2 id="14.tcp和udp的区别？">14.TCP和UDP的区别？</h2>

1. TCP面向连接，UDP是无连接的；
2. TCP提供可靠的服务，也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达；UDP尽最大努力交付，即不保证可靠交付；
3. TCP的逻辑通信信道是全双工的可靠信道；UDP则是不可靠信道；
4. 每一条TCP连接只能是点到点的；UDP支持一对一，一对多，多对一和多对多的交互通信；
5. TCP面向字节流（可能出现黏包问题），本质上是TCP把数据看成一连串无结构的字节流；UDP是面向报文的（不会出现黏包问题）；
6. UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）；
7. TCP首部开销20字节；UDP的首部开销小，只需8个字节。

<h2 id="15.conda创建管理虚拟环境的命令大全">15.conda创建管理虚拟环境的命令大全</h2>

在AIGC时代，各种AI工作流飞速繁荣，我们要做好每个工作流与算法解决方案的环境配置与隔离，避免冲突，以免导致日常工作中的思维混乱。

Conda是一个主流的包管理器和环境管理器，常用于数据科学、机器学习和科学计算领域。使用Conda可以轻松地创建、管理和共享虚拟环境，下面是Conda在虚拟环境管理方面的一些常用命令：

### 创建虚拟环境

- **创建新的虚拟环境**：
  ```bash
  conda create -n myenv
  ```
  这会创建一个名为 `myenv` 的新虚拟环境。

- **指定Python版本**：
  ```bash
  conda create -n myenv python=3.8
  ```
  创建一个包含特定 Python 版本（例如 Python 3.8）的虚拟环境。

  - **指定环境保存地址**：
  ```bash
  conda create -p /本地路径/myenv python=3.8
  ```
  
### 激活和停用虚拟环境

- **激活虚拟环境**：
  ```bash
  conda activate myenv
  ```
  激活名为 `myenv` 的虚拟环境。

- **停用虚拟环境**：
  ```bash
  conda deactivate
  ```
  停用当前激活的虚拟环境。

### 管理虚拟环境中的包

- **安装包**：
  ```bash
  conda install numpy
  ```
  在当前激活的环境中安装包（例如安装 NumPy）。

- **列出环境中的包**：
  ```bash
  conda list
  ```
  显示当前环境中安装的所有包。

- **更新包**：
  ```bash
  conda update numpy
  ```
  更新当前环境中的特定包（例如 NumPy）。

- **卸载包**：
  ```bash
  conda remove numpy
  ```
  从当前环境中卸载特定的包。

### 管理环境

- **列出所有虚拟环境**：
  ```bash
  conda env list
  或者
  conda info --envs
  ```
  这些命令显示所有已创建的虚拟环境。

- **复制虚拟环境**：
  ```bash
  conda create -n myenv2 --clone myenv
  ```
  创建一个名为 `myenv2` 的新环境，这是 `myenv` 的完整副本。

- **删除虚拟环境**：
  ```bash
  conda remove -n myenv --all
  ```
  删除名为 `myenv` 的虚拟环境。

- **导出环境到文件**：
  ```bash
  conda env export > environment.yml
  ```
  将当前环境的所有包导出到 `environment.yml` 文件中，便于再现环境。

- **从文件创建环境**：
  ```bash
  conda env create -f environment.yml
  ```
  根据 `environment.yml` 文件创建虚拟环境。

这些命令为使用Conda管理虚拟环境提供了全面的控制，使得在不同项目之间隔离依赖关系和环境配置变得简单。这对于确保项目的可重现性和避免依赖冲突非常重要。

<h2 id="16.如何让两台服务器之间ssh免密通信？">16.如何让两台服务器之间ssh免密通信？</h2>

在Ubuntu系统中设置基于SSH密钥的认证，可以在两台服务器之间进行无密码连接。这对于自动化任务（如文件传输、备份和远程命令执行）特别有用。以下是设置SSH密钥认证的步骤：

1. 创建.ssh目录

假定有2台Linux服务器主机，分别为A，B。

我们先在所有服务器主机上创建ssh目录并赋予权限:
```bash
mkdir /root/.ssh 
chmod 700 /root/.ssh
```

2. 生成公钥与私钥

我们接着需要生成所有服务器主机的公钥与私钥，执行以下命令：

```bash
$ cd ~  # 进⼊入用户目录
$ ssh-keygen -t rsa -P ""  # 生成ssh密码，-t 参数表示生成算法，可以选择rsa和dsa；-P表示使用的密码，""表示无密码。
```

3. 将公钥追加authorized_keys文件中

将第一台服务器主机A上生成公钥追加到authorized_keys文件中:
```bash
$ cd ~/.ssh  # 进入.ssh目录
$ cat id_rsa.pub >> authorized_keys   # 将id_rsa.pub的内容追加到authorized_keys文件中
```

接下来我们使用同样的命令在服务器主机B上生成id_rsa.pub并写入到服务器主机A的authorized_keys文件中，再将服务器主机A的authorized_keys文件复制到服务器主机B的对应路径下即可：/root/.ssh/

<h2 id="17.什么是主机ip和BMC信息？">17.什么是主机ip和BMC信息？</h2>

### 主机IP和BMC信息

#### 主机IP
主机IP（Internet Protocol）地址是分配给每台连接到网络的设备的唯一标识符。IP地址用于在网络中标识和通信设备。IP地址有两种类型：
- **IPv4**：如`192.168.1.1`
- **IPv6**：如`2001:0db8:85a3:0000:0000:8a2e:0370:7334`

#### BMC（Baseboard Management Controller）
BMC是一个基板管理控制器，用于在不依赖操作系统的情况下管理和监控计算机系统。BMC常见于服务器中，提供远程管理功能，包括电源控制、系统监控、日志记录等。BMC通常有自己的IP地址，用于远程管理接口（如IPMI, Intelligent Platform Management Interface）。

### 获取主机IP和BMC信息

#### 在不同操作系统中获取主机IP

1. **Windows**
   - **获取主机IP**：
     1. 打开命令提示符（Win+R，输入`cmd`，按Enter）。
     2. 输入命令`ipconfig`并按Enter。
     3. 在输出中找到当前连接的网络适配器的IP地址，通常在`IPv4 Address`或`IPv6 Address`项下。
   - **示例**：
     ```
     C:\>ipconfig

     Windows IP Configuration

     Ethernet adapter Ethernet:

        Connection-specific DNS Suffix  . :
        Link-local IPv6 Address . . . . . : fe80::1c4b:aaaa:bbbb:cccc%12
        IPv4 Address. . . . . . . . . . . : 192.168.1.100
        Subnet Mask . . . . . . . . . . . : 255.255.255.0
        Default Gateway . . . . . . . . . : 192.168.1.1
     ```

2. **Linux**
   - **获取主机IP**：
     1. 打开终端。
     2. 输入命令`hostname -I`。
     3. 在输出中找到网络接口（如`eth0`、`wlan0`）的IP地址。
   - **示例**：
     ```
     $ hostname -I
     192.168.1.100
     ```

3. **macOS**
   - **获取主机IP**：
     1. 打开终端。
     2. 输入命令`ipconfig getifaddr en0`。
     3. 在输出中找到网络接口（如`en0`、`en1`）的IP地址。
   - **示例**：
     ```
     $ ipconfig getifaddr en0
     192.168.1.100
     ```
     
#### 获取BMC信息

BMC通常通过独立的管理接口（如IPMI）提供访问。要获取BMC的IP地址和其他信息，可以使用BMC管理工具或命令行工具。

1. **通过操作系统获取BMC IP**
   - **Windows/Linux**：
     1. 使用IPMI工具（如`ipmitool`）查询BMC信息。
     2. 安装`ipmitool`（Windows下需要额外下载并安装）。
     3. 执行命令获取BMC IP信息：
        ```bash
        ipmitool lan print 1
        ```
     - **示例输出**：
       ```
       Set in Progress         : Set Complete
       Auth Type Support       : MD2 MD5 PASSWORD
       Auth Type Enable        : Callback : MD2 MD5 PASSWORD
                               : User     : MD2 MD5 PASSWORD
                               : Operator : MD2 MD5 PASSWORD
                               : Admin    : MD2 MD5 PASSWORD
                               : OEM      : MD2 MD5 PASSWORD
       IP Address Source       : DHCP Address
       IP Address              : 192.168.1.50
       Subnet Mask             : 255.255.255.0
       MAC Address             : 00:25:90:ff:ff:ff
       ```
   - **使用BMC Web接口**：
     1. 登录BMC的Web管理界面（通常需要知道BMC IP地址）。
     2. 在网络设置或系统信息页面查找BMC的IP地址和其他信息。

2. **通过服务器BIOS获取BMC IP**
   - 重新启动服务器并进入BIOS设置。
   - 在BIOS中找到BMC或IPMI设置页面。
   - 查找和配置BMC的IP地址和网络设置。

### 总结

获取主机IP和BMC信息是系统管理和维护中的常见任务。不同操作系统提供了多种工具和命令来方便地获取这些信息。通过熟练掌握这些工具和命令，管理员可以有效地管理和监控服务器及其远程管理功能。

<h2 id="18.Linux中的find命令使用大全">18.Linux中的find命令使用大全</h2>

`find` 命令是 Linux 中非常强大的文件查找工具，适用于搜索目录树中的文件和目录。它支持多种搜索条件、动作和选项。以下是 `find` 命令的使用大全，包括常见的使用示例和解释。

### 基本用法
```bash
find [起始目录] [搜索条件] [操作]
```
- **起始目录**：指定搜索的起点目录。如果不指定，默认是当前目录。
- **搜索条件**：用于指定搜索的条件，如文件名、类型、大小等。
- **操作**：对找到的文件执行的操作，如打印、删除等。

### 常见搜索条件

#### 按文件名搜索
```bash
# 按名称精确匹配
find /path/to/start -name "filename"
# 按名称模糊匹配（大小写敏感）
find /path/to/start -name "*.txt"
# 按名称模糊匹配（大小写不敏感）
find /path/to/start -iname "*.txt"
```

#### 按文件类型搜索
```bash
# 查找目录
find /path/to/start -type d
# 查找普通文件
find /path/to/start -type f
# 查找符号链接
find /path/to/start -type l
```

#### 按文件大小搜索
```bash
# 查找大于 100MB 的文件
find /path/to/start -size +100M
# 查找小于 10KB 的文件
find /path/to/start -size -10k
# 查找正好 1GB 的文件
find /path/to/start -size 1G
```

#### 按文件时间搜索
```bash
# 查找在最近 7 天内修改的文件
find /path/to/start -mtime -7
# 查找在最近 30 分钟内修改的文件
find /path/to/start -mmin -30
# 查找在最近 7 天内访问的文件
find /path/to/start -atime -7
# 查找在最近 30 分钟内访问的文件
find /path/to/start -amin -30
```

#### 按文件权限搜索
```bash
# 查找权限为 755 的文件
find /path/to/start -perm 755
# 查找权限中包含执行权限的文件
find /path/to/start -perm /111
```

#### 按用户和组搜索
```bash
# 查找属于用户 "username" 的文件
find /path/to/start -user username
# 查找属于组 "groupname" 的文件
find /path/to/start -group groupname
```

### 常见操作

#### 打印文件路径
```bash
# 默认操作是打印文件路径
find /path/to/start -name "*.txt"
```

#### 删除文件
```bash
# 删除查找到的文件
find /path/to/start -name "*.tmp" -delete
```

#### 执行命令
```bash
# 对查找到的每个文件执行 ls -l 命令
find /path/to/start -name "*.txt" -exec ls -l {} \;
# 对查找到的每个文件执行 rm 命令
find /path/to/start -name "*.tmp" -exec rm -f {} \;
```

### 组合条件
```bash
# 查找 .txt 和 .log 文件
find /path/to/start \( -name "*.txt" -o -name "*.log" \)
# 查找大于 100MB 且在最近 7 天内修改的文件
find /path/to/start -size +100M -and -mtime -7
```

### 排除目录
```bash
# 查找过程中排除某个目录
find /path/to/start -path /path/to/exclude -prune -o -name "*.txt" -print
```

### 高级用法

#### 查找空文件和空目录
```bash
# 查找空文件
find /path/to/start -type f -empty
# 查找空目录
find /path/to/start -type d -empty
```

#### 查找符号链接
```bash
# 查找所有符号链接
find /path/to/start -type l
```

#### 查找最近修改的文件
```bash
# 查找最近修改的文件，并按时间排序
find /path/to/start -type f -printf '%T+ %p\n' | sort -r
```

<h2 id="19.通过Dockerfile构建镜像">19.通过Dockerfile构建镜像</h2>

### 什么是Dockerfile
dockerfile 是一种用于定义和构建 docker 镜像的文本文件。它包含一系列的指令和参数，用于描述镜像的构建过程，包括基础映像、软件包安装、文件拷贝、环境变量设置等。

通过编写 dockerfile，可以将应用程序、环境和依赖项打包成一个独立的容器镜像，使其可以在不同的环境和平台上运行，实现应用程序的可移植性和可扩展性。
### Dockerfile的基本结构
1) 基础镜像（Base Image）：使用 FROM 指令指定基础映像，作为构建镜像的起点。基础映像通常包含了操作系统和一些预装的软件和工具
2) 构建过程指令：使用一系列指令来描述构建过程，例如 RUN 用于执行命令和安装软件包，COPY 用于拷贝文件和目录，ADD 用于拷贝和提取文件，WORKDIR 用于设置工作目录等
3) 容器启动指令：使用 CMD 或 ENTRYPOINT 指令来定义容器启动时要执行的命令，也就是默认的容器执行命令

通过编写Dockerfile，可以自定义构建过程，选择所需的软件和配置，以及设置环境变量、暴露端口等。Dockerfile 的语法简单且易于理解，使得镜像的构建过程变得可重复和可维护。

Dockerfile 是定义和构建 docker 镜像的文本文件，通过编写指令和参数来描述镜像的构建过程和配置，以实现应用程序的打包和部署。它是使用 docker 进行容器化开发和部署的重要工具。

### 关键字
```
FROM         # 基础镜像，当前新镜像是基于哪个镜像的
MAINTAINER   # 镜像维护者的姓名混合邮箱地址
RUN          # 容器构建时需要运行的命令
EXPOSE       # 当前容器对外保留出的端口
WORKDIR      # 指定在创建容器后，终端默认登录的进来工作目录，一个落脚点
ENV          # 用来在构建镜像过程中设置环境变量
ADD          # 将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包
COPY         # 类似ADD，拷贝文件和目录到镜像中！
VOLUME       # 容器数据卷，用于数据保存和持久化工作
CMD          # 指定一个容器启动时要运行的命令，dockerFile中可以有多个CMD指令，但只有最后一个生效！
ENTRYPOINT   # 指定一个容器启动时要运行的命令！和CMD一样
ONBUILD      # 当构建一个被继承的DockerFile时运行命令，父镜像在被子镜像继承后，父镜像的ONBUILD被触发

```
### 示例(构建一个tensorrt镜像)
```

FROM nvcr.io/nvidia/tensorrt:22.10-py3

# install opencv 
RUN apt update && apt install libopencv-dev -y 

# install python essential dependencies
RUN pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 
RUN pip3 install numpy pillow matplotlib pycocotools opencv-python onnx onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple
```

<h2 id="20.docker-compose的使用">20.docker-compose的使用</h2>

### 介绍
Docker Compose是一个用来定义和运行复杂应用的Docker工具。一个使用Docker容器的应用，通常由多个容器组成。使用Docker Compose不再需要使用shell脚本来启动容器。 

Compose 通过一个配置文件来管理多个Docker容器，在配置文件中，所有的容器通过services来定义，然后使用docker-compose脚本来启动，停止和重启应用，和应用中的服务以及所有依赖服务的容器，非常适合组合使用多个容器进行开发的场景。

### 示例
```
version: '3.7'

services:
  yolov5-service:
    container_name: yolov5
    environment:
      - TZ=Asia/Shanghai
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    restart: "no"
    # 指定镜像 
    image: image镜像链接
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ gpu ]
    # command: "bash"
    # 服务自启动
    command: >
      bash -c "./start.sh"
    # 端口映射
    ports:
      - 8080:8888
    # 工作目录
    working_dir: /mnt/
    # 与宿主机目录挂载
    volumes:
      - /data/logs:/mnt/logs
    tty: true
    ipc: 'host'  # 使用共享内存的类型
```

### 启动
```
docker-compose up
```
执行命令后可启动一个名为yolov5的容器


<h2 id="21.CPU和GPU的区别？">21.CPU和GPU的区别？</h2>

CPU（中央处理单元）和GPU（图形处理单元）是计算机系统中两种非常重要的处理器，它们在设计、功能和使用场景上有明显的区别：

1. **基本功能和设计**：
   - **CPU**：设计为通用处理器，优化用于执行复杂的逻辑和控制任务。CPU通常有较少的核心（通常在4到32核之间），但每个核心的功能强大，能够处理多种类型的计算任务。
   - **GPU**：最初设计用于处理计算机图形和图像处理任务。GPU包含成百上千个小核心，这些核心能够并行处理大量相似的计算任务，非常适合于执行大规模的数值计算，如图形渲染或科学计算。

2. **性能和并行处理**：
   - **CPU**：强调每个核心的性能，更适合执行顺序指令和处理需要复杂决策和数据依赖性的任务。CPU更适合执行需要快速、复杂决策的应用程序，如运行操作系统、办公软件等。
   - **GPU**：设计用于同时执行大量比较简单的计算任务，非常适合于并行处理。因此，GPU在进行视频编辑、3D渲染、深度学习和大规模科学计算等任务时表现出较高的效率。

3. **应用场景**：
   - **CPU**：几乎出现在所有类型的计算设备中，是执行程序的主要硬件。它处理输入输出、系统管理、复杂运算以及与其他设备的通信等任务。
   - **GPU**：虽然最初主要用于图形相关的处理，但现在广泛用于科学计算、机器学习、大数据分析等领域，特别是在需要处理大量数据的并行计算中。

4. **发展趋势**：
   - **CPU**：近年来，CPU也在增加核心数，引入更高级的多任务处理和虚拟化技术，以提高性能和能效。
   - **GPU**：随着AIGC、传统深度学习、自动驾驶的持续发展，GPU的重要性日益增加，GPU制造商也在不断推出针对这些领域优化的产品。

总的来说，CPU更擅长处理需要较高逻辑复杂性和任务多样性的计算任务，而GPU则优于处理可大规模并行化的计算密集型任务。在现代计算系统中，CPU和GPU往往协同工作，提供更高效、更强大的计算能力。

<h2 id="22.两个Linux服务器之间数据传输的命令有哪些？">22.两个Linux服务器之间数据传输的命令有哪些？</h2>

在Linux系统中，有几种常用的命令可以用来在两台服务器之间传输数据。这些命令各有特点，适用于不同的场景和需求，Rocky下面为大家进行详细的总结与梳理：

1. **`scp` (Secure Copy Protocol)**
   - `scp` 命令是基于 SSH (Secure Shell) 协议的一种文件传输工具，它可以在两台服务器之间安全地复制文件或目录。由于`scp`使用SSH进行数据传输，所以它在传输过程中提供了通信加密，保证了数据的安全性和完整性。
   - 基本语法:
     ```bash
     scp [选项] 源文件 用户名@目标主机:目标路径
     scp [选项] 用户名@源主机:源文件路径 目标路径
     ```
   - 常用选项
     - `-P port`：指定SSH连接使用的端口。
     - `-p`：保留原文件的修改时间、访问时间和访问权限。
     - `-r`：递归复制整个目录。
     - `-q`：静默模式，不显示传输进度条和消息。
     - `-C`：开启压缩选项，传输时自动压缩数据。
   - 例子：
     - 将本地文件 `file.txt` 复制到远程服务器：
       ```bash
       scp file.txt username@remotehost:/path/to/destination/
       ```
     - 从远程服务器复制文件到本地：
       ```bash
       scp username@remotehost:/path/to/file.txt /local/destination/
       ```
     - 递归复制目录到远程服务器：
       ```bash
       scp -r /local/dir username@remotehost:/remote/dir
       ```
     - 指定SSH端口：
       ```bash
       scp -P 2222 file.txt username@remotehost:/path/
       ```
   - `scp` 支持递归复制目录（使用 `-r` 选项），指定端口（使用 `-P` 选项），以及更多功能。

   - 安全考虑

      由于`scp`依赖于SSH，它继承了SSH的所有安全特性，包括数据加密和用户身份验证。尽管如此，用户在使用`scp`时仍应注意：

      - 避免使用明文密码认证，应使用基于密钥的认证。
      - 确保SSH服务配置得当，如禁用root直接登录，使用强密码或密钥对等。
      - 注意检查目标主机的身份，以防止中间人攻击。

      `scp`虽然功能强大且安全，但由于其不支持同步更新（只能盲目复制），在需要高效同步文件或目录时，可能需要考虑使用`rsync`等工具。
  
2. **`rsync` (Remote Synchronization)**
   - `rsync` 是一个更为强大的数据同步工具，用于高效地同步文件和目录到不同的主机或本地文件系统。与 `scp` 相比，**`rsync` 最大的优势在于它能够进行增量备份，只复制变化的部分，大大提高了传输效率**。此外，`rsync` 还支持错误校正、无需开启远程 shell 用户的文件同步等功能。
   - 基本语法：
     ```bash
     rsync [选项] 源路径 目标用户@目标主机:目标路径
     rsync [选项] 目标用户@源主机:源路径 目标路径
     ```
   - 常用选项
      - `-a`（archive）: 归档模式，等同于 `-rlptgoD`，它包含了递归、保留符号链接、保留权限、保留时间戳、保留属主、保留组和保留设备文件（如果有必要）。
      - `-v`（verbose）: 输出详细信息，可以帮助调试。
      - `-u, --update`: 在复制文件时跳过那些在目标目录中已经存在且文件修改时间更新的文件。
      - `-z`（compress）: 数据传输过程中启用压缩。
      - `-h`（human-readable）: 输出易于阅读的格式。
      - `-n`（dry run）: 模拟运行，不进行实际的文件传输，常用于测试。
      - `--delete`: 删除目标目录中源目录不存在的文件，常用于镜像。
      - `-e`（executor）: 指定使用的远程 shell，通常是 `ssh`。
      - `--progress`: 显示进度条。
   - 例子：
     - 同步本地目录到远程服务器目录：
       ```bash
       rsync -avz /local/dir username@remotehost:/remote/dir
       ```
     - 从远程服务器同步目录到本地：
       ```bash
       rsync -avz username@remotehost:/remote/dir /local/dir
       ```
     - 使用非标准 SSH 端口：
       ```bash
       rsync -avz -e "ssh -p 2222" /local/dir/ username@remotehost:/remote/dir/
       ```
   - `rsync` 的 `-a` 选项代表归档模式，可保持所有权限等属性，`-v` 选项表示详细模式，`-z` 选项表示传输过程中进行压缩。

   - 安全考虑
      - 使用 `rsync` 时，最好通过 SSH 进行数据传输，这样可以保证数据在传输过程中的安全性。
      - 配置文件和权限应当谨慎设置，特别是在使用 `--delete` 选项时，因为这可能导致目标路径中的数据被删除。

  `rsync` 由于其灵活性和效率，是进行大规模文件同步和备份的首选工具。它的增量备份能力特别适合定期备份大数据量的场景。

3. **`sftp` (SSH File Transfer Protocol)**
   - `sftp` 是另一个基于 SSH 的文件传输协议，提供交互式的文件传输会话。
   - 基本使用：
     - 进入 `sftp` 会话：
       ```bash
       sftp username@remotehost
       ```
     - 在会话中，可以使用类似 `ftp` 的命令来上传或下载文件，如 `put`, `get`, `ls`, `cd` 等。

这些命令各有优势，选择哪个取决于我们的具体需求，如是否需要加密（`scp` 和 `rsync` 通过 SSH 提供加密），是否需要同步目录或仅传输单个文件，以及是否需要压缩等。在实际应用中，我们可以根据具体场景和需求灵活选择使用。


<h2 id="23.计算机中同步调用和异步调用有哪些区别？">23.计算机中同步调用和异步调用有哪些区别？</h2>

在计算机编程中，同步调用和异步调用是两种处理任务的方式，它们在处理任务的机制、使用场景和效率上有显著的区别。Rocky认为我们了解这两种调用方式的区别对于编写高效、响应迅速的程序非常重要。

### 同步调用

#### 定义
同步调用是一种阻塞的调用方式。在执行同步调用时，程序会等待调用的任务完成之后再继续执行后续的代码。也就是说，在同步调用中，任务是按顺序执行的，前一个任务完成后才会执行下一个任务。

#### 特点
1. **阻塞**：调用线程会被阻塞，直到任务完成。
2. **简单直观**：代码执行顺序与编写顺序一致，易于理解和调试。
3. **适用于简单任务**：在不需要并发处理的简单任务中，同步调用更直观。

#### 示例
假设我们有一个函数 `do_task`，需要执行两次任务，每次任务耗时2秒。

```python
import time

def do_task(task_name):
    print(f"Starting {task_name}")
    time.sleep(2)  # 模拟任务耗时
    print(f"Finished {task_name}")

# 同步调用
do_task("Task 1")
do_task("Task 2")
```

在上述代码中，`Task 2` 只有在 `Task 1` 完成后才会开始执行。

### 异步调用

#### 定义
异步调用是一种非阻塞的调用方式。在执行异步调用时，程序不必等待任务完成便可以继续执行后续的代码。异步调用通常使用回调函数、Promise（如 JavaScript 中的 Promise）、Future 或 async/await 机制来处理任务完成后的操作。

#### 特点
1. **非阻塞**：调用线程不会被阻塞，可以继续执行其他任务。
2. **并发处理**：可以同时处理多个任务，提高程序的响应速度和效率。
3. **更复杂**：异步编程相对复杂，需要处理回调、事件循环等机制。

#### 示例
使用 Python 中的 `asyncio` 库实现异步调用：

```python
import asyncio

async def do_task(task_name):
    print(f"Starting {task_name}")
    await asyncio.sleep(2)  # 模拟异步任务耗时
    print(f"Finished {task_name}")

# 异步调用
async def main():
    await asyncio.gather(
        do_task("Task 1"),
        do_task("Task 2")
    )

# 运行异步任务
asyncio.run(main())
```

在上述代码中，`Task 1` 和 `Task 2` 是同时启动的，并且在2秒后几乎同时完成。

### 同步与异步的区别总结

1. **执行顺序**：
   - **同步调用**：任务按顺序执行，前一个任务完成后才会执行下一个任务。
   - **异步调用**：任务可以同时启动，程序可以在等待任务完成期间执行其他操作。

2. **阻塞行为**：
   - **同步调用**：调用线程被阻塞，直到任务完成。
   - **异步调用**：调用线程不会被阻塞，可以继续执行其他任务。

3. **效率和响应速度**：
   - **同步调用**：适用于简单、顺序的任务，易于理解和调试。
   - **异步调用**：适用于需要并发处理的任务，提高程序的效率和响应速度，但编程复杂度较高。

4. **复杂度**：
   - **同步调用**：代码逻辑直观，容易调试和维护。
   - **异步调用**：需要处理回调、事件循环等机制，复杂度较高，但能显著提高性能。

### 使用场景

- **同步调用**：
  - 适用于简单的任务顺序执行，如读取本地文件、顺序执行的计算任务等。
  - 当程序不需要处理并发任务，且性能要求不高时，使用同步调用更为合适。

- **异步调用**：
  - 适用于需要高并发处理的场景，如网络请求、I/O 操作、并行计算等。
  - 当程序需要在等待某些任务（如网络请求、数据库查询）时继续执行其他任务，异步调用能够显著提高效率和响应速度。

理解同步调用和异步调用的区别，有助于我们在合适的场景下选择合适的编程模式，编写出高效、可靠的程序。


<h2 id="24.计算机中串行操作和并行操作有哪些区别？">24.计算机中串行操作和并行操作有哪些区别？</h2>

计算机中的串行和并行操作是两种处理任务的方式，它们在任务执行的顺序、效率和应用场景上有显著的区别。Rocky认为我们理解这两种操作模式有助于设计和优化计算机程序，以提高性能和效率。

### 串行操作

#### 定义
串行操作是一种按顺序执行任务的方式。每个任务必须在前一个任务完成之后才能开始执行，这意味着任务是一个接一个地进行，没有重叠。

#### 特点
1. **顺序执行**：任务按先后顺序逐一执行。
2. **简单直观**：代码逻辑清晰，容易编写和调试。
3. **无竞争条件**：因为只有一个任务在运行，不会发生资源竞争或冲突。

#### 示例
假设我们有三个任务：Task 1、Task 2 和 Task 3，它们必须按顺序执行。

```python
def task1():
    print("Executing Task 1")

def task2():
    print("Executing Task 2")

def task3():
    print("Executing Task 3")

# 串行执行
task1()
task2()
task3()
```

在上述代码中，Task 1 完成后才会开始 Task 2，然后是 Task 3。

#### 优缺点
- **优点**：
  - 实现简单，逻辑清晰。
  - 易于调试和维护。

- **缺点**：
  - 处理大量任务时效率低。
  - 不能充分利用多核处理器的计算能力。

### 并行操作

#### 定义
并行操作是一种同时执行多个任务的方式，通常利用多核处理器或多台计算机来同时处理多个任务，从而提高效率。

#### 特点
1. **同时执行**：多个任务可以同时开始并执行。
2. **复杂性**：代码逻辑复杂，需要处理同步、资源共享和竞争条件等问题。
3. **高效利用资源**：能够充分利用多核处理器和分布式计算资源。

#### 示例
假设我们有三个任务：Task 1、Task 2 和 Task 3，它们可以同时执行。我们可以使用 Python 的 `concurrent.futures` 模块来实现并行操作。

```python
import concurrent.futures

def task1():
    print("Executing Task 1")

def task2():
    print("Executing Task 2")

def task3():
    print("Executing Task 3")

# 并行执行
with concurrent.futures.ThreadPoolExecutor() as executor:
    futures = [executor.submit(task) for task in [task1, task2, task3]]
    for future in concurrent.futures.as_completed(futures):
        future.result()
```

在上述代码中，Task 1、Task 2 和 Task 3 会同时开始执行，利用线程池实现并行处理。

#### 优缺点
- **优点**：
  - 高效处理大量任务。
  - 能充分利用多核处理器和分布式计算资源，提高整体性能。

- **缺点**：
  - 实现复杂，需要处理同步和资源共享问题。
  - 存在资源竞争和死锁风险。

### 比较和应用场景

#### 1. 执行顺序
- **串行操作**：任务按顺序逐一执行。
- **并行操作**：多个任务可以同时执行。

#### 2. 适用场景
- **串行操作**：适用于简单的、无需高并发的任务，如顺序文件处理、简单的计算任务等。
- **并行操作**：适用于需要高并发处理的任务，如大规模数据处理、图像处理、网络请求等。

#### 3. 资源利用
- **串行操作**：通常只能利用单个处理器核，无法充分利用多核处理器的优势。
- **并行操作**：能够利用多核处理器或多台计算机，提高资源利用率和任务处理效率。

#### 4. 代码复杂度
- **串行操作**：实现简单，代码逻辑直观。
- **并行操作**：实现复杂，需要处理任务同步、资源共享和竞争条件等问题。

### 总结

串行操作和并行操作在计算机科学中不仅代表了两种不同的技术实现方式，更深层次上反映了不同的哲学思想：
- **时间观念**：线性 vs 非线性。
- **资源利用**：单任务专注 vs 多任务并行。
- **复杂性**：秩序和简单 vs 复杂性管理。
- **任务关系**：协作和依赖 vs 竞争和协同。


<h2 id="25.Linux中的tail命令使用大全">25.Linux中的tail命令使用大全</h2>

`tail` 命令是一个非常有用的命令行工具，用于查看文件的末尾部分，尤其是在AIGC、传统深度学习、自动驾驶领域中查看日志文件时。下面是 `tail` 命令在 Ubuntu（以及其他类Linux系统）中的各种用法及其选项的详细说明。

### 基本用法

- **查看文件的最后 10 行**（默认行为）：
  ```bash
  tail /path/to/your/file
  ```

- **指定行数**：使用 `-n` 选项来指定要查看的行数。
  ```bash
  tail -n 20 /path/to/your/file
  ```
  或者使用简短的 `-20` 形式：
  ```bash
  tail -20 /path/to/your/file
  ```

### 实时监控文件（AI行业高价值命令）

- **持续跟踪文件更新**：使用 `-f` 选项，`tail` 会显示文件的最后几行并在文件有新内容追加时，实时显示新增内容。这对于监控实时日志非常有用。
  ```bash
  tail -f /path/to/your/file
  ```

- **结合 `-n` 和 `-f`**：可以结合 `-n` 和 `-f` 选项，先显示文件的最后几行并持续跟踪文件更新。
  ```bash
  tail -n 20 -f /path/to/your/file
  ```

### 多文件查看

- **查看多个文件的尾部**：可以同时查看多个文件的尾部，`tail` 会在输出中显示文件名作为标识。
  ```bash
  tail -n 20 /path/to/your/file1 /path/to/your/file2
  ```

### 显示字节数

- **按字节显示**：使用 `-c` 选项按字节显示文件的末尾部分。
  ```bash
  tail -c 100 /path/to/your/file
  ```

### 持续监控文件变化并进行高级操作

- **附加模式**：使用 `--follow` 选项的 `name` 参数，可以在文件重命名或旋转（如日志文件轮转）后继续跟踪。
  ```bash
  tail --follow=name /path/to/your/file
  ```

- **与其他命令结合使用**：结合管道和其他命令进行更复杂的操作。例如，过滤实时日志输出中的某些关键字：
  ```bash
  tail -f /path/to/your/file | grep "keyword"
  ```

### 高级选项

- **从特定行开始显示**：使用 `+` 号表示从文件的第几行开始显示。
  ```bash
  tail -n +5 /path/to/your/file
  ```

- **使用 `--max-unchanged-stats` 选项**：设置 tail 在文件未变化时检查文件变化的最大次数。
  ```bash
  tail --max-unchanged-stats=5 -f /path/to/your/file
  ```
  
通过这些命令和选项，我们可以高效地查看和监控文件内容，特别是日志文件，帮助我们在AIGC、传统深度学习、自动驾驶等领域更好地进行系统管理、调试和故障排除中的获取和分析信息。


<h2 id="26.Linux中有哪些常用的查看文件夹占用空间的命令？">26.Linux中有哪些常用的查看文件夹占用空间的命令？</h2>

在AIGC、传统深度学习、自动驾驶领域，我们经常需要进行大规模的数据整理与迁移等工作，所以掌握Linux系统中的不同文件夹的占用空间，对我们日常的工作能够降本增效。下面是Linux系统中常用的查看文件夹占用空间的命令：

在 Linux 中，有多种命令可以用来查看文件夹占用的空间。以下是几种常用的方法和工具：

### 1. `du`（Disk Usage）

`du` 是一个非常强大的命令，用于检查文件和目录的磁盘使用情况。它的基本用法和常见选项如下：

#### 查看当前目录下的每个文件夹的大小

```bash
du -h --max-depth=1
```
- `-h`：以人类可读的格式显示（例如：K，M，G）。
- `--max-depth=1`：限制显示深度为1层，只显示当前目录下的文件夹大小。

#### 查看特定目录下的每个文件夹的大小

```bash
du -h --max-depth=1 /path/to/directory
```

#### 只显示总大小

```bash
du -sh /path/to/directory
```
- `-s`：只显示总计。

在 Ubuntu 系统中，你可以使用 `du`（disk usage）命令来查看每个文件夹占用的磁盘空间。以下是一些常见的方法和选项，可以帮助你获取所需的信息。

### 使用 `du` 命令查看每个文件夹的占用空间

#### 查看当前目录下的每个文件夹的大小

```bash
du -h --max-depth=1
```

- `-h`：以人类可读的格式显示（例如：K，M，G）。
- `--max-depth=1`：仅显示当前目录下的文件夹大小。

#### 查看特定目录下的每个文件夹的大小

例如，要查看 `/var` 目录下每个文件夹的大小，可以使用：

```bash
du -h --max-depth=1 /var
```

#### 查看所有子目录的大小

如果你想查看所有子目录的详细大小，可以省略 `--max-depth` 选项：

```bash
du -h /var
```

#### 只显示总大小

如果你只想查看某个目录的总大小，可以使用 `-s` 选项：

```bash
du -sh /var
```

#### 结合 `sort` 命令查看占用空间最大的文件夹

我们可以结合 `sort` 命令查看占用空间最大的文件夹：

```bash
du -h --max-depth=1 /home | sort -hr
```

- `sort -hr`：根据大小进行降序排序。

#### 示例解释

假设我们在 `/home` 目录下运行 `du -h --max-depth=1`，输出可能如下所示：

```bash
4.0K    ./WeThinkIn
16K     ./Documents
3.1G    ./Downloads
8.0K    ./Music
24K     ./Pictures
5.2M    ./Videos
3.2G    .
```

这里每一行表示每个子目录的大小，最后一行 `3.2G .` 表示当前目录（即 `/home` 目录）的总大小。

### 2. `ncdu`（NCurses Disk Usage）

`ncdu` 是一个基于文本的磁盘使用分析工具，提供了交互式的界面。

#### 安装 `ncdu`

```bash
sudo apt install ncdu  # 对于Debian/Ubuntu
sudo yum install ncdu  # 对于RHEL/CentOS
```

#### 使用 `ncdu`

```bash
ncdu /path/to/directory
```

### 3. `df`（Disk Free）

虽然 `df` 主要用于显示文件系统的总的磁盘空间使用情况，但也可以用来查看特定挂载点的使用情况。

#### 查看所有文件系统的使用情况

```bash
df -h
```
- `-h`：以人类可读的格式显示。

#### 查看特定目录的文件系统使用情况

```bash
df -h /path/to/directory
```

### 4. `duf`（Disk Usage/Free Utility）

`duf` 是一个现代的磁盘使用情况查看工具，具有彩色输出和交互式界面。

#### 安装 `duf`

```bash
sudo apt install duf  # 对于Debian/Ubuntu
sudo yum install duf  # 对于RHEL/CentOS
```

#### 使用 `duf`

```bash
duf
```

### 5. `ls` 和 `find` 配合使用

通过组合 `ls` 和 `find` 命令，可以获取每个文件夹和文件的大小。

#### 查看当前目录下所有文件和文件夹的大小

```bash
find . -type f -exec ls -lh {} + | awk '{print $9 ": " $5}'
```

### 6. `tree` 命令

`tree` 可以以树状结构显示目录内容，并提供每个文件和目录的大小。

#### 安装 `tree`

```bash
sudo apt install tree  # 对于Debian/Ubuntu
sudo yum install tree  # 对于RHEL/CentOS
```

#### 使用 `tree`

```bash
tree -h /path/to/directory
```
- `-h`：以人类可读的格式显示大小。


<h1 id='27.为什么有符号INT数据类型可表示的负数比正数多1？'>27.为什么有符号INT数据类型可表示的负数比正数多1？</h1>

#### 一句话总结

在有符号的整型数据类型中，用'0 00...0' 表示正负0； '1 00...0' 表示最小负数。

#### 原理解释

整型数据类型通常使用有符号表示法，其中最高有效位用作符号位。以INT8为例，符号位占据了第一位，而剩下的7位用于表示数值。因此，正数范围是从00 000000（二进制）到01111111（二进制），转换为十进制即为0到127。负数范围是从10000000（二进制）到11111111（二进制），转换为十进制即为-128到-1。这种表示方式称为二进制补码。

#### 二进制补码

**正数**：

- 1 $\rightarrow$ '0 0000001'

-  '0 0000001' $\rightarrow$ 判断为正数直接转化为1。

**负数**：

- -127 $\rightarrow$ 127原码， '0 1111111' $\rightarrow$ 取反，'1 0000000' $\rightarrow$ +1， '1 0000001' 

- '1 0000001'  $\rightarrow$ 判断为负数 $\rightarrow$取反， '0 1111110'  $\rightarrow$ +1，‘0 1111111'  $\rightarrow$ 结合判断得出值为-127


<h1 id='28.介绍一下Linux系统中的Shell脚本'>28.介绍一下Linux系统中的Shell脚本</h1>

**在Linux系统中，Shell脚本是一种非常强大的工具，在AIGC、传统深度学习、自动驾驶领域中主要用于自动化任务、管理系统和处理日常操作**。直观上看，**Shell脚本是一系列shell命令的集合，以脚本文件的形式存储，并通过shell解释器来执行**。下面是Rocky对Linux系统中shell脚本的详细讲解，包括基础知识、编写示例和一些高级用法：

### Shell脚本基础

#### 什么是Shell

Shell是用户与Linux操作系统之间的命令解释器，它可以执行命令、脚本和其他操作。常见的shell包括：
- **Bourne Shell (sh)**: /bin/sh
- **Bourne Again Shell (bash)**: /bin/bash
- **C Shell (csh)**: /bin/csh
- **Korn Shell (ksh)**: /bin/ksh
- **Z Shell (zsh)**: /bin/zsh

在AI领域中，主要使用/bin/sh和/bin/bash两种命令解释器。Rocky再讲一下两者的区别，以便大家更好的理解：
1. Bash是sh的超集，这意味着Bash包含了sh的所有功能，并且向后兼容sh脚本。使用sh编写的脚本几乎可以在Bash中不做修改地运行。
2. Bash提供了许多增强的交互功能，如命令行编辑（使用Emacs和Vi模式）、命令历史记录、Tab补全等。sh提供的交互功能较少，更适合编写简单的脚本而不是交互式使用。
3. Bash支持一系列高级编程特性，如数组、命令替换、进程替换、改进的变量替换、字符串操作等。sh提供的编程功能较为基本，主要用于简单的脚本和系统级任务。
4. sh通常比bash更轻量级，因为它的功能较少。因此，sh可能在一些简单的脚本中表现更快。bash提供了更多功能和灵活性，适用于更复杂的脚本和交互式使用，但在某些情况下可能比sh略慢。

#### 创建Shell脚本

1. **脚本文件**：创建一个以 `.sh` 结尾的文件，例如 `WeThinkIn.sh`。
2. **Shebang 行**：脚本文件的第一行通常是 `#!` 开头，指定要使用的解释器，例如第一行设置为 `#!/bin/bash` 时，在运行Shell脚本后，系统会读取 shebang 行，并使用 `/bin/bash` 来解释和执行脚本的内容。
3. **命令和逻辑**：在 shebang 行之后，我们需要编写要执行的shell命令和脚本逻辑。

### Shell脚本示例与运行

#### Shell脚本示例

```bash
#!/bin/bash

# 简单示例
echo "Hello, World!"

# 带变量的情况
name="Alice"

echo "Hello, $name!"

# 带条件的情况
if [ "$name" == "Alice" ]; then
    echo "Your name is Alice."
else
    echo "Your name is not Alice."
fi

# 带函数和循环的情况
greet() {
    echo "Hello, $1!"
}

for name in Alice Bob Charlie; do
    greet $name
done
```

#### 执行Shell脚本

1. **赋予执行权限**：使用 `chmod +x WeThinkIn.sh` 命令为脚本赋予执行权限。
2. **运行脚本**：通过 `sh WeThinkIn.sh` 、 `bash WeThinkIn.sh` 或者 `./WeThinkIn.sh` 命令运行脚本。

我们可以在命令行中通过 `sh WeThinkIn.sh` 执行刚才的Shell脚本示例，可以看到如下的输出结果：

```bash
Hello, World!
Hello, Alice!
Your name is Alice.
Hello, Alice!
Hello, Bob!
Hello, Charlie!
```

### Shell常用命令和特性

#### 设置变量和参数

- **定义变量**：`variable_name=value`
- **引用变量**：`$variable_name`
- **脚本参数**：`$0` 表示脚本名称，`$1, $2, ...` 表示脚本参数，`$#` 表示参数个数，`$@` 表示所有参数。

#### 条件语句

- **if语句**：

  ```bash
  if [ condition ]; then
      # do something
  elif [ condition ]; then
      # do something else
  else
      # do another thing
  fi
  ```

- **case语句**：

  ```bash
  case "$variable" in
      pattern1)
          # do something
          ;;
      pattern2)
          # do something else
          ;;
      *)
          # default case
          ;;
  esac
  ```

#### 循环

- **for循环**：

  ```bash
  for var in list; do
      # do something with $var
  done
  ```

- **while循环**：

  ```bash
  while [ condition ]; do
      # do something
  done
  ```

#### 函数

- **定义函数**：

  ```bash
  function_name() {
      # function body
  }
  ```

- **调用函数**：

  ```bash
  function_name argument1 argument2
  ```

### Shell高级用法

#### 输入输出重定向

- **重定向输出**：`command > WeThinkIn.txt` （内容覆盖文件），`command >> WeThinkIn.txt`（内容追加到文件）。
- **重定向输入**：`command < WeThinkIn.txt`。
- **管道**：`command1 | command2`。
下面Rocky对管道的用法进行举例，让大家更好的理解：
  ```bash
  ps aux | grep "python"
  ```
上面的命令表示将 `ps` 输出传递给 `grep` 以查找Python进程。

#### 错误处理

- **捕获错误**：在Shell脚本中添加 `set -e` 行，当脚本中发生任何命令的失败时退出脚本。
- **错误消息**：`command || echo "Command failed"`。

#### 调试脚本

- **开启调试模式**：

  1. **使用 `set -x` 和 `set +x` 命令**
     - `set -x`：开启调试模式。
     - `set +x`：关闭调试模式。

  2. **在 shebang 行中添加 `-x` 选项**
     - `#!/bin/bash -x`：直接在脚本的shebang行中添加 `-x` 选项，整个脚本都将处于调试模式。

- **示例脚本**：

  1.  **示例1：使用 `set -x` 和 `set +x`**

  ```bash
  #!/bin/bash
  # Script to demonstrate debugging

  echo "This is a test script."

  set -x  # 开启调试模式

  # 变量定义
  name="Alice"
  echo "Hello, $name!"

  # 条件语句
  if [ "$name" == "Alice" ]; then
      echo "Your name is Alice."
  else
      echo "Your name is not Alice."
  fi

  set +x  # 关闭调试模式

  echo "Script execution completed."
  ```

  执行这个脚本时，`set -x` 和 `set +x` 之间的部分会显示调试信息：

  ```bash
  This is a test script.
  + name=Alice
  + echo 'Hello, Alice!'
  Hello, Alice!
  + '[' Alice == Alice ']'
  + echo 'Your name is Alice.'
  Your name is Alice.
  Script execution completed.
  ```

  2. **示例2：在 shebang 行中添加 `-x` 选项**

  ```bash
  #!/bin/bash -x
  # Script to demonstrate debugging

  echo "This is a test script."

  # 变量定义
  name="Alice"
  echo "Hello, $name!"

  # 条件语句
  if [ "$name" == "Alice" ]; then
      echo "Your name is Alice."
  else
      echo "Your name is not Alice."
  fi

  echo "Script execution completed."
  ```

  执行这个脚本时，整个脚本都会显示调试信息：

  ```bash
  + echo 'This is a test script.'
  This is a test script.
  + name=Alice
  + echo 'Hello, Alice!'
  Hello, Alice!
  + '[' Alice == Alice ']'
  + echo 'Your name is Alice.'
  Your name is Alice.
  + echo 'Script execution completed.'
  Script execution completed.
  ```


<h1 id='29.介绍一下AI行业中规范的AI服务启动Shell脚本'>29.介绍一下AI行业中规范的AI服务启动Shell脚本</h1>

**在AI行业中，AIGC、传统深度学习、自动驾驶等核心领域一般都使用Shell脚本启动AI程序服务**。Shell脚本可以自动化AI程序的启动、停止和监控过程，使得对AI服务管理变得更加高效和可靠。以下是Rocky总结的编写规范Shell脚本的详细步骤，并对每个步骤进行深入浅出的讲解。

### 1. 先配置Shebang行和脚本说明

#### Shebang行
```sh
#!/bin/bash
```
- **解释**：`#!/bin/bash` 指定了脚本使用 `/bin/bash` 解释器执行。**Shebang行必须是脚本的第一行**。

#### 脚本说明
```sh
# This script is used to start the AI service.
# Author: WeTHinkIn
# Date: 2024-08-12
```
- **解释**：脚本头部的注释部分提供了脚本的基本信息，包括用途、作者和创建日期。这有助于后续的AI服务维护和扩展。

### 2. 设置环境变量

```sh
# 定义环境变量
export MODEL_DIR="/path/to/model"
export DATA_DIR="/path/to/data"
export LOG_DIR="/path/to/logs"
export CONFIG_FILE="/path/to/config.yaml"
export PYTHON_ENV="/path/to/virtualenv/bin/python"
```
- **解释**：环境变量用于存储配置文件、数据目录、日志目录等路径。使用 `export` 命令将这些变量导出，**使其在当前Shell脚本的所有子进程中都可见和可访问，这意味着当前脚本启动的任何其他脚本或命令中都能访问到这些变量**。

### 3. AI服务代码构建

#### 通过git相关命令获取最新AI服务代码
```sh
# 首先确定要获取的AI服务代码的分支：检查启动Shell脚本时是否提供了第一个命令行参数，如果没有提供，则将变量branch设置为默认值"master"；如果提供了第一个命令行参数，则将变量branch设置为该参数的值。
if [ -z "$1" ]; then
  branch="master"
else
  branch="$1"
fi

# 清理当前工作目录，获取制定分支的最新AI服务代码
cd /path/to/code_path
git pull
git reset
git checkout .
git clean -f
git checkout $branch
git pull

# 下面两个命令可以停止特定端口（比如8888）上的已有程序，为即将启动的AI程序服务提供空闲端口
lsof -i :8888 | grep LISTEN | awk '{print $2}' | xargs kill -9
# 或者
pkill -f 'python.*8888'

```
- **解释**：上面的AI服务代码构建命令中主要分成三步，分别是确定AI服务代码的分支；清理当前工作目录，获取最新AI服务代码；为AI服务代码提供空闲端口。

#### 启动AI程序服务
```sh
echo "Starting WeThinkIn AI service..."
nohup python /path/to/WeThinkIn.py --model_dir ${MODEL_DIR} --data_dir ${DATA_DIR} --config ${CONFIG_FILE} > ${LOG_DIR}/service.log 2>&1 &
echo "WeThinkIn AI service started. Logs are available at ${LOG_DIR}/service.log"
```
- **解释**：正式启动AI程序服务，**使用 `nohup` 和 `&` 将服务放入后台运行，并将输出和错误重定向到日志文件**。

### 4. 错误捕获和错误处理

#### 错误处理函数
```sh
handle_error() {
    echo "Error occurred in script at line: $1."
    exit 1
}
```
- **解释**：`handle_error` 函数打印错误信息并退出脚本。`$1` 是错误发生的行号。

#### 设置错误捕获
```sh
trap 'handle_error $LINENO' ERR
```
- **解释**：`trap` 命令捕获错误信号（ERR），并调用 `handle_error` 函数，传递错误发生的行号 `$LINENO`。

### 完整示例脚本

```sh
#!/bin/bash
# This script is used to start the AI service.
# Author: WeTHinkIn
# Date: 2024-08-12

# 如果出现报错，及时结束程序
set -e

# 定义环境变量
export MODEL_DIR="/path/to/model"
export DATA_DIR="/path/to/data"
export LOG_DIR="/path/to/logs"
export CONFIG_FILE="/path/to/config.yaml"
export PYTHON_ENV="/path/to/virtualenv/bin/python"

# 首先确定要获取的AI服务代码的分支：检查启动Shell脚本时是否提供了第一个命令行参数，如果没有提供，则将变量branch设置为默认值"master"；如果提供了第一个命令行参数，则将变量branch设置为该参数的值。
if [ -z "$1" ]; then
  branch="master"
else
  branch="$1"
fi

# 清理当前工作目录，获取制定分支的最新AI服务代码
cd /path/to/code_path
git pull
git reset
git checkout .
git clean -f
git checkout $branch
git pull

# 下面两个命令可以停止特定端口（比如8888）上的已有程序，为即将启动的AI程序服务提供空闲端口
lsof -i :8888 | grep LISTEN | awk '{print $2}' | xargs kill -9
# 或者
pkill -f 'python.*8888'

# 启动AI程序服务
echo "Starting WeThinkIn AI service..."
nohup python /path/to/WeThinkIn.py --model_dir ${MODEL_DIR} --data_dir ${DATA_DIR} --config ${CONFIG_FILE} > ${LOG_DIR}/service.log 2>&1 &
echo "WeThinkIn AI service started. Logs are available at ${LOG_DIR}/service.log"

# 错误捕获与错误处理
handle_error() {
    echo "Error occurred in script at line: $1."
    exit 1
}

trap 'handle_error $LINENO' ERR
```

通过以上步骤，我们就可以编写出一个结构清晰、功能完整的Shell脚本，用于启动AI程序服务。Shell脚本中要包括环境变量设置、AI服务代码获取、服务启动和错误处理等功能，来保证Shell脚本的可维护性和鲁棒性。


<h1 id='30.如何使用Git将AI项目切换到特定分支？'>30.如何使用Git将AI项目切换到特定分支？</h1>

在Git中，我们可以使用以下命令查看AI项目中有多少分支，并切换到特定的分支。以下是详细的步骤和命令解析：

### 1. 查看项目中的所有分支

要查看本地和远程的所有分支，我们可以使用以下命令：

#### 查看本地分支

```bash
git branch
```

这会列出所有本地分支，当前所在的分支会用 `*` 标记。

#### 查看远程分支

```bash
git branch -r
```

这会列出所有远程分支。

#### 查看本地和远程分支

```bash
git branch -a
```

这会列出所有本地和远程分支。

### 2. 切换到特定分支

要切换到特定的分支，我们可以使用 `git checkout` 或 `git switch` 命令。

#### 使用 `git checkout` 切换分支

```bash
git checkout <branch-name>
```

例如，切换到名为 `WeThinkIn` 的分支：

```bash
git checkout WeThinkIn
```

#### 使用 `git switch` 切换分支

```bash
git switch <branch-name>
```

例如，切换到名为 `WeThinkIn` 的分支：

```bash
git switch WeThinkIn
```

### 操作示例详解

假设我们在一个AI项目中，想要查看所有分支并切换到一个名为 `WeThinkIn` 的分支，以下是具体的操作步骤：

1. **查看本地分支**：

```bash
git branch
```

输出示例：

```bash
* master
  WeThinkIn
  feature-1
```

2. **查看远程分支**：

```bash
git branch -r
```

输出示例：

```bash
  origin/HEAD -> origin/master
  origin/WeThinkIn
  origin/feature-1
  origin/master
```

3. **查看所有分支**：

```bash
git branch -a
```

输出示例：

```bash
* master
  WeThinkIn
  feature-1
  remotes/origin/HEAD -> origin/master
  remotes/origin/develop
  remotes/origin/feature-1
  remotes/origin/master
```

4. **切换到 `WeThinkIn` 分支**：

```bash
git checkout WeThinkIn
```

或者使用 `git switch`：

```bash
git switch WeThinkIn
```


<h1 id='31.在AI项目中，从云端读取传输信息失败，显示网络不可用，一般可以怎么解决呢？'>31.在AI项目中，从云端读取传输信息失败，显示网络不可用，一般可以怎么解决呢？</h1>

当在AI项目中从云端读取或传输信息失败，并显示网络不可用时，可能有多种原因。**作为算法工程师，我们不需要知根知底关于网络方面的所有知识，但是我们需要知道常见的问题可能性，这样能够与网络部门的同事更好的沟通，同时知道网络端问题排查优化的相应成本，有利于整体AI项目的成本管理**。

以下是一些常见的网络不可用原因和相应的解决方案：

### 1. 检查网络连接

确保服务器或本地机器的网络连接正常。这是最基本的步骤，我们可以通过以下方式检查：

- **ping 命令**：检查与目标服务器的连接。
  ```bash
  ping www.WeThinkIn.com
  ```
- **curl 命令**：尝试从目标 URL 下载文件。
  ```bash
  curl -I https://www.WeThinkIn.com/path/to/image.jpg
  ```

### 2. 检查防火墙和安全组设置

如果我们使用的是云服务器（如 AWS、GCP、Azure），确保防火墙或安全组设置允许出站 HTTP/HTTPS 请求。

- **AWS 安全组**：在AWS管理控制台中，检查安全组的出站规则，确保允许端口 80（HTTP）和 443（HTTPS）。
- **本地防火墙**：检查本地机器的防火墙设置，确保允许 HTTP/HTTPS 流量。

### 3. 检查代理设置

有时项目组的网络或服务器环境可能需要通过代理访问。检查是否需要设置代理：

- **配置环境变量**：
  ```bash
  export http_proxy=http://proxy.WeThinkIn.com:port
  export https_proxy=https://proxy.WeThinkIn.com:port
  ```

### 4. 检查URL和权限

我们需要确保提供的URL正确，并且有权限访问该URL：

- **URL是否正确**：检查URL是否拼写正确。
- **权限问题**：如果URL需要身份验证，确保我们提供了正确的认证信息。

### 5. 重试机制

网络请求有时会因为临时问题失败，我们可以通过添加重试机制可以提高成功率。可以使用 `requests` 库的 `Retry` 机制：

```python
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

url = 'https://www.WeThinkIn.com/path/to/image.jpg'

# 创建一个 session
session = requests.Session()

# 定义重试策略
retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])

# 将重试策略应用到 HTTPAdapter
session.mount('http://', HTTPAdapter(max_retries=retries))
session.mount('https://', HTTPAdapter(max_retries=retries))

try:
    response = session.get(url, timeout=10)
    response.raise_for_status()  # 如果请求失败，抛出 HTTPError
    with open('image.jpg', 'wb') as f:
        f.write(response.content)
    print('Image downloaded successfully')
except requests.exceptions.RequestException as e:
    print(f'Error downloading image: {e}')
```

### 6. 检查云端服务状态

有时云端服务可能会有临时故障或维护，可以及时联系云服务提供商进行问题排查和解决。

### 7. 日志记录和错误处理

我们可以记录详细的错误日志，有助于诊断问题。确保捕获和记录所有可能的异常：

```python
import logging

logging.basicConfig(filename='download.log', level=logging.ERROR)

try:
    response = session.get(url, timeout=10)
    response.raise_for_status()
    with open('image.jpg', 'wb') as f:
        f.write(response.content)
    print('Image downloaded successfully')
except requests.exceptions.RequestException as e:
    logging.error(f'Error downloading image: {e}')
    print(f'Error downloading image: {e}')
```

<h1 id='32.docker中如何使用gpu？'>32.docker中如何使用gpu？</h1>

在启动容器的时候，使用--gpus参数时，需要安装nvidia-container-toolkit，才能在容器中使用显卡

官方安装命令：
```
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sed -i -e '/experimental/ s/^#//g' /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
```
一般在搜索该问题的时候，大部分都是如下命令：
```
curl -s -L https://nvidia.github.io/nvidia-container-runtime/gpgkey | \
  sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list
sudo apt-get update
apt-get install nvidia-container-runtime
```
或者
```
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \
  sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
sudo apt-get install -y nvidia-docker2
```
在ubuntu版本较高时，使用上述两种方法，会存在
E: 无法定位软件包 nvidia-container-runtime/ E: 无法定位软件包 nvidia-docker2这个问题，原因在添加的源里面没有对应的版本
建议使用英伟达官方安装命令
参考链接:https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html

<h1 id='33.docker中如何使用摄像头并显示画面？'>33.docker中如何使用摄像头并显示画面？</h1>

在docker中想要使用宿主机的摄像头时，需要在docker创建之初挂载宿主机的摄像头设备
使用--device参数进行挂载，--device=/dev/video0:/dev/video0，可挂载多个摄像头

显示画面
linux系统目前的主流图像界面服务X11支持 客户端/服务端（C/S）的工作模式，需要在容器启动的时候，将unix:端口或主机名:端口共享给docker，docker 就可以通过端口找到显示输出的地方，和linux系统共用显示接口。
1、安装xserver
```
sudo apt install x11-xserver-utils
```
2、设置权限
```
# 允许所有用户访问显示接口
xhost +
# 只允许Docker用户访问显示接口 (两者选其一即可)
xhost +local:docker 
```
3、运行docker镜像的时候，设置环境变量
```
#共享本地unix端口
-v /tmp/.X11-unix:/tmp/.X11-unix           
#修改环境变量DISPLAY
-e DISPLAY=unix$DISPLAY 
```
示例：
```
docker run -it -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=unix$DISPLAY <image_name>
```