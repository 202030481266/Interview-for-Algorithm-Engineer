# 简介
以各个大厂为维度，整理经典算法面试题，仅供参考，如有错误，欢迎指正！！ by 猫先生

# 目录

- [1.携程推荐算法面试题8道](#1.携程推荐算法面试题8道)
- [2.趣玩科技推荐算法面试题9道](#2.趣玩科技推荐算法面试题9道)
- [3.快手推荐算法面试题7道](#3.快手推荐算法面试题7道)


<h2 id="1.携程推荐算法面试题8道">1.携程推荐算法面试题8道</h2>

###  **问题1：讲一讲推荐系统包含哪些流程？**
推荐系统的流程通常包括以下几个步骤：
- **数据收集**：收集用户行为数据（如浏览记录、购买记录、点击记录等）和物品数据（如物品特征、分类、标签等）。
- **数据预处理**：对数据进行清洗、归一化、特征提取等预处理操作。
- **特征工程**：构建用户画像和物品画像，提取有助于推荐的特征。
- **模型选择**：选择合适的推荐算法，如基于内容的推荐、协同过滤、矩阵分解、深度学习等。
- **模型训练**：使用历史数据训练推荐模型。
- **推荐生成**：根据训练好的模型生成推荐列表。
- **评估与调优**：使用评价指标（如准确率、召回率、F1-score等）评估推荐效果，并进行模型调优。
- **上线与更新**：将推荐系统上线，并定期更新模型和数据。

### 问题2：**Transformer 位置编码是什么？**
Transformer中的位置编码（Positional Encoding）是为了弥补自注意力机制中缺乏顺序信息的缺陷。位置编码有两种常见方式：
- **固定位置编码**：如原始Transformer论文中使用的正弦和余弦函数。每个位置的编码是一个固定的向量，不随训练变化。
- **可训练位置编码**：将位置编码作为可训练的参数，与模型其他参数一起训练。

### 问题3：QKV 注意力公式为什么除以根号 d
除以根号 $\( \sqrt{d_k} \)$ 的原因是为了防止内积值过大导致softmax函数的梯度消失问题。由于Q和K的维度较高，其内积的值会随 $\( d_k \)$ 增加而增大，
从而导致softmax的输出极端化（接近0或1）。除以根号 $\( \sqrt{d_k} \)$ 可以使得内积值的方差接近1，保持梯度稳定。

### 问题4：简单讲讲 GCN
GCN（Graph Convolutional Network，图卷积网络）是一种用于图数据的神经网络。GCN通过在图的节点上进行卷积操作来提取节点的特征。基本的GCN操作步骤包括：

- **邻接矩阵**：用邻接矩阵 \( A \) 表示图结构。
- **节点特征矩阵**：用矩阵 \( X \) 表示节点特征。
- **卷积操作**：将节点特征与邻接矩阵进行卷积。

### 问题5：**简单讲讲RNN**

RNN（Recurrent Neural Network，循环神经网络）是一类用于处理序列数据的神经网络。RNN通过循环结构使得网络可以在序列的每个时间步共享参数，
从而记忆和处理序列中的上下文信息。RNN的基本结构包括：
- **输入层**：接收序列的当前时间步输入。
- **隐藏层**：通过循环连接，将前一时间步的隐藏状态和当前时间步的输入一起处理，生成当前时间步的隐藏状态。
- **输出层**：生成当前时间步的输出。

### 问题6：RNN 里的参数有什么特点？

RNN的参数具有共享性，即在序列的每个时间步使用相同的一组参数。这使得RNN能够有效处理不同长度的序列，并在参数数量固定的情况下学习序列中的时间依赖关系。

### 问题7：Dropout 是怎么做的？有什么作用？推理和训练时 Dropout 的区别？如果推理也用 dropout 会怎么样？

Dropout是一种正则化技术，通过在训练过程中随机丢弃（即设置为零）一部分神经元来防止过拟合。具体步骤为：

1. 在每一层的输出中，以一定的概率 $\( p \)$ 随机丢弃一些神经元。
2. 对保留的神经元进行放大 $( \frac{1}{1-p} \)$ ，以保持总的激活值不变。

作用：通过随机丢弃神经元，减少节点之间的相互依赖，从而提高模型的泛化能力。

推理和训练时 Dropout 的区别
- **训练时**：应用Dropout，即随机丢弃神经元，并对保留的神经元进行放大。
- **推理时**：不应用Dropout，使用完整的网络。

如果推理时也用Dropout，模型的输出将变得不稳定，因为每次推理时网络结构都不同，导致结果不可预测且精度下降。


### 问题8：讲讲 BN？BN 训练和推理什么区别？有什么用？

Batch Normalization（BN）是一种加速深层神经网络训练并提高其稳定性的方法。BN通过对每一层的输入进行标准化，使得输入具有零均值和单位方差，
同时允许网络学习最优的均值和方差。

训练和推理的区别

- **训练时**：使用mini-batch的均值和方差，并更新全局均值和方差的移动平均值。
- **推理时**：使用训练过程中计算的全局均值和方差。

作用

- 减少内部协变量偏移（Internal Covariate Shift）。
- 加快训练速度。
- 提高模型的泛化能力。


<h2 id="2.趣玩科技推荐算法面试题9道">2. 趣玩科技推荐算法面试题9道</h2>

### 问题1： 二分类的分类损失函数？
二分类的分类损失函数一般采用交叉熵（Cross Entropy）损失函数，即 CE 损失函数。二分类问题的 CE 损失函数可以写成： 

$$-y \log(p) - (1 - y) \log(1 - p)$$

其中， $y$ 是真实标签， $p$ 是预测标签，取值为0或1。
    
### 问题2：多分类的分类损失函数(Softmax)？
多分类问题一般采用交叉熵损失函数与Softmax激活函数结合使用。多分类问题的交叉熵损失函数可以写成： 

$$-\sum_{i=1}^{N} y_i \log(p_i)$$

其中， $N$ 是类别的数量， $y_i$ 是第 $i$ 类的真实标签， $p_i$ 是第 $i$ 类的预测概率。

### 问题3：关于梯度下降的sgdm,adagrad，介绍一下。
SGD（Stochastic Gradient Descent）是最基础的梯度下降算法，每次迭代随机选取一个样本计算梯度并更新模型参数。
SGDM（Stochastic Gradient Descent with Momentum）在 SGD 的基础上增加了动量项，可以加速收敛。
Adagrad（Adaptive Gradient）是一种自适应学习率的梯度下降算法，它根据每个参数的梯度历史信息调整学习率，可以更好地适应不同参数的变化范围。

### 问题4：为什么不用 MSE 分类用交叉熵？
MSE （ Mean Squared Error ）损失函数对离群点敏感，而交叉熵（Cross Entropy）损失函数在分类问题中表现更好，
因为它能更好地刻画分类任务中标签概率分布与模型输出概率分布之间的差异。

### 问题5：yolov5 相比于之前增加的特性有哪些？
YOLOv5 相比于之前版本增加了一些特性，包括：使用 CSP（Cross Stage Partial）架构加速模型训练和推理；采用 Swish 激活函数代替 ReLU；
引入多尺度训练和测试，以提高目标检测的精度和召回率；引入 AutoML 技术，自动调整超参数以优化模型性能。

### 问题6：可以介绍一下 attention 机制吗？
Attention 机制是一种用于序列建模的技术，它可以自适应地对序列中的不同部分赋予不同的权重，以实现更好的特征表示。
在 Attention 机制中，通过计算查询向量与一组键值对之间的相似度，来确定每个键值对的权重，最终通过加权平均的方式得到 Attention 向量。

### 问题7：关于 attention 机制，三个矩阵 Q,K,V的作用是什么？
在 Attention 机制中，KQV 是一组与序列中每个元素对应的三个矩阵，其中 K 和 V 分别代表键和值，用于计算对应元素的权重，Q 代表查询向量，
用于确定权重分配的方式。三个矩阵 K、Q、V 在 Attention 机制中的具体作用如下：
- K（Key）矩阵：K 矩阵用于计算每个元素的权重，是一个与输入序列相同大小的矩阵。通过计算查询向量 Q 与每个元素的相似度，
确定每个元素在加权平均中所占的比例。
- Q（Query）向量：Q 向量是用来确定权重分配方式的向量，与输入序列中的每个元素都有一个对应的相似度，可以看作是一个加权的向量。
- V（Value）矩阵：V 矩阵是与输入序列相同大小的矩阵，用于给每个元素赋予一个对应的特征向量。在 Attention 机制中，
加权平均后的向量就是 V 矩阵的加权平均向量。

通过K、Q、V三个矩阵的计算，Attention机制可以自适应地为输入序列中的每个元素分配一个权重，以实现更好的特征表示。

### 问题8：介绍一下文本检测 EAST？
EAST（Efficient and Accurate Scene Text）是一种用于文本检测的神经网络模型。EAST 通过以文本行为单位直接预测文本的位置、方向和尺度，
避免了传统方法中需要多次检测和合并的过程，从而提高了文本检测的速度和精度。EAST 采用了一种新的训练方式，即以真实文本行作为训练样本，
以减少模型对背景噪声的干扰，并在测试阶段通过非极大值抑制（NMS）算法进行文本框的合并。

### 问题9：编程题(讲思路)：给定两个字符串 s,t，在 s 字符串中找到包含 t 字符串的最小字符串。
给定两个字符串 s、t，可以采用滑动窗口的方式在 s 中找到包含 t 的最小子串。具体做法如下：
- 定义两个指针 left 和 right，分别指向滑动窗口的左右边界。
- 先移动 right 指针，扩展滑动窗口，直到包含了 t 中的所有字符。
- 移动 left 指针，缩小滑动窗口，直到无法再包含 t 中的所有字符。
- 记录当前滑动窗口的长度，如果小于之前记录的长度，则更新最小长度和最小子串。
- 重复（2）到（4）步骤，直到 right 指针到达 s 的末尾为止。


<h2 id="3.快手推荐算法面试题7道">3.快手推荐算法面试题7道</h2>

### 问题1：为什么 self-attention 可以堆叠多层，有什么作用？
   Self-attention 能够捕捉输入序列中的长距离依赖关系，通过堆叠多层 self-attention，模型可以学习序列中更深层次的模式和依赖关系。
   多层 self-attention 就像神经网络中的多个隐藏层一样，使模型能够学习和表示更复杂的函数。

### 问题2：多头有什么作用？如果想让不同头之间有交互，可以怎么做？
   多头注意力（Multi-head attention）的设计是为了让模型同时学习到输入序列的不同表示。每个“头”都有自己的参数，可以学习到不同的注意力分布，
   这样可以让模型同时关注不同的特征或信息。至于不同头之间的交互，这通常在所有头的输出被拼接和线性转换之后自然实现。如果你希望在这之前增加交互，
   你可能需要设计新的结构或者机制。

### 问题3：讲一讲多目标优化，MMoE 怎么设计？如果权重为 1,0,0 这样全部集中在某一个专家上该怎么办？
   多目标优化是指优化多个目标函数，通常需要在不同目标间找到一个权衡。多门专家混合网络（MMoE, Multi-gate Mixture-of-Experts）
   是一种处理多目标优化的方法，其中每个目标都由一个专家网络来处理，而门网络则决定每个专家对最终输出的贡献。如果权重全部集中在某一个专家上，
   那么模型的输出就完全由那个专家决定。这可能在某些情况下是合理的，但在大多数情况下，你可能希望各个专家都能对输出有所贡献，
   这需要通过训练和调整权重来实现。

### 问题4：介绍一下神经网络的优化器有哪些。
   常见的神经网络优化器有：
   - 梯度下降（GD）
   - 随机梯度下降（SGD）
   - 带动量的随机梯度下降（Momentum SGD）
   - Adagrad
   - RMSProp
   - Adam
   - Adadelta
   - Nadam 等

### 问题5：介绍一下推荐算法的链路流程。
   推荐系统通常包括以下步骤：
   - 数据收集（用户行为、物品信息等）
   - 特征工程
   - 模型选择和训练
   - 推荐列表生成
   - 排序等

### 问题6：介绍一下神经网络的初始化方法。
   常见的神经网络初始化方法有：
   - 零初始化（所有权重设为 0）
   - 随机初始化（权重随机设定，如高斯初始化或均匀分布初始化）
   - Xavier/Glorot 初始化（权重初始化为均值为 0，方差为 $\( \frac{1}{n} \)$ 的正态分布或均匀分布，其中 $\( n \)$ 为输入神经元的数量）
   - He 初始化（类似于 Xavier，但方差为 $\( \frac{2}{n} \)$，适用于 ReLU 激活函数）

### 问题7：讲一讲推荐算法序列建模的模型。
   推荐算法中的序列建模通常使用序列模型来捕捉用户行为的时间依赖性。常见的序列模型有：
   - RNN（如 LSTM 和 GRU）
   - 序列到序列模型（Seq2Seq）
   - 注意力模型（如 Transformer）
   - 预训练模型（如 BERT、GPT 等）

   这些模型可以处理用户行为序列，学习用户的历史行为对他们未来行为的影响，并据此进行推荐。