<h1 id="目录">目录</h1>

- [1.反向传播算法（BP）的概念及简单推导](#user-content-1反向传播算法（bp）的概念及简单推导)
- [2.滑动平均的相关概念](#user-content-2滑动平均的相关概念)
- [3.什么是模型微调(fine-tuning)?](#user-content-3什么是模型微调(fine-tuning)?)
- [4.简要介绍一下FLOPs](#user-content-4简要介绍一下FLOPs)
- [5.简要介绍一下FPS](#user-content-5简要介绍一下FPS)
- [6.介绍一下CNN模型和Transformer模型有哪些异同？](#user-content-6介绍一下CNN模型和Transformer模型有哪些异同？)
- [7.什么是PyTorch计算图？](#7.什么是PyTorch计算图？)
- [8.如何解决CNN的过拟合问题？](#如何解决CNN的过拟合问题？)
- [9.什么是感受野（ReceptiveField）？](#9.什么是感受野（ReceptiveField）？)

<h1 id="1反向传播算法（bp）的概念及简单推导">1.反向传播算法（BP）的概念及简单推导</h1>

<font color=DeepSkyBlue>反向传播（Backpropagation，BP）算法是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见算法</font>。BP算法对网络中所有权重计算损失函数的梯度，并将梯度反馈给最优化方法，用来更新权值以最小化损失函数。<font color=DeepSkyBlue>该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数</font>。

接下来我们以全连接层，使用sigmoid激活函数，Softmax+MSE作为损失函数的神经网络为例，推导BP算法逻辑。由于篇幅限制，这里只进行简单推导，后续Rocky将专门写一篇PB算法完整推导流程，大家敬请期待。

首先，我们看看sigmoid激活函数的表达式及其导数：

$$sigmoid表达式：\sigma(x) = \frac{1}{1+e^{-x}}$$
$$sigmoid导数：\frac{d}{dx}\sigma(x) = \sigma(x) - \sigma(x)^2 = \sigma(1- \sigma)$$

可以看到sigmoid激活函数的导数最终可以表达为输出值的简单运算。

我们再看MSE损失函数的表达式及其导数：

$$MSE损失函数的表达式：L = \frac{1}{2}\sum^{K}_{k=1}(y_k - o_k)^2$$

其中 $y_k$ 代表ground truth（gt）值， $o_k$ 代表网络输出值。

$$MSE损失函数的偏导：\frac{\partial L}{\partial o_i} = (o_i - y_i)$$

由于偏导数中单且仅当 $k = i$ 时才会起作用，故进行了简化。

接下来我们看看全连接层输出的梯度：

![](https://files.mdnice.com/user/33499/4f1b33bf-53c7-440e-811d-644c9956414a.png)

$$MSE损失函数的表达式：L = \frac{1}{2}\sum^{K}_{i=1}(o_i^1 - t_i)^2$$

$$MSE损失函数的偏导：\frac{\partial L}{\partial w_{jk}} = (o_k - t_k)o_k(1-o_k)x_j$$

我们用 $\delta_k = (o_k - t_k)o_k(1-o_k)$ ，则能再次简化：

$$MSE损失函数的偏导：\frac{dL}{dw_{jk}} = \delta_kx_j$$

最后，我们看看那PB算法中每一层的偏导数：

![](https://files.mdnice.com/user/33499/182e5d6f-711b-496f-86af-c86a8f135623.png)

输出层：
$$\frac{\partial L}{\partial w_{jk}} = \delta_k^K o_j$$
$$\delta_k^K = (o_k - t_k)o_k(1-o_k)$$

倒数第二层：
$$\frac{\partial L}{\partial w_{ij}} = \delta_j^J o_i$$
$$\delta_j^J = o_j(1 - o_j) \sum_{k}\delta_k^Kw_{jk}$$

倒数第三层：
$$\frac{\partial L}{\partial w_{ni}} = \delta_i^I o_n$$
$$\delta_i^I = o_i(1 - o_i) \sum_{j}\delta_j^Jw_{ij}$$

像这样依次往回推导，再通过梯度下降算法迭代优化网络参数，即可走完PB算法逻辑。

<h1 id="2滑动平均的相关概念">2.滑动平均的相关概念</h1>

滑动平均（exponential moving average），或者叫做指数加权平均（exponentially weighted moving avergae），可以用来估计变量的局部均值，<font color=DeepSkyBlue>使得变量的更新与一段时间内的历史取值有关</font>。

变量 $v$ 在 $t$ 时刻记为 $v_{t}$ ， $\theta_{t}$ 为变量 $v$ 在 $t$ 时刻训练后的取值，当不使用滑动平均模型时 $v_{t} = \theta_{t}$ ，在使用滑动平均模型后， $v_{t}$ 的更新公式如下：

![](https://img-blog.csdnimg.cn/20200805140509325.png#pic_center)

上式中， $\beta\epsilon[0,1)$ 。 $\beta = 0$ 相当于没有使用滑动平均。

$t$ 时刻变量 $v$ 的滑动平均值大致等于过去 $1/(1-\beta)$ 个时刻 $\theta$ 值的平均。并使用bias correction将 $v_{t}$ 除以 $(1 - \beta^{t})$ 修正对均值的估计。

加入Bias correction后， $v_{t}$ 和 $v_{biased_{t}}$ 的更新公式如下：

![](https://img-blog.csdnimg.cn/20200805140434908.png#pic_center)

当 $t$ 越大， $1 - \beta^{t}$ 越接近1，则公式（1）和（2）得到的结果（ $v_{t}$ 和 $v_{biased_{1}}$ ）将越来越接近。

当 $\beta$ 越大时，滑动平均得到的值越和 $\theta$ 的历史值相关。如果 $\beta = 0.9$ ，则大致等于过去10个 $\theta$ 值的平均；如果 $\beta = 0.99$ ，则大致等于过去100个 $\theta$ 值的平均。

下图代表不同方式计算权重的结果：

![](https://img-blog.csdnimg.cn/20200805141002734.png)

![](https://img-blog.csdnimg.cn/20200805141448823.png)

如上图所示，滑动平均可以看作是变量的过去一段时间取值的均值，<font color=DeepSkyBlue>相比对变量直接赋值而言，滑动平均得到的值在图像上更加平缓光滑，抖动性更小，不会因为某种次的异常取值而使得滑动平均值波动很大</font>。

**滑动平均的优势：** 占用内存少，不需要保存过去10个或者100个历史 $\theta$ 值，就能够估计其均值。滑动平均虽然不如将历史值全保存下来计算均值准确，但后者占用更多内存，并且计算成本更高。

**为什么滑动平均在测试过程中被使用？**

<font color=DeepSkyBlue>滑动平均可以使模型在测试数据上更鲁棒（robust）</font>。

采用随机梯度下降算法训练神经网络时，使用滑动平均在很多应用中都可以在一定程度上提高最终模型在测试数据上的表现。

训练中对神经网络的权重 $weights$ 使用滑动平均，之后在测试过程中使用滑动平均后的 $weights$ 作为测试时的权重，这样在测试数据上效果更好。因为滑动平均后的 $weights$ 的更新更加平滑，对于随机梯度下降而言，更平滑的更新说明不会偏离最优点很远。比如假设decay=0.999，一个更直观的理解，在最后的1000次训练过程中，模型早已经训练完成，正处于抖动阶段，而滑动平均相当于将最后的1000次抖动进行了平均，这样得到的权重会更加鲁棒。

<h1 id="3什么是模型微调(fine-tuning)?">3.什么是模型微调(fine-tuning)?</h1>

在AI行业中，模型微调（Fine-tuning）是一种基础有效的技术，特别适用于迁移学习场景，其中预训练模型的参数被稍作训练调整以适应新的、但与原始训练任务相似的任务。这种方法非常适合于数据量有限的情况，可以显著提高模型的性能和泛化能力。

### 模型微调的基本步骤：

1. **选择预训练模型**：
   - 开始微调之前，首先需要一个已经在相关任务上预训练好的模型，通常这些模型在大规模数据集（如ImageNet、Laion等）上进行预训练。因为这些模型已经学习到了丰富的特征表示，可以作为新任务的起点。

2. **初始化**：
   - 微调时，通常保留预训练模型的大部分或所有权重，作为新任务训练的初始化点。

3. **修改模型结构**：
   - 根据新任务的需求，可能需要对模型的最后几层进行修改。例如，在图像分类任务中，最后的全连接层（输出层）可能需要根据新任务的类别数进行调整。

4. **重新训练**：
   - 在新的数据集上继续训练模型。通常只需重新训练模型的一部分，特别是那些针对特定任务调整过的层，而其他层可以保持原始预训练时的参数或者以较小的学习率进行微调，以避免过度拟合。

5. **调整学习率**：
   - 微调时通常使用比原始预训练时更小的学习率，这有助于保持已经学习到的有用特征，并仅对它们进行精细的调整。

### 模型微调的应用场景：

- **AIGC**：AI绘画、AI视频、大模型、AI多模态、数字人、AI音频等。
- **传统深度学习**：图像分类、图像分割、目标检测、目标跟踪等。
- **自动驾驶**：车载图像分类、车载图像分割、车载目标检测等。

### 微调的好处：

- **加速训练**：由于模型从有效的初始状态开始学习，微调通常比从头开始训练快得多。
- **需要更少的数据**：微调可以在相对较少的数据上进行，因为模型已经从预训练中获得了大量的通用知识。
- **提高性能**：通过利用预训练模型的知识，可以提高模型在新任务上的表现，特别是当新任务的数据不足以从头开始训练复杂模型时。

总的来说，模型微调是一种高效利用已有知识以适应新任务的方法，特别适用于数据资源有限的场景。


<h1 id='4简要介绍一下FLOPs'>4.简要介绍一下FLOPs</h1>

首先注意FLOPs和FLOPS是有区别的：

FLOPS是指**每秒浮点运算次数**(Floating Point Operations per Second)，常用于评估硬件性能。

FLOPs是指**浮点运算次数**(Floating Point Operations)，常用于描述模型/算法的总计算量(复杂度)。

以矩阵乘法运算为例，矩阵$W \in \mathbb{R}^{M \times N}$，矩阵$A \in \mathbb{R}^{N \times K}$，二者相乘时，矩阵中每个元素会发生$N$次乘法运算和$N-1$次加法运算，那么FLOPs计算方式为：

$$FLOPs = M \times K \times N + M \times K \times (N-1)$$

计算FLOPs的工具有torchstat，ptflops等。


<h1 id='5简要介绍一下FPS'>5.简要介绍一下FPS</h1>

FPS，每秒帧数(Frame Per Second)，用于评估图像处理或模型推断速度的指标。

FPS表示在一秒内处理的图像帧数，其计算公式为：

$$FPS = \frac{1}{每帧数据所需处理时间}$$


<h1 id='6介绍一下CNN模型和Transformer模型有哪些异同？'>6.介绍一下CNN模型和Transformer模型有哪些异同？</h1>

CNN模型（Convolutional Neural Network）和Transformer模型是AI领域中两种常见的神经网络结构，广泛应用于AIGC、传统深度学习、自动驾驶等领域。下面Rocky为大家详细讲解它们的异同：

### CNN模型

#### 主要特点：
1. **局部连接（Local Connectivity）：** CNN利用卷积层中的滤波器（或称卷积核）在输入图像上进行滑动，以提取局部特征。每个滤波器只与输入的一小部分连接，这称为局部感受野。
   
2. **权重共享（Weight Sharing）：** 同一个滤波器在整个输入图像上滑动，并应用相同的权重。这减少了参数的数量，提高了模型的训练效率。

3. **平移不变性（Translation Invariance）：** 由于滤波器在图像上滑动，CNN能够很好地捕捉图像中的局部特征，并对平移变换具有鲁棒性。

4. **层次化特征表示（Hierarchical Feature Representation）：** CNN通过多个卷积层和池化层的叠加，逐步提取图像的低级特征（如边缘、角点）和高级特征（如复杂形状和对象）。

#### 应用：
- AIGC领域、传统深度学习领域、自动驾驶领域等。

### Transformer模型

#### 主要特点：
1. **自注意力机制（Self-Attention Mechanism）：** Transformer通过自注意力机制，可以对输入序列中的每个元素进行加权求和，从而捕捉全局依赖关系。注意力机制可以动态调整各个元素之间的权重。
   
2. **并行处理（Parallel Processing）：** 与RNN不同，Transformer不需要逐步处理序列数据，可以同时处理整个序列，利用并行计算加速训练。

3. **位置编码（Positional Encoding）：** 由于Transformer不具有序列的内在顺序信息，它通过位置编码将序列的位置信息显式地加入到输入中。

4. **层次化结构（Layered Architecture）：** Transformer通常由多个编码器层和解码器层堆叠而成，每一层包含多头自注意力机制和前馈神经网络。

#### 应用：
- AIGC领域、传统深度学习领域、自动驾驶领域等。

### 异同点对比

#### 相同点：
1. **深度学习框架：** 都是基于深度学习的神经网络模型。
2. **非线性激活函数：** 都使用非线性激活函数（如ReLU）来引入非线性特性。
3. **梯度下降优化：** 都使用反向传播和梯度下降方法来优化模型参数。

#### 不同点：
1. **结构设计：**
   - **CNN**：主要依靠卷积层和池化层，局部连接和权重共享是其核心特性。
   - **Transformer**：主要依靠自注意力机制和前馈神经网络，全局依赖关系和并行处理是其核心特性。
   
2. **处理类型：**
   - **CNN**：擅长捕捉局部和层次化的空间特征。
   - **Transformer**：擅长捕捉全局的序列依赖关系。
   
3. **计算效率：**
   - **CNN**：卷积操作计算高效。
   - **Transformer**：使用自注意力机制，计算复杂度较高。

4. **参数数量：**
   - **CNN**：由于权重共享，参数数量相对较少。
   - **Transformer**：由于自注意力机制的存在，参数数量较多。

### 总结
上面Rocky已经分析了CNN模型和Transformer模型的异同，我们还是需要根据实际场景，选择合适的模型架构作为AI产品和AI算法解决方案的技术工具。

<h1 id='7.什么是PyTorch计算图？'>7.什么是PyTorch计算图？</h1>

计算图是深度学习中的一个核心概念，它用于表示和跟踪数据在多个数学操作中的流动。在PyTorch中，这种结构被称为动态计算图，它与传统的静态计算图相区别，后者需要在执行任何计算前完全定义整个图的结构。

PyTorch的计算图是在运行时动态构建的。这意味着可以直接在代码中插入标准的Python控制流结构，如循环和条件语句，这些控制流可以无缝地融入到计算图中。这种方法提供了极大的灵活性，使得模型的实验和调试过程变得更简单和直观。

动态计算图的工作方式简述如下：当执行操作，如加法或乘法时，PyTorch会即时创建节点（代表操作）和边（代表数据流向），从而构建计算图。这种即时构建方式允许图结构随着代码执行而动态改变，非常适合于需要条件执行和循环迭代的复杂模型。

此外，动态计算图极大地简化了自动求导的实现。在PyTorch中，当执行例如反向传播的操作时，框架会自动计算所需的梯度，并通过已构建的计算图追踪每个操作的影响，从而精确地更新模型参数。

使用PyTorch的动态计算图时，需要注意一些关键的实践：

- **梯度清零**：在每个训练步骤开始前，使用`optimizer.zero_grad()`清除之前步骤中累积的梯度。
- **避免in-place操作**：如`x += 1`这样的操作可能会在不经意间修改数据，这可以破坏梯度流并导致错误。
- **使用`torch.no_grad()`**：在进行推理或评估模型时，应使用`torch.no_grad()`上下文管理器，以避免进行不必要的计算梯度操作，从而节省计算资源和内存。

通过掌握PyTorch的计算图，可以更有效地构建、调试和优化深度学习模型，充分利用其提供的灵活性和强大功能。


<h1 id="8.如何解决CNN的过拟合问题？">8.如何解决CNN的过拟合问题？</h1>
过拟合是机器学习中常见的问题，尤其是在深度学习模型，如CNN中。过拟合发生时，模型在训练数据上表现良好，但在未见过的数据上表现较差。为了减少过拟合，
可以采取以下策略：

- **数据增强（Data Augmentation）** ：通过旋转、平移、缩放、翻转等方法人为增加训练数据的多样性，帮助模型学习到更加泛化的特征。
- **Dropout**：在训练过程中随机“丢弃”（即设置为零）某些神经元的输出，这种方法能够有效减少模型对特定训练样本的依赖，增强模型的泛化能力。
- **正则化（Regularization）** ：向损失函数中添加正则项，如L1或L2正则化，限制模型权重的大小，防止模型过于复杂。
- **早停（Early Stopping）** ：在验证集上监控模型的性能，当模型的验证误差开始增加时停止训练，以避免过拟合。
- **使用预训练模型（Transfer Learning）** ：利用在大型数据集上预训练的模型作为初始模型，对特定任务进行微调，可以有效利用预训练模型的泛化能力，
减少过拟合风险。

<h1 id="9.什么是感受野（ReceptiveField）？">9.什么是感受野（ReceptiveField）？</h1>

在CNN中，**感受野（Receptive Field）** 是指卷积神经网络中某一层输出特征图上的一个元素对原始输入图像中区域的映射大小。
换句话说，它描述了输出特征图中单个元素视野范围内包含的输入图像的区域大小。

感受野的大小由网络中所有前面层的滤波器（卷积核）大小、步长和池化操作共同决定。**感受野越大**，网络能够捕获的输入图像的全局信息越多，
但同时可能会丢失一些细节信息；反之，**感受野较小**则能够捕获更多的局部细节信息。
