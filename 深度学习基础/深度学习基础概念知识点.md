# 目录
[1.反向传播算法（BP）的概念及简单推导](##1.反向传播算法（BP）的概念及简单推导)
[2.分组卷积的相关知识](##2.分组卷积的相关知识)
## 1.反向传播算法（BP）的概念及简单推导

<font color=DeepSkyBlue>反向传播（Backpropagation，BP）算法是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见算法</font>。BP算法对网络中所有权重计算损失函数的梯度，并将梯度反馈给最优化方法，用来更新权值以最小化损失函数。<font color=DeepSkyBlue>该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数</font>。

接下来我们以全连接层，使用sigmoid激活函数，Softmax+MSE作为损失函数的神经网络为例，推导BP算法逻辑。由于篇幅限制，这里只进行简单推导，后续Rocky将专门写一篇PB算法完整推导流程，大家敬请期待。

首先，我们看看sigmoid激活函数的表达式及其导数：

$$sigmoid表达式：\sigma(x) = \frac{1}{1+e^{-x}}$$
$$sigmoid导数：\frac{d}{dx}\sigma(x) = \sigma(x) - \sigma(x)^2 = \sigma(1- \sigma)$$

可以看到sigmoid激活函数的导数最终可以表达为输出值的简单运算。

我们再看MSE损失函数的表达式及其导数：

$$MSE损失函数的表达式：L = \frac{1}{2}\sum^{K}_{k=1}(y_k - o_k)^2$$

其中$y_k$代表ground truth（gt）值，$o_k$代表网络输出值。

$$MSE损失函数的偏导：\frac{\partial L}{\partial o_i} = (o_i - y_i)$$

由于偏导数中单且仅当$k = i$时才会起作用，故进行了简化。

接下来我们看看全连接层输出的梯度：

![](https://files.mdnice.com/user/33499/4f1b33bf-53c7-440e-811d-644c9956414a.png)

$$MSE损失函数的表达式：L = \frac{1}{2}\sum^{K}_{i=1}(o_i^1 - t_i)^2$$

$$MSE损失函数的偏导：\frac{\partial L}{\partial w_{jk}} = (o_k - t_k)o_k(1-o_k)x_j$$

我们用$\delta_k = (o_k - t_k)o_k(1-o_k)$，则能再次简化：

$$MSE损失函数的偏导：\frac{dL}{dw_{jk}} = \delta_kx_j$$

最后，我们看看那PB算法中每一层的偏导数：

![](https://files.mdnice.com/user/33499/182e5d6f-711b-496f-86af-c86a8f135623.png)

输出层：
$$\frac{\partial L}{\partial w_{jk}} = \delta_k^K o_j$$
$$\delta_k^K = (o_k - t_k)o_k(1-o_k)$$

倒数第二层：
$$\frac{\partial L}{\partial w_{ij}} = \delta_j^J o_i$$
$$\delta_j^J = o_j(1 - o_j) \sum_{k}\delta_k^Kw_{jk}$$

倒数第三层：
$$\frac{\partial L}{\partial w_{ni}} = \delta_i^I o_n$$
$$\delta_i^I = o_i(1 - o_i) \sum_{j}\delta_j^Jw_{ij}$$

像这样依次往回推导，再通过梯度下降算法迭代优化网络参数，即可走完PB算法逻辑。


## 2.分组卷积的相关知识

分组卷积（Group Convolution）最早出现在AlexNet网络中，分组卷积被用来切分网络，使其能在多个GPU上并行运行。

![分组卷积和普通卷积的区别](https://img-blog.csdnimg.cn/20201104211649626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JvY2t5NjY4OA==,size_16,color_FFFFFF,t_70#pic_center)

普通卷积进行运算的时候，如果输入feature map尺寸是$C\times H \times W$，卷积核有N个，那么输出的feature map与卷积核的数量相同也是N个，每个卷积核的尺寸为$C\times K \times K$，N个卷积核的总参数量为$N \times C \times K \times K$。

分组卷积的主要对输入的feature map进行分组，然后每组分别进行卷积。如果输入feature map尺寸是$C\times H \times W$，输出feature map的数量为$N$个，如果我们设定要分成G个group，则每组的输入feature map数量为$\frac{C}{G}$，则每组的输出feature map数量为$\frac{N}{G}$，每个卷积核的尺寸为$\frac{C}{G} \times K \times K$，卷积核的总数仍为N个，每组的卷积核数量为$\frac{N}{G}$，卷积核只与其同组的输入map进行卷积，卷积核的总参数量为$N \times \frac{C}{G} \times K \times K$，<font color=DeepSkyBlue>易得总的参数量减少为原来的$\frac{1}{G}$</font>。

**分组卷积的作用:**

1. 分组卷积可以减少参数量。
2. 分组卷积可以看成是稀疏操作，有时可以在较少参数量的情况下获得更好的效果（相当于正则化操作）。
3. 当分组数量等于输入feature map通道数量，输出feature map数量也等于输入feature map数量时，分组卷积就成了Depthwise卷积，可以使参数量进一步缩减。
