# 目录
- [1.全连接层的作用](#user-content-1全连接层的作用)

<h1 id="1全连接层的作用">1.全连接层的作用</h1>

全连接层将卷积学习到的高维特征映射到label空间，可以作为整个网络的<font color=DeepSkyBlue>分类器</font>模块。

虽然全连接层参数存在冗余的情况，但是在模型进行迁移学习时，其能保持较大的模型capacity。

目前很多模型使用全局平均池化（GAP）取代全连接层以减小模型参数，并且依然能达到SOTA的性能。

<h2 id="2.Pytorch中nn.Linear()和nn.Embedding()的区别是什么？">2.Pytorch中nn.Linear()和nn.Embedding()的区别是什么？</h2>

nn.Embedding()可以认为是一个查找表。

假设nn.Embedding(5,64)，即表示5个token的查找表，每个token映射为64维。当输入某个input_ids为3时，会把查找表中第4行(索引为3)向量取出来。

若换做nn.Linear(5, 64, bias=False)，则需要先将input_ids one-hot化，才能实现一样的效果，因为nn.Linear会将权重和输入做矩阵乘法。