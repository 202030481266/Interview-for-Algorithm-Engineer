# 目录

- [1.卷积有什么特点](#user-content-1卷积有什么特点)
- [2.不同层次的卷积都提取什么类型的特征](#user-content-2不同层次的卷积都提取什么类型的特征？)
- [3.卷积核大小如何选取](#user-content-3卷积核大小如何选取)
- [4.卷积感受野的相关概念](#user-content-4卷积感受野的相关概念)
- [5.网络每一层是否只能用一种尺寸的卷积核](#user-content-5网络每一层是否只能用一种尺寸的卷积核)
- [6.1*1卷积的作用](#user-content-61*1卷积的作用)
- [7.转置卷积的作用](#user-content-7转置卷积的作用)
- [8.空洞卷积的作用](#user-content-8空洞卷积的作用)

<h1 id="1卷积有什么特点">1.卷积有什么特点</h1>

卷积主要有**三大特点**：

1. <font color=DeepSkyBlue>局部连接</font>。比起全连接，局部连接会大大减少网络的参数。在二维图像中，局部像素的关联性很强，设计局部连接保证了卷积网络对图像局部特征的强响应能力。

2. <font color=DeepSkyBlue>权值共享</font>。参数共享也能减少整体参数量，增强了网络训练的效率。一个卷积核的参数权重被整张图片共享，不会因为图像内位置的不同而改变卷积核内的参数权重。

3. <font color=DeepSkyBlue>下采样</font>。下采样能逐渐降低图像分辨率，实现了数据的降维，并使浅层的局部特征组合成为深层的特征。下采样还能使计算资源耗费变少，加速模型训练，也能有效控制过拟合。

<h1 id="2不同层次的卷积都提取什么类型的特征">2.不同层次的卷积都提取什么类型的特征</h1>

1. 浅层卷积 $\rightarrow$ 提取边缘特征

2. 中层卷积 $\rightarrow$ 提取局部特征

3. 深层卷积 $\rightarrow$ 提取全局特征

<h1 id="3卷积核大小如何选取">3.卷积核大小如何选取</h1>

最常用的是$3\times3$大小的卷积核，两个$3 \times 3$卷积核和一个$5 \times 5$卷积核的感受野相同，但是减少了参数量和计算量，加快了模型训练。与此同时由于卷积核的增加，模型的非线性表达能力大大增强。

![](https://files.mdnice.com/user/33499/bedb2e10-0899-4577-b94c-8d83212bb8c4.png)


不过大卷积核（$7 \times 7，9 \times 9$）也有使用的空间，在GAN，图像超分辨率，图像融合等领域依然有较多的应用，大家可按需切入感兴趣的领域查看相关论文。

<h1 id="4卷积感受野的相关概念">4.卷积感受野的相关概念</h1>

目标检测和目标跟踪很多模型都会用到RPN层，anchor是RPN层的基础，而感受野（receptive field，RF）是anchor的基础。

**感受野的作用：**

1. 一般来说感受野越大越好，比如分类任务中最后卷积层的感受野要大于输入图像。

2. 感受野足够大时，被忽略的信息就较少。

3. 目标检测任务中设置anchor要对齐感受野，anchor太大或者偏离感受野会对性能产生一定的影响。

感受野计算：

![](https://files.mdnice.com/user/33499/b3f35f69-fc9a-4311-9aa7-a084baa3d9d3.png)

增大感受野的方法：

1. 使用空洞卷积

2. 使用池化层

3. 增大卷积核

<h1 id="5网络每一层是否只能用一种尺寸的卷积核">5.网络每一层是否只能用一种尺寸的卷积核</h1>

常规的神经网络一般每层仅用一个尺寸的卷积核，但同一层的特征图可以分别<font color=DeepSkyBlue>使用多个不同尺寸的卷积核，以获得不同尺度的特征</font>，再把这些特征结合起来，得到的特征往往比使用单一尺寸卷积核的要好，如GoogLeNet 、Inception系列的网络，均是每层使用了多个不同的卷积核结构。如下图所示，输入的特征图在同一层分别经过$1\times 1$，$3\times3$ 和$5\times5$三种不同尺寸的卷积核，再将各自的特征图进行整合，得到的新特征可以看作不同感受野提取的特征组合，相比于单一尺寸卷积核会有更强的表达能力。

![](https://files.mdnice.com/user/33499/255d01e9-1255-427b-9d85-600860d61d13.png)

<h1 id="61*1卷积的作用">6.1*1卷积的作用</h1>

$1 * 1$卷积的作用主要有以下几点：

1. 实现特征信息的交互与整合。

2. 对特征图通道数进行升维和降维，降维时可以减少参数量。

3. $1*1$卷积+ 激活函数 $\rightarrow$ 增加非线性，提升网络表达能力。


![升维与降维](https://files.mdnice.com/user/33499/2d53bb9a-32e5-4876-80a9-c4f1e1907721.png)


![1 * 1卷积在GoogLeNet中的应用](https://files.mdnice.com/user/33499/92cf4bfe-dcd5-4d2e-a976-36dcee368a7f.png)

$1 * 1$卷积首发于NIN（Network in Network），后续也在GoogLeNet和ResNet等网络中使用。感兴趣的朋友可追踪这些论文研读细节。

<h1 id="7转置卷积的作用">7.转置卷积的作用</h1>

转置卷积通过训练过程学习到最优的上采样方式，来代替传统的插值上采样方法，以提升图像分割，图像融合，GAN等特定任务的性能。

转置卷积并不是卷积的反向操作，从信息论的角度看，卷积运算是不可逆的。转置卷积可以将输出的特征图尺寸恢复卷积前的特征图尺寸，但不恢复原始数值。

**转置卷积的计算公式：**

我们设卷积核尺寸为$K\times K$，输入特征图为$i \times i$。

（1）当$stride = 1，padding = 0$时：

![](https://files.mdnice.com/user/33499/df26b9a5-8875-4ccc-96ef-bfb523942e9e.gif)

输入特征图在进行转置卷积操作时相当于进行了$padding = K - 1$的填充，接着再进行正常卷积转置之后的标准卷积运算。

输出特征图的尺寸 = $i + (K - 1)$

（2）当$stride > 1，padding = 0$时：

![](https://files.mdnice.com/user/33499/da594d05-2e5f-46c9-b2c3-81bd31f8961f.gif)

输入特征图在进行转置卷积操作时相当于进行了$padding = K - 1$的填充，相邻元素间的空洞大小为$stride - 1$，接着再进行正常卷积转置之后的标准卷积运算。

输出特征图的尺寸 = $stride * (i - 1) + K$

<h1 id="8空洞卷积的作用">8.空洞卷积的作用</h1>

空洞卷积的作用<font color=DeepSkyBlue>是在不进行池化操作损失信息的情况下，增大感受野，让每个卷积输出都包含较大范围的信息</font>。

空洞卷积有一个参数可以设置dilation rate，其在卷积核中填充dilation rate个0，因此，当设置不同dilation rate时，感受野就会不一样，也获取了多尺度信息。

![](https://files.mdnice.com/user/33499/03827ca6-6abb-4565-a924-91983bc5611d.png)

(a) 图对应3x3的1-dilated conv，和普通的卷积操作一样。(b)图对应$3\times3$的2-dilated conv，实际的卷积kernel size还是$3\times3$，但是空洞为$1$，也就是对于一个$7\times7$的图像patch，只有$9$个红色的点和$3\times3$的kernel发生卷积操作，其余的点的权重为$0$。(c)图是4-dilated conv操作。
