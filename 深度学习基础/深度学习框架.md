# 目录

- [1.Pytorch中的view、reshape方法的异同](#user-content-1.Pytorch中的view、reshape方法的异同)
- [2.PyTorch矩阵乘法详解](#2.PyTorch矩阵乘法详解)
- [3.PyTorch维度变化操作详解](#3.PyTorch维度变化操作详解)

<h1 id="1.Pytorch中的view、reshape方法的异同">1.Pytorch中的view、reshape方法的异同</h1>

## Pytorch官方文档的描述
![image.png](./Images/深度学习框架-image-1.png)
## 深入探究
要想深入理解view和reshape方法的区别，我们需要先知道Pytorch中的Tensor是如何储存的。
### Pytorch中Tensor的储存形式
Pytorch中tensor采用分开储存的形式，分为头信息区（Tensor）和存储区（Storage）。tensor的形状（size）、步长（stride）、数据类型（type）等信息储存在头部信息区，而真正的数据则存储在存储区。
![image.png](./Images/深度学习框架-image-2.png)
举个例子

```python
import torch
a = torch.arange(5) # 初始化张量 a 为 [0, 1, 2, 3, 4]
b = a[2:] # 截取张量a的部分值并赋值给b，b其实只是改变了a对数据的索引方式
print('a:', a)
print('b:', b)
print('ptr of storage of a:', a.storage().data_ptr()) # 打印a的存储区地址
print('ptr of storage of b:', b.storage().data_ptr()) # 打印b的存储区地址,可以发现两者是共用存储区
    
print('==================================================================')
    
b[1] = 0 # 修改b中索引为1，即a中索引为3的数据为0
print('a:', a)
print('b:', b)
print('ptr of storage of a:', a.storage().data_ptr()) # 打印a的存储区地址
print('ptr of storage of b:', b.storage().data_ptr()) # 打印b的存储区地址，可以发现两者是共用存储区
    
    
''' 运行结果 '''
a: tensor([0, 1, 2, 3, 4])
b: tensor([2, 3, 4])
ptr of storage of a: 2862826251264
ptr of storage of b: 2862826251264
==================================================================
a: tensor([0, 1, 2, 0, 4])
b: tensor([2, 0, 4])
ptr of storage of a: 2862826251264
ptr of storage of b: 2862826251264
```
以发现a、b这两个tensor的Storage都是一样的，但它们的头信息区不同。
### Pytorch中Tensor的stride属性
官方文档描述：stride是在指定维度dim中从一个元素跳到下一个元素所必需的步长。
举个例子
```python
import torch
x = torch.tensor([[1, 3, 5, 7], [7, 7, 7, 7]])
print(x)
print(x.stride(0))  # 打印第0维度中第一个元素到下一个元素的步长
print(x.stride(1))   # 打印第1维度中第一个元素到下一个元素的步长

''' 运行结果 '''
tensor([[1, 3, 5, 7],
        [7, 7, 7, 7]])
4
1
```
![image.png](./Images/深度学习框架-image-3.png)
### view方法的限制
view方法能够将tensor转换为指定的shape，且原始的data不改变。返回的tensor与原始的tensor共享存储区。但view方法需要满足以下连续条件：
$\operatorname{stride}[i]=\text { stride }[i+1] \times \operatorname{size}[i+1]$
### 连续条件的理解
举个例子，我们初始化一个tensor a与b
```python
import torch
a = torch.arange(9).reshape(3, 3)  # 初始化张量a
b = a.permute(1, 0)  # 令b等于a的转置
print(a)   # 打印a
print(a.size())  # 查看a的shape
print(a.stride())  # 查看a的stride
print('==================================================================')
print(b)  # 打印b
print(b.size())  # 查看b的shape
print(b.stride())  # 查看b的stride

''' 运行结果 '''
tensor([[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])
torch.Size([3, 3])
(3, 1)
==================================================================
tensor([[0, 3, 6],
        [1, 4, 7],
        [2, 5, 8]])
torch.Size([3, 3])
(1, 3)
```
我们将tensor a与b分别带入连续性条件公式进行验证，发现a可以满足而b不满足，下面我们尝试对tensor a与b进行view操作
```python
import torch
a = torch.arange(9).reshape(3, 3)  # 初始化张量a
b = a.permute(1, 0)  # 令b等于a的转置
print(a.view(-1))

''' 运行结果 '''
tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])
```
```python
import torch
a = torch.arange(9).reshape(3, 3)  # 初始化张量a
b = a.permute(1, 0)  # 令b等于a的转置
print(b.view(-1))

''' 运行结果 '''
Traceback (most recent call last):
  File "C:/Users/97987/PycharmProjects/pytorch/test.py", line 4, in <module>
    print(b.view(-1))
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
```
果然只有在满足连续性条件下才可以使用view方法。
如果不满足此条件，则需要先使用contiguous方法将原始tensor转换为满足连续条件的tensor，然后再使用view方法进行shape变换。但是经过contiguous方法变换后的tensor将重新开辟一个储存空间，不再与原始tensor共享内存。
```python
import torch
a = torch.arange(9).reshape(3, 3)  # 初始化张量a
b = a.permute(1, 0)  # 令b等于a的转置
c = b.contiguous()  # 使用contiguous方法
print(c.view(-1))
print(a.storage().data_ptr())
print(b.storage().data_ptr())
print(c.storage().data_ptr())

''' 运行结果 '''
tensor([0, 3, 6, 1, 4, 7, 2, 5, 8])
2610092185792
2610092185792
2610092184704
```
从以上结果可以看到，tensor a与c是属于不同存储区的张量，也就是说经过contiguous方法变换后的tensor将重新开辟一个储存空间，不再与原始tensor共享内存。
### reshape方法
与view方法类似，将输入tensor转换为新的shape格式，但是reshape方法是view方法与contiguous方法的综合。
也就是说当tensor满足连续性条件时，reshape方法返回的结果与view方法相同，否则返回的结果与先经过contiguous方法在进行view方法的结果相同。
## 结论
view方法和reshape方法都可以用来更改tensor的shape，但view只适合对满足连续性条件的tensor进行操作，而reshape同时还可以对不满足连续性条件的tensor进行操作，兼容性更好，而view方法可以节省内存，如果不满足连续性条件使用reshape方法则会重新开辟储存空间。



<h1 id="2.PyTorch矩阵乘法详解">2.PyTorch矩阵乘法详解</h1>

PyTorch作为深度学习领域的主流框架之一,提供了多种矩阵乘法操作。本文将详细介绍PyTorch中的各种矩阵乘法函数,帮助您在不同场景下选择最适合的方法。

## 1. torch.matmul()

`torch.matmul()`是PyTorch中最通用的矩阵乘法函数,可以处理多维张量。

### 特点:
- 支持广播机制
- 可以处理1维到4维的张量
- 根据输入张量的维度自动选择适当的乘法操作

### 示例:
```python
import torch

a = torch.randn(2, 3)
b = torch.randn(3, 4)
c = torch.matmul(a, b)  # 结果形状为 (2, 4)

# 也可以用@运算符
c = a @ b
```

## 2. torch.mm()

`torch.mm()`专门用于2维矩阵相乘。

### 特点:
- 只能处理2维矩阵
- 比`torch.matmul()`在某些情况下更快

### 示例:
```python
a = torch.randn(2, 3)
b = torch.randn(3, 4)
c = torch.mm(a, b)  # 结果形状为 (2, 4)
```

## 3. torch.bmm()

`torch.bmm()`用于批量矩阵乘法,处理3维张量。

### 特点:
- 输入必须是3维张量
- 用于同时计算多个矩阵乘法

### 示例:
```python
a = torch.randn(10, 3, 4)
b = torch.randn(10, 4, 5)
c = torch.bmm(a, b)  # 结果形状为 (10, 3, 5)
```

## 4. @运算符

Python 3.5+引入的矩阵乘法运算符,在PyTorch中也可使用。

### 特点:
- 语法简洁
- 功能等同于`torch.matmul()`

### 示例:
```python
a = torch.randn(2, 3)
b = torch.randn(3, 4)
c = a @ b  # 结果形状为 (2, 4)
```

## 5. torch.dot()

`torch.dot()`计算两个一维张量的点积。

### 特点:
- 只能用于1维张量
- 返回一个标量

### 示例:
```python
a = torch.randn(5)
b = torch.randn(5)
c = torch.dot(a, b)  # 结果是一个标量
```

## 6. torch.mv()

`torch.mv()`用于矩阵与向量相乘。

### 特点:
- 第一个参数必须是2维矩阵
- 第二个参数必须是1维向量

### 示例:
```python
matrix = torch.randn(3, 4)
vector = torch.randn(4)
result = torch.mv(matrix, vector)  # 结果形状为 (3,)
```

## 7. torch.einsum()

`torch.einsum()`使用爱因斯坦求和约定,可以执行更复杂的张量运算,包括矩阵乘法。

### 特点:
- 非常灵活,可以表达复杂的张量运算
- 语法简洁但可能难以理解

### 示例:
```python
a = torch.randn(2, 3)
b = torch.randn(3, 4)
c = torch.einsum('ij,jk->ik', a, b)  # 等同于矩阵乘法,结果形状为 (2, 4)
```

## 总结

PyTorch提供了多种矩阵乘法操作,适用于不同的场景:
- 对于一般情况,使用`torch.matmul()`或`@`运算符
- 对于2维矩阵乘法,可以使用`torch.mm()`
- 对于批量矩阵乘法,使用`torch.bmm()`
- 对于向量点积,使用`torch.dot()`
- 对于矩阵与向量相乘,使用`torch.mv()`
- 对于更复杂的张量运算,可以考虑`torch.einsum()`

选择合适的函数可以提高代码的可读性和运行效率。在实际应用中,建议根据具体情况选择最合适的方法。

<h1 id="3.PyTorch维度变化操作详解">3.PyTorch维度变化操作详解</h1>

PyTorch作为深度学习领域的主流框架，提供了丰富的维度变化操作。这些操作在数据预处理、模型构建和结果处理中都扮演着重要角色。本文将详细介绍PyTorch中的各种维度变化操作，帮助您更好地理解和使用这些功能。

## 1. view() 和 reshape()

这两个函数用于改变张量的形状，但不改变其数据。

### view()
- 要求张量在内存中是连续的
- 不会复制数据，只是改变视图

```python
import torch

x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  # -1表示这个维度的大小将被自动计算
```

### reshape()
- 类似于view()，但可以处理非连续的张量
- 如果可能，不会复制数据

```python
a = torch.randn(4, 4)
b = a.reshape(2, 8)
```

## 2. squeeze() 和 unsqueeze()

这对函数用于移除或添加维度。

### squeeze()
移除大小为1的维度

```python
x = torch.zeros(2, 1, 3, 1, 4)
y = x.squeeze()  # y.shape: (2, 3, 4)
z = x.squeeze(1)  # z.shape: (2, 3, 1, 4)
```

### unsqueeze()
在指定位置添加大小为1的维度

```python
x = torch.tensor([1, 2, 3])
y = x.unsqueeze(0)  # y.shape: (1, 3)
z = x.unsqueeze(1)  # z.shape: (3, 1)
```

## 3. transpose() 和 permute()

这两个函数用于交换维度。

### transpose()
交换两个指定的维度

```python
x = torch.randn(2, 3, 5)
y = x.transpose(0, 2)  # y.shape: (5, 3, 2)
```

### permute()
可以对任意维度进行重新排列

```python
x = torch.randn(2, 3, 5)
y = x.permute(2, 0, 1)  # y.shape: (5, 2, 3)
```

## 4. expand() 和 repeat()

这两个函数用于扩展tensor的大小。

### expand()
- 不会分配新内存，只是创建一个新的视图
- 只能扩展大小为1的维度

```python
x = torch.tensor([[1], [2], [3]])
y = x.expand(3, 4)  # y: [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]
```

### repeat()
- 会分配新内存，复制数据
- 可以沿着任意维度重复tensor

```python
x = torch.tensor([1, 2, 3])
y = x.repeat(2, 3)  # y: [[1, 2, 3, 1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3, 1, 2, 3]]
```

## 5. flatten() 和 ravel()

这两个函数用于将多维张量展平成一维。

### flatten()
将张量展平成一维

```python
x = torch.randn(2, 3, 4)
y = x.flatten()  # y.shape: (24,)
z = x.flatten(start_dim=1)  # z.shape: (2, 12)
```

### ravel()
功能类似于flatten()，但返回的可能是一个视图

```python
x = torch.randn(2, 3, 4)
y = x.ravel()  # y.shape: (24,)
```

## 6. stack() 和 cat()

这两个函数用于连接张量。

### stack()
沿着新维度连接张量

```python
x = torch.randn(3, 4)
y = torch.randn(3, 4)
z = torch.stack([x, y])  # z.shape: (2, 3, 4)
```

### cat()
沿着已存在的维度连接张量

```python
x = torch.randn(2, 3)
y = torch.randn(2, 5)
z = torch.cat([x, y], dim=1)  # z.shape: (2, 8)
```

## 7. split() 和 chunk()

这两个函数用于将张量分割成多个部分。

### split()
将张量分割成指定大小的块

```python
x = torch.randn(5, 10)
y = torch.split(x, 2, dim=0)  # 返回一个元组，包含3个tensor，形状分别为(2, 10), (2, 10), (1, 10)
```

### chunk()
将张量均匀分割成指定数量的块

```python
x = torch.randn(5, 10)
y = torch.chunk(x, 3, dim=1)  # 返回一个元组，包含3个tensor，形状分别为(5, 4), (5, 3), (5, 3)
```

## 8. broadcast_to()

将张量广播到指定的形状。

```python
x = torch.randn(3, 1)
y = torch.broadcast_to(x, (3, 5))  # y.shape: (3, 5)
```

## 9. narrow()

可以用来缩小张量的某个维度。

```python
x = torch.randn(3, 5)
y = x.narrow(1, 1, 2)  # 在第1维（列）上，从索引1开始，选择2个元素
```

## 10. unfold()

将张量的某个维度展开。

```python
x = torch.arange(1, 8)
y = x.unfold(0, 3, 1)  # 步长为1的滑动窗口操作，窗口大小为3
```

## 总结

PyTorch提供了丰富的维度变化操作，可以满足各种数据处理和模型构建的需求：
- 改变形状：view(), reshape()
- 添加/删除维度：squeeze(), unsqueeze()
- 交换维度：transpose(), permute()
- 扩展大小：expand(), repeat()
- 展平：flatten(), ravel()
- 连接：stack(), cat()
- 分割：split(), chunk()
- 广播：broadcast_to()
- 裁剪和展开：narrow(), unfold()

熟练掌握这些操作可以帮助你更高效地处理张量数据，构建复杂的神经网络模型。
