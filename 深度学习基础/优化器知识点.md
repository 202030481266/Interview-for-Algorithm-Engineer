# 目录

- [1.深度学习中有哪些经典的优化器？](#user-content-1深度学习中有哪些经典的优化器？)
- [2.深度学习中常用的学习率衰减策略有哪些？](#user-content-2深度学习中常用的学习率衰减策略有哪些？)
- [3.什么是方向导数？](#user-content-3什么是方向导数?)
- [4.什么是梯度](#user-content-4什么是梯度？)
- [5.为什么沿梯度方向是函数变化最快的方向？](#user-content-5为什么沿梯度方向是函数变化最快的方向？)

<h1 id="1深度学习中有哪些经典的优化器？">1.深度学习中有哪些经典的优化器？</h1>

### SGD（随机梯度下降）

随机梯度下降的优化算法在科研和工业界是很常用的。

<font color=DeepSkyBlue>很多理论和工程问题都能转化成对目标函数进行最小化的数学问题。</font>

举个例子：梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式奔向最低的位置。

SGD的公式：

![](https://img-blog.csdnimg.cn/20200802220806705.png#pic_center)

动量（Momentum）公式：

![](https://img-blog.csdnimg.cn/20200802220955480.png#pic_center)

基本的mini-batch SGD优化算法在深度学习取得很多不错的成绩。然而也存在一些问题需解决：

1. 选择恰当的初始学习率很困难。
2. 学习率调整策略受限于预先指定的调整规则。
3. 相同的学习率被应用于各个参数。
4. 高度非凸的误差函数的优化过程，如何避免陷入大量的局部次优解或鞍点。

### AdaGrad（自适应梯度）

AdaGrad优化算法（Adaptive Gradient，自适应梯度），它能够对每个不同的参数调整不同的学习率，对频繁变化的参数以更小的步长进行更新，而稀疏的参数以更大的步长进行更新。

AdaGrad公式：

![](https://img-blog.csdnimg.cn/20200802222758468.png#pic_center)

![](https://img-blog.csdnimg.cn/20200802222812287.png#pic_center)


$g_{t,i}$ 表示t时刻的 $\theta_{i}$ 梯度。

$G_{t,ii}$ 表示t时刻参数 $\theta_{i}$ 的梯度平方和。

与SGD的核心区别在于计算更新步长时，增加了分母：<font color=DeepSkyBlue>梯度平方累积和的平方根</font>。此项能够累积各个参数 $\theta_{i}$ 的历史梯度平方，频繁更新的梯度，则累积的分母逐渐偏大，那么更新的步长相对就会变小，而稀疏的梯度，则导致累积的分母项中对应值比较小，那么更新的步长则相对比较大。

AdaGrad能够自动为不同参数适应不同的学习率（平方根的分母项相当于对学习率α进进行了自动调整，然后再乘以本次梯度），大多数的框架实现采用默认学习率α=0.01即可完成比较好的收敛。

**优势：** 在数据分布稀疏的场景，能更好利用稀疏梯度的信息，比标准的SGD算法更有效地收敛。

**缺点：** 主要缺陷来自分母项的对梯度平方不断累积，随时间的增加，分母项越来越大，最终导致学习率收缩到太小无法进行有效更新。

### RMSProp
RMSProp结合梯度平方的指数移动平均数来调节学习率的变化。能够在不稳定的目标函数情况下进行很好地收敛。

计算t时刻的梯度：

![](https://img-blog.csdnimg.cn/20200802224130311.png#pic_center)

计算梯度平方的指数移动平均数（Exponential Moving Average）， $\gamma$ 是遗忘因子（或称为指数衰减率），依据经验，默认设置为0.9。

![](https://img-blog.csdnimg.cn/20200802224414890.png#pic_center)

梯度更新的时候，与AdaGrad类似，只是更新的梯度平方的期望（指数移动均值），其中 $\varepsilon = 10^{-8}$ ，避免除数为0。默认学习率 $\alpha = 0.001$ 。

![](https://img-blog.csdnimg.cn/20200802224711856.png#pic_center)

**优势：** 能够克服AdaGrad梯度急剧减小的问题，在很多应用中都展示出优秀的学习率自适应能力。尤其在不稳定(Non-Stationary)的目标函数下，比基本的SGD、Momentum、AdaGrad表现更良好。

### Adam

Adam优化器结合了AdaGrad和RMSProp两种优化算法的优点。对梯度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑，计算出更新步长。

**Adam的优势：**

1. 实现简单，计算高效，对内存需求少。
2. 参数的更新不受梯度的伸缩变换影响。
3. 超参数具有很好的解释性，且通常无需调整或仅需很少的微调。
4. 更新的步长能够被限制在大致的范围内（初始学习率）。
5. 能自然地实现步长退火过程（自动调整学习率）。
6. 很适合应用于大规模的数据及参数的场景。
7. 适用于不稳定目标函数。
8. 适用于梯度稀疏或梯度存在很大噪声的问题。

**Adam的实现原理：**

![](https://img-blog.csdnimg.cn/20200802230838428.png)

计算t时刻的梯度：

![](https://img-blog.csdnimg.cn/20200802230926200.png#pic_center)

然后计算梯度的指数移动平均数， $m_{0}$ 初始化为0。

类似于Momentum算法，综合考虑之前累积的梯度动量。

$\beta_{1}$ 系数为指数衰减率，控制动量和当前梯度的权重分配，通常取接近于1的值。默认为0.9。

![](https://img-blog.csdnimg.cn/20200802232541831.png#pic_center)

接着，计算梯度平方的指数移动平均数， $v_{0}$ 初始化为0。

$\beta_{2}$ 系数为指数衰减率，控制之前的梯度平方的影响情况。默认为0.999。

类似于RMSProp算法，对梯度平方进行加权均值。

![](https://img-blog.csdnimg.cn/20200802233221851.png#pic_center)

由于 $m_{0}$ 初始化为0，会导致$m_{t}$偏向于0，尤其在训练初期阶段。

所以，此处需要对梯度均值$m_{t}$进行偏差纠正，降低偏差对训练初期的影响。

![](https://img-blog.csdnimg.cn/2020080223350261.png#pic_center)

同时 $v_{0}$ 也要进行偏差纠正：

![](https://img-blog.csdnimg.cn/20200802233536671.png#pic_center)

最后总的公式如下所示：

![](https://img-blog.csdnimg.cn/20200802233611955.png#pic_center)

其中默认学习率 $\alpha = 0.001$ ， $\varepsilon = 10^{-8}$ 避免除数变为0。

从表达式中可以看出，对更新的步长计算，能够从梯度均值和梯度平方两个角度进行自适应地调节，而不是直接由当前梯度决定。

**Adam的不足：**

虽然Adam算法目前成为主流的优化算法，不过在很多领域里（如计算机视觉的图像识别、NLP中的机器翻译）的最佳成果仍然是使用带动量（Momentum）的SGD来获取到的。

<h1 id="2深度学习中常用的学习率衰减策略有哪些？">2.深度学习中常用的学习率衰减策略有哪些？</h1>

在AI领域中，合适的学习率调整策略对于模型的训练效果和收敛速度至关重要。

学习率衰减（或调整）是用来在训练过程中逐步减小学习率的方法，目的是帮助模型更好地收敛，避免训练过程中的振荡或不稳定。

以下是几种常用的学习率衰减方法：

### 1. 固定学习率衰减
这是最简单的衰减方法之一，其中学习率按预定的固定间隔和固定因子进行减少。例如，每过10个epoch将学习率乘以0.1。

### 2. 指数衰减
在指数衰减模式下，学习率按照指数函数逐步减小，通常定义为：
$\text{Learning Rate} = \text{Initial Learning Rate} \times e^{-\text{decay rate} \times \text{epoch}}$
其中，decay rate是一个预先设定的常数，epoch是当前的训练轮数。

### 3. 时间基衰减
时间基衰减与指数衰减类似，但学习率的衰减与训练的具体时间（通常是epoch数）更直接相关：
$\text{Learning Rate} = \frac{\text{Initial Learning Rate}}{1 + \text{decay rate} \times \text{epoch}}$

### 4. 阶梯衰减
在阶梯衰减中，学习率在一系列预设的epoch（如每20个epoch）后显著下降。这种衰减通常是手动设置的，非常直观，允许模型在较高的学习率下快速学习，并在训练后期通过较低的学习率细化模型。

### 5. 余弦衰减
余弦衰减是一种较新的策略，它模仿了余弦函数的形状来调整学习率，从初始学习率逐渐减小到接近零的值。这种方法在一些周期性或重启的训练策略中特别有效，可以帮助模型跳出局部最小值。

### 6. 适应性学习率方法
这包括像Adam和RMSprop这样的优化算法，这些算法本身就能够调整每个参数的学习率，通常基于历史梯度的平方（用于归一化梯度）。这些方法自动调整学习率，无需显式的衰减策略。

### 7. 热身和重启
- **学习率热身**：在训练初期，学习率从一个较低的值逐渐增加到设定的初始学习率。这有助于模型在训练初期稳定下来，防止初始学习率过高导致的不稳定。
- **周期性重启**：学习率按周期性地增加和减少，每次“重启”都从较高的学习率开始，然后再次衰减。这种方法有助于模型跳出局部最小值，寻找更好的全局最小值。

这些学习率调整方法可以单独使用，也可以结合使用，以达到最佳的训练效果。选择哪种衰减策略取决于具体任务、模型的复杂性以及训练数据的特点。在实际应用中，通常需要通过多次实验来确定最合适的学习率调整策略。

<h1 id="3什么是方向导数">3.什么是方向导数</h1>

- 导数是二维平面中，曲线上某一点沿着x轴方向变化的速率
- 偏导数是在三维空间中，曲面上某一点沿着x轴方向或y轴方向变化的速率
- 方向导数是在三维空间中，曲面上某一点沿着任一方向的变化率

定理：如果函数$f(x,y)$ 在点 $P_{0}\left(x_{0}, y_{0}\right)$可微分,那么函数在该点沿任一方向$l$的方向导数存在,且有$\frac{\partial f}{\partial \vec{l}}=\frac{\partial f}{\partial x} \cdot \cos \alpha+\frac{\partial f}{\partial y} \cdot \cos \beta$其中$\vec{l}=\{\cos \alpha, \cos \beta\}$

<h1 id="4什么是梯度">4.什么是梯度</h1>

设函数$z=f(x,y)$在平面区域$D$内具有一阶连续偏导数，对于区域$D$中的每一个点都可以确定一个向量$f_{x}(x, y) \vec{i}+f_{y}(x, y) \vec{j}$成为函数在该点的梯度，记为：
$\operatorname{gradf}(\mathrm{x}, \mathrm{y})=\nabla f(x, y)=\left\{\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right\}=f_{x}(x, y) \vec{i}+f_{y}(x, y) \vec{j}$

<h1 id="5为什么沿梯度方向是函数变化最快的方向？">5.为什么沿梯度方向是函数变化最快的方向？</h1>

梯度下降法，是机器学习、深度学习中比较核心也是较为常用的优化算法，但为什么沿梯度方向，函数变化最快？方向导数与梯度有什么关系？

设$\vec{e}=\{\cos \alpha, \cos \beta\}$是方向$l$上的单位向量，则$\frac{\partial f}{\partial \vec{l}}=\frac{\partial f}{\partial x} \cdot \cos \alpha+\frac{\partial f}{\partial y} \cdot \cos \beta=\left\{\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right\}\{\cos \alpha, \cos \beta\}=\operatorname{grad} f(x, y) \cdot \vec{e}$
$=|\operatorname{gradf}(x, y)|\cdot\cos\theta$，其中$\theta$为$\operatorname{gradf}(\mathrm{x}, \mathrm{y})$与$\vec{e}$的夹角

1. 当$\theta =0$时，即$\vec{e}$沿着梯度方向，方向导数取得最大值
2. 当$\theta = \pi/2$时，方向导数为0
3. 当$\theta =\pi$时，即$\vec{e}$沿着梯度反方向，方向导数取得最小值
## 综上
当方向导数大于0时，方向导数的大小用来描述函数上升的速率快慢，方向导数越大，表明函数上升越快，当方向导数小于0时，方向导数的大小用来描述函数下降的速率快慢，方向导数越小，表明函数下降越快，即$\vec{e}$沿着梯度反方向，方向导数取得最小值，此时函数下降速率最快。
