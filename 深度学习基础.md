# <center>----【深度学习基础】----</center>

## 【一】卷积有什么特点？

卷积主要有**三大特点**：

1. <font color=DeepSkyBlue>局部连接</font>。比起全连接，局部连接会大大减少网络的参数。在二维图像中，局部像素的关联性很强，设计局部连接保证了卷积网络对图像局部特征的强响应能力。

2. <font color=DeepSkyBlue>权值共享</font>。参数共享也能减少整体参数量，增强了网络训练的效率。一个卷积核的参数权重被整张图片共享，不会因为图像内位置的不同而改变卷积核内的参数权重。

3. <font color=DeepSkyBlue>下采样</font>。下采样能逐渐降低图像分辨率，实现了数据的降维，并使浅层的局部特征组合成为深层的特征。下采样还能使计算资源耗费变少，加速模型训练，也能有效控制过拟合。

## 【二】不同层次的卷积都提取什么类型的特征？

1. 浅层卷积 $\rightarrow$ 提取边缘特征

2. 中层卷积 $\rightarrow$ 提取局部特征

3. 深层卷积 $\rightarrow$ 提取全局特征

## 【三】卷积核大小如何选取？

最常用的是$3\times3$大小的卷积核，两个$3 \times 3$卷积核和一个$5 \times 5$卷积核的感受野相同，但是减少了参数量和计算量，加快了模型训练。与此同时由于卷积核的增加，模型的非线性表达能力大大增强。

![](https://files.mdnice.com/user/33499/bedb2e10-0899-4577-b94c-8d83212bb8c4.png)


不过大卷积核（$7 \times 7，9 \times 9$）也有使用的空间，在GAN，图像超分辨率，图像融合等领域依然有较多的应用，大家可按需切入感兴趣的领域查看相关论文。

## 【四】卷积感受野的相关概念

目标检测和目标跟踪很多模型都会用到RPN层，anchor是RPN层的基础，而感受野（receptive field，RF）是anchor的基础。

**感受野的作用：**

1. 一般来说感受野越大越好，比如分类任务中最后卷积层的感受野要大于输入图像。

2. 感受野足够大时，被忽略的信息就较少。

3. 目标检测任务中设置anchor要对齐感受野，anchor太大或者偏离感受野会对性能产生一定的影响。

感受野计算：


![](https://files.mdnice.com/user/33499/b3f35f69-fc9a-4311-9aa7-a084baa3d9d3.png)

增大感受野的方法：

1. 使用空洞卷积

2. 使用池化层

3. 增大卷积核

## 【五】网络每一层是否只能用一种尺寸的卷积核？

常规的神经网络一般每层仅用一个尺寸的卷积核，但同一层的特征图可以分别<font color=DeepSkyBlue>使用多个不同尺寸的卷积核，以获得不同尺度的特征</font>，再把这些特征结合起来，得到的特征往往比使用单一尺寸卷积核的要好，如GoogLeNet 、Inception系列的网络，均是每层使用了多个不同的卷积核结构。如下图所示，输入的特征图在同一层分别经过$1\times 1$，$3\times3$ 和$5\times5$三种不同尺寸的卷积核，再将各自的特征图进行整合，得到的新特征可以看作不同感受野提取的特征组合，相比于单一尺寸卷积核会有更强的表达能力。

![](https://files.mdnice.com/user/33499/255d01e9-1255-427b-9d85-600860d61d13.png)

## 【六】$1*1$ 卷积的作用？

$1 * 1$卷积的作用主要有以下几点：

1. 实现特征信息的交互与整合。

2. 对特征图通道数进行升维和降维，降维时可以减少参数量。

3. $1*1$卷积+ 激活函数 $\rightarrow$ 增加非线性，提升网络表达能力。


![升维与降维](https://files.mdnice.com/user/33499/2d53bb9a-32e5-4876-80a9-c4f1e1907721.png)


![1 * 1卷积在GoogLeNet中的应用](https://files.mdnice.com/user/33499/92cf4bfe-dcd5-4d2e-a976-36dcee368a7f.png)

$1 * 1$卷积首发于NIN（Network in Network），后续也在GoogLeNet和ResNet等网络中使用。感兴趣的朋友可追踪这些论文研读细节。

## 【七】转置卷积的作用？

转置卷积通过训练过程学习到最优的上采样方式，来代替传统的插值上采样方法，以提升图像分割，图像融合，GAN等特定任务的性能。

转置卷积并不是卷积的反向操作，从信息论的角度看，卷积运算是不可逆的。转置卷积可以将输出的特征图尺寸恢复卷积前的特征图尺寸，但不恢复原始数值。

**转置卷积的计算公式：**

我们设卷积核尺寸为$K\times K$，输入特征图为$i \times i$。

（1）当$stride = 1，padding = 0$时：

![](https://files.mdnice.com/user/33499/df26b9a5-8875-4ccc-96ef-bfb523942e9e.gif)

输入特征图在进行转置卷积操作时相当于进行了$padding = K - 1$的填充，接着再进行正常卷积转置之后的标准卷积运算。

输出特征图的尺寸 = $i + (K - 1)$

（2）当$stride > 1，padding = 0$时：

![](https://files.mdnice.com/user/33499/da594d05-2e5f-46c9-b2c3-81bd31f8961f.gif)

输入特征图在进行转置卷积操作时相当于进行了$padding = K - 1$的填充，相邻元素间的空洞大小为$stride - 1$，接着再进行正常卷积转置之后的标准卷积运算。

输出特征图的尺寸 = $stride * (i - 1) + K$

## 【八】空洞卷积的作用？

空洞卷积的作用<font color=DeepSkyBlue>是在不进行池化操作损失信息的情况下，增大感受野，让每个卷积输出都包含较大范围的信息</font>。

空洞卷积有一个参数可以设置dilation rate，其在卷积核中填充dilation rate个0，因此，当设置不同dilation rate时，感受野就会不一样，也获取了多尺度信息。

![](https://files.mdnice.com/user/33499/03827ca6-6abb-4565-a924-91983bc5611d.png)

(a) 图对应3x3的1-dilated conv，和普通的卷积操作一样。(b)图对应$3\times3$的2-dilated conv，实际的卷积kernel size还是$3\times3$，但是空洞为$1$，也就是对于一个$7\times7$的图像patch，只有$9$个红色的点和$3\times3$的kernel发生卷积操作，其余的点的权重为$0$。(c)图是4-dilated conv操作。

## 【九】全连接层的作用？

全连接层将卷积学习到的高维特征映射到label空间，可以作为整个网络的<font color=DeepSkyBlue>分类器</font>模块。

虽然全连接层参数存在冗余的情况，但是在模型进行迁移学习时，其能保持较大的模型capacity。

目前很多模型使用全局平均池化（GAP）取代全连接层以减小模型参数，并且依然能达到SOTA的性能。

## 【十】CNN中池化的作用？

池化层的作用是<font color=DeepSkyBlue>对感受野内的特征进行选择，提取区域内最具代表性的特征，能够有效地减少输出特征数量，进而减少模型参数量</font>。按操作类型通常分为最大池化(Max Pooling)、平均池化(Average Pooling)和求和池化(Sum Pooling)，它们分别提取感受野内最大、平均与总和的特征值作为输出，最常用的是最大池化和平均池化。

## 【十一】有哪些方法能提升CNN模型的泛化能力？

1. 采集更多数据：数据决定算法的上限。

2. 优化数据分布：数据类别均衡。

3. 选用合适的目标函数。

4. 设计合适的网络结构。

5. 数据增强。

6. 权值正则化。

7. 使用合适的优化器等。

## 【十二】BN层面试高频问题大汇总

<font color=DeepSkyBlue>BN层解决了什么问题？</font>

统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如transfer learning/domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。对于神经网络的各层输出，由于它们经过了层内卷积操作，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，但是它们所能代表的label仍然是不变的，这便符合了covariate shift的定义。

因为神经网络在做非线性变换前的激活输入值随着网络深度加深，其分布逐渐发生偏移或者变动（即上述的covariate shift）。之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（比如sigmoid），所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。而BN就是通过一定的正则化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，避免因为激活函数导致的梯度弥散问题。所以与其说BN的作用是缓解covariate shift，也可以说BN可缓解梯度弥散问题。

<font color=DeepSkyBlue>BN的公式</font>

![](https://files.mdnice.com/user/33499/41b0c64d-97db-4ca7-b116-3f4b5402f48f.png)

其中scale和shift是两个可学的参数，因为减去均值除方差未必是最好的分布。比如数据本身就很不对称，或者激活函数未必是对方差为1的数据有最好的效果。所以要加入缩放及平移变量来完善数据分布以达到比较好的效果。

<font color=DeepSkyBlue>BN层训练和测试的不同</font>

在训练阶段，BN层是对每个batch的训练数据进行标准化，即用每一批数据的均值和方差。（每一批数据的方差和标准差不同）

而在测试阶段，我们一般只输入一个测试样本，并没有batch的概念。因此这个时候用的均值和方差是整个数据集训练后的均值和方差，可以通过滑动平均法求得：

![](https://files.mdnice.com/user/33499/a1659e39-7ce1-484c-9b65-a777814b23ae.png)

上面式子简单理解就是：对于均值来说直接计算所有batch $u$值的平均值；然后对于标准偏差采用每个batch $σ_B$的无偏估计。

在测试时，BN使用的公式是：
  
![](https://files.mdnice.com/user/33499/e21b91a4-63f5-4259-b9e2-55fdf337e111.png)

<font color=DeepSkyBlue>BN训练时为什么不用整个训练集的均值和方差？</font>
  
因为用整个训练集的均值和方差容易过拟合，对于BN，其实就是对每一batch数据标准化到一个相同的分布，而不同batch数据的均值和方差会有一定的差别，而不是固定的值，这个差别能够增加模型的鲁棒性，也会在一定程度上减少过拟合。

<font color=DeepSkyBlue>BN层用在哪里？</font>
  
在CNN中，BN层应该用在非线性激活函数前面。由于神经网络隐藏层的输入是上一层非线性激活函数的输出，在训练初期其分布还在剧烈改变，此时约束其一阶矩和二阶矩无法很好地缓解 Covariate Shift；而BN的分布更接近正态分布，限制其一阶矩和二阶矩能使输入到激活函数的值分布更加稳定。

<font color=DeepSkyBlue>BN层的参数量</font>
  
我们知道$γ$和$β$是需要学习的参数，而BN的本质就是利用优化学习改变方差和均值的大小。在CNN中，因为网络的特征是对应到一整张特征图上的，所以做BN的时候也是以特征图为单位而不是按照各个维度。比如在某一层，特征图数量为$c$，那么做BN的参数量为$c * 2$。

<font color=DeepSkyBlue>BN的优缺点</font>

**优点：**

1. 可以选择较大的初始学习率。因为这个算法收敛很快。

2. 可以不用dropout，L2正则化。

3. 不需要使用局部响应归一化。
 
4. 可以把数据集彻底打乱。

5. 模型更加健壮。

**缺点：**

1. Batch Normalization非常依赖Batch的大小，当Batch值很小时，计算的均值和方差不稳定。

2. 所以BN不适用于以下几个场景：小Batch，RNN等。

## 【十三】什么是转置卷积的棋盘效应？

![](https://files.mdnice.com/user/33499/34a6b1e9-a4dd-4bc6-8979-492773be51f7.png)

造成棋盘效应的原因是转置卷积的不均匀重叠（uneven overlap）。这种重叠会造成图像中某个部位的颜色比其他部位更深。

在下图展示了棋盘效应的形成过程，深色部分代表了不均匀重叠：

![棋盘效应](https://files.mdnice.com/user/33499/c4a37f1c-8843-4425-ba64-01b3a9db2499.png)

接下来我们将卷积步长改为2，可以看到输出图像上的所有像素从输入图像中接收到同样多的信息，它们都从输入图像中接收到一个像素的信息，这样就不存在转置卷带来的重叠区域。

![](https://files.mdnice.com/user/33499/797f9ae7-6b34-438a-b67e-355066183421.png)

我们也可以直接进行插值Resize操作，然后再进行卷积操作来消除棋盘效应。这种方式在超分辨率重建场景中比较常见。例如使用双线性插值和近邻插值等方法来进行上采样。

## 【十四】Instance Normalization的作用？

Instance Normalization（IN）和Batch Normalization（BN）一样，也是Normalization的一种方法，<font color=DeepSkyBlue>只是IN是作用于单张图片，而BN作用于一个Batch</font>。

BN对Batch中的每一张图片的同一个通道一起进行Normalization操作，而IN是指单张图片的单个通道单独进行Normalization操作。如下图所示，其中C代表通道数，N代表图片数量（Batch）。

![](https://img-blog.csdnimg.cn/20201127225740900.png)

IN适用于生成模型中，比如图片风格迁移。因为图片生成的结果主要依赖于某个图像实例，所以对整个Batch进行Normalization操作并不适合图像风格化的任务，在风格迁移中使用IN不仅可以加速模型收敛，并且可以保持每个图像实例之间的独立性。

下面是IN的公式：

![](https://img-blog.csdnimg.cn/20201127231032309.png)

其中t代表图片的index，i代表的是feature map的index。

## 【十五】什么是有效感受野？

感受野的相关知识在之前的文章[【三年面试五年模拟】算法工程师的独孤九剑秘籍（前六式汇总篇）](https://mp.weixin.qq.com/s?__biz=Mzg4NDYwOTUwNA==&mid=2247484988&idx=1&sn=5ca80f281478d589359b7275267342f6&chksm=cfb4deb3f8c357a5d8ebecf142972f8349d85d5360038380cfbe2559185eccee47ac7c9360b3&token=758604668&lang=zh_CN#rd)中介绍过。

我们接着再看看有效感受野(effective receptive field, ERF)的相关知识。

一般而言，feature map上有效感受野要小于实际感受野。其有效性，以中心点为基准，类似高斯分布向边缘递减。

总的来说，感受野主要描述feature map中的最大信息量，有效感受野则主要描述信息的有效性。

## 【十六】全局池化的作用？

全局池化主要包括全局平均池化和全局最大池化。

![全局最大池化](https://files.mdnice.com/user/33499/4a9a663c-49ba-4259-b4cf-5838ae1ff781.png)

![全局平均池化](https://files.mdnice.com/user/33499/f92a2877-ec95-485b-8882-ed6845cef9fd.png)

接下来，Rocky以全局平均池化为例，讲述其如何在深度学习网络中发挥作用。

刚才已经讲过，全局平均池化就是对最后一层卷积的特征图，每个通道求整个特征图的均值。如下图所示：

![全局平均池化](https://img-blog.csdnimg.cn/20200312000813310.png)

一般网络的最后会再接几个全连接层，但全局池化后的feature map相当于一像素，所以最后的全连接其实就成了一个加权相加的操作。这种结构比起直接的全连接更加直观，参数量大大幅下降，并且泛化性能更好：

![](https://img-blog.csdnimg.cn/2020031200241849.png)

全局池化的作用：

1. 代替全连接层，降低参数量。
2. 减少过拟合，增加泛化能力。

## 【十七】深度学习中有哪些经典的优化器？

### SGD（随机梯度下降）

随机梯度下降的优化算法在科研和工业界是很常用的。

<font color=DeepSkyBlue>很多理论和工程问题都能转化成对目标函数进行最小化的数学问题。</font>

举个例子：梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式奔向最低的位置。

SGD的公式：

![](https://img-blog.csdnimg.cn/20200802220806705.png#pic_center)

动量（Momentum）公式：

![](https://img-blog.csdnimg.cn/20200802220955480.png#pic_center)

基本的mini-batch SGD优化算法在深度学习取得很多不错的成绩。然而也存在一些问题需解决：

1. 选择恰当的初始学习率很困难。
2. 学习率调整策略受限于预先指定的调整规则。
3. 相同的学习率被应用于各个参数。
4. 高度非凸的误差函数的优化过程，如何避免陷入大量的局部次优解或鞍点。

### AdaGrad（自适应梯度）

AdaGrad优化算法（Adaptive Gradient，自适应梯度），它能够对每个不同的参数调整不同的学习率，对频繁变化的参数以更小的步长进行更新，而稀疏的参数以更大的步长进行更新。

AdaGrad公式：

![](https://img-blog.csdnimg.cn/20200802222758468.png#pic_center)

![](https://img-blog.csdnimg.cn/20200802222812287.png#pic_center)


$g_{t,i}$表示t时刻的$\theta_{i}$梯度。

$G_{t,ii}$表示t时刻参数$\theta_{i}$的梯度平方和。

与SGD的核心区别在于计算更新步长时，增加了分母：<font color=DeepSkyBlue>梯度平方累积和的平方根</font>。此项能够累积各个参数$\theta_{i}$的历史梯度平方，频繁更新的梯度，则累积的分母逐渐偏大，那么更新的步长相对就会变小，而稀疏的梯度，则导致累积的分母项中对应值比较小，那么更新的步长则相对比较大。

AdaGrad能够自动为不同参数适应不同的学习率（平方根的分母项相当于对学习率α进进行了自动调整，然后再乘以本次梯度），大多数的框架实现采用默认学习率α=0.01即可完成比较好的收敛。

**优势：** 在数据分布稀疏的场景，能更好利用稀疏梯度的信息，比标准的SGD算法更有效地收敛。

**缺点：** 主要缺陷来自分母项的对梯度平方不断累积，随时间的增加，分母项越来越大，最终导致学习率收缩到太小无法进行有效更新。

### RMSProp
RMSProp结合梯度平方的指数移动平均数来调节学习率的变化。能够在不稳定的目标函数情况下进行很好地收敛。

计算t时刻的梯度：

![](https://img-blog.csdnimg.cn/20200802224130311.png#pic_center)

计算梯度平方的指数移动平均数（Exponential Moving Average），$\gamma$是遗忘因子（或称为指数衰减率），依据经验，默认设置为0.9。

![](https://img-blog.csdnimg.cn/20200802224414890.png#pic_center)

梯度更新的时候，与AdaGrad类似，只是更新的梯度平方的期望（指数移动均值），其中$\varepsilon = 10^{-8}$，避免除数为0。默认学习率$\alpha = 0.001$。

![](https://img-blog.csdnimg.cn/20200802224711856.png#pic_center)

**优势：** 能够克服AdaGrad梯度急剧减小的问题，在很多应用中都展示出优秀的学习率自适应能力。尤其在不稳定(Non-Stationary)的目标函数下，比基本的SGD、Momentum、AdaGrad表现更良好。

### Adam

Adam优化器结合了AdaGrad和RMSProp两种优化算法的优点。对梯度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑，计算出更新步长。

**Adam的优势：**

1. 实现简单，计算高效，对内存需求少。
2. 参数的更新不受梯度的伸缩变换影响。
3. 超参数具有很好的解释性，且通常无需调整或仅需很少的微调。
4. 更新的步长能够被限制在大致的范围内（初始学习率）。
5. 能自然地实现步长退火过程（自动调整学习率）。
6. 很适合应用于大规模的数据及参数的场景。
7. 适用于不稳定目标函数。
8. 适用于梯度稀疏或梯度存在很大噪声的问题。

**Adam的实现原理：**

![](https://img-blog.csdnimg.cn/20200802230838428.png)

计算t时刻的梯度：

![](https://img-blog.csdnimg.cn/20200802230926200.png#pic_center)

然后计算梯度的指数移动平均数，$m_{0}$初始化为0。

类似于Momentum算法，综合考虑之前累积的梯度动量。

$\beta_{1}$系数为指数衰减率，控制动量和当前梯度的权重分配，通常取接近于1的值。默认为0.9。

![](https://img-blog.csdnimg.cn/20200802232541831.png#pic_center)

接着，计算梯度平方的指数移动平均数，$v_{0}$初始化为0。

$\beta_{2}$系数为指数衰减率，控制之前的梯度平方的影响情况。默认为0.999。

类似于RMSProp算法，对梯度平方进行加权均值。

![](https://img-blog.csdnimg.cn/20200802233221851.png#pic_center)

由于$m_{0}$初始化为0，会导致$m_{t}$偏向于0，尤其在训练初期阶段。

所以，此处需要对梯度均值$m_{t}$进行偏差纠正，降低偏差对训练初期的影响。

![](https://img-blog.csdnimg.cn/2020080223350261.png#pic_center)

同时$v_{0}$也要进行偏差纠正：

![](https://img-blog.csdnimg.cn/20200802233536671.png#pic_center)

最后总的公式如下所示：

![](https://img-blog.csdnimg.cn/20200802233611955.png#pic_center)

其中默认学习率$\alpha = 0.001$，$\varepsilon = 10^{-8}$避免除数变为0。

从表达式中可以看出，对更新的步长计算，能够从梯度均值和梯度平方两个角度进行自适应地调节，而不是直接由当前梯度决定。

**Adam的不足：**

虽然Adam算法目前成为主流的优化算法，不过在很多领域里（如计算机视觉的图像识别、NLP中的机器翻译）的最佳成果仍然是使用带动量（Momentum）的SGD来获取到的。

## 【十八】有哪些提高GAN训练稳定性的Tricks？

### 1.输入Normalize

 1. 将输入图片Normalize到	$[-1，1]$之间。
 2. 生成器最后一层的输出使用Tanh激活函数。

Normalize非常重要，没有处理过的图片是没办法收敛的。图片Normalize一种简单的方法是（images-127.5）/127.5，然后送到判别器去训练。同理生成的图片也要经过判别器，即生成器的输出也是-1到1之间，所以使用Tanh激活函数更加合适。

### 2.替换原始的GAN损失函数和标签反转

1. 原始GAN损失函数会出现训练早期梯度消失和Mode collapse（模型崩溃）问题。可以使用Earth Mover distance（推土机距离）来优化。

2. 实际工程中用反转标签来训练生成器更加方便，即把生成的图片当成real的标签来训练，把真实的图片当成fake来训练。

### 3.使用具有球形结构的随机噪声$Z$作为输入

1. 不要使用均匀分布进行采样

![](https://img-blog.csdnimg.cn/202003111920127.png)

2. 使用高斯分布进行采样
![](https://img-blog.csdnimg.cn/20200311192036539.png)

### 4.使用BatchNorm

1. 一个mini-batch中必须只有real数据或者fake数据，不要把他们混在一起训练。
2. 如果能用BatchNorm就用BatchNorm，如果不能用则用instance normalization。

![](https://img-blog.csdnimg.cn/20200311192617441.png)

### 5.避免使用ReLU，MaxPool等操作引入稀疏梯度

1. GAN的稳定性会因为引入稀疏梯度受到很大影响。
2. 最好使用类LeakyReLU的激活函数。（D和G中都使用）
3. 对于下采样，最好使用：Average Pooling或者卷积+stride。
4. 对于上采样，最好使用：PixelShuffle或者转置卷积+stride。

最好去掉整个Pooling逻辑，因为使用Pooling会损失信息，这对于GAN训练没有益处。

### 6.使用Soft和Noisy的标签

1. Soft Label，即使用$[0.7-1.2]$和$[0-0.3]$两个区间的随机值来代替正样本和负样本的Hard Label。
2. 可以在训练时对标签加一些噪声，比如随机翻转部分样本的标签。

### 7.使用Adam优化器

1. Adam优化器对于GAN来说非常有用。
2. 在生成器中使用Adam，在判别器中使用SGD。

### 8.追踪训练失败的信号

1. 判别器的损失=0说明模型训练失败。
2. 如果生成器的损失稳步下降，说明判别器没有起作用。

### 9.在输入端适当添加噪声

1. 在判别器的输入中加入一些人工噪声。
2. 在生成器的每层中都加入高斯噪声。

### 10.生成器和判别器差异化训练

1. 多训练判别器，尤其是加了噪声的时候。

### 11.Two Timescale Update Rule (TTUR)

对判别器和生成器使用不同的学习速度。使用较低的学习率更新生成器，判别器使用较高的学习率进行更新。

### 12.Gradient Penalty （梯度惩罚）

使用梯度惩罚机制可以极大增强 GAN 的稳定性，尽可能减少mode collapse问题的产生。

### 13.Spectral Normalization（谱归一化）

Spectral normalization可以用在判别器的weight normalization技术，可以确保判别器是K-Lipschitz连续的。

### 14.使用多个GAN结构

可以使用多个GAN/多生成器/多判别器结构来让GAN训练更稳定，提升整体效果，解决更难的问题。

## 【十九】深度学习炼丹可以调节的一些超参数？

1. 预处理（数据尺寸，数据Normalization）
2. Batch-Size
3. 学习率
4. 优化器
5. 损失函数
6. 激活函数
7. Epoch
8. 权重初始化
9. NAS网络架构搜索

## 【二十】滑动平均的相关概念

滑动平均（exponential moving average），或者叫做指数加权平均（exponentially weighted moving avergae），可以用来估计变量的局部均值，<font color=DeepSkyBlue>使得变量的更新与一段时间内的历史取值有关</font>。

变量$v$在$t$时刻记为$v_{t}$，$\theta_{t}$为变量$v$在$t$时刻训练后的取值，当不使用滑动平均模型时$v_{t} = \theta_{t}$，在使用滑动平均模型后，$v_{t}$的更新公式如下：

![](https://img-blog.csdnimg.cn/20200805140509325.png#pic_center)

上式中，$\beta\epsilon[0,1)$。$\beta = 0$相当于没有使用滑动平均。

$t$时刻变量$v$的滑动平均值大致等于过去$1/(1-\beta)$个时刻$\theta$值的平均。并使用bias correction将$v_{t}$除以$(1 - \beta^{t})$修正对均值的估计。

加入Bias correction后，$v_{t}$和$v_{biased_{t}}$的更新公式如下：

![](https://img-blog.csdnimg.cn/20200805140434908.png#pic_center)

当$t$越大，$1 - \beta^{t}$越接近1，则公式（1）和（2）得到的结果（$v_{t}$和$v_{biased_{1}}$）将越来越接近。

当$\beta$越大时，滑动平均得到的值越和$\theta$的历史值相关。如果$\beta = 0.9$，则大致等于过去10个$\theta$值的平均；如果$\beta = 0.99$，则大致等于过去100个$\theta$值的平均。

下图代表不同方式计算权重的结果：

![](https://img-blog.csdnimg.cn/20200805141002734.png)

![](https://img-blog.csdnimg.cn/20200805141448823.png)

如上图所示，滑动平均可以看作是变量的过去一段时间取值的均值，<font color=DeepSkyBlue>相比对变量直接赋值而言，滑动平均得到的值在图像上更加平缓光滑，抖动性更小，不会因为某种次的异常取值而使得滑动平均值波动很大</font>。

**滑动平均的优势：** 占用内存少，不需要保存过去10个或者100个历史$\theta$值，就能够估计其均值。滑动平均虽然不如将历史值全保存下来计算均值准确，但后者占用更多内存，并且计算成本更高。

**为什么滑动平均在测试过程中被使用？**

<font color=DeepSkyBlue>滑动平均可以使模型在测试数据上更鲁棒（robust）</font>。

采用随机梯度下降算法训练神经网络时，使用滑动平均在很多应用中都可以在一定程度上提高最终模型在测试数据上的表现。

训练中对神经网络的权重 $weights$ 使用滑动平均，之后在测试过程中使用滑动平均后的 $weights$ 作为测试时的权重，这样在测试数据上效果更好。因为滑动平均后的 $weights$ 的更新更加平滑，对于随机梯度下降而言，更平滑的更新说明不会偏离最优点很远。比如假设decay=0.999，一个更直观的理解，在最后的1000次训练过程中，模型早已经训练完成，正处于抖动阶段，而滑动平均相当于将最后的1000次抖动进行了平均，这样得到的权重会更加鲁棒。

## 【二十一】Spectral Normalization的相关知识

Spectral Normalization是一种wegiht Normalization技术，和weight-clipping以及gradient penalty一样，也是让模型满足1-Lipschitz条件的方式之一。

<font color=DeepSkyBlue>Lipschitz（利普希茨）条件限制了函数变化的剧烈程度，即函数的梯度，来确保统计的有界性。因此函数更加平滑，在神经网络的优化过程中，参数变化也会更稳定，不容易出现梯度爆炸</font>。

Lipschitz条件的约束如下所示：

$$\frac{||f(x) -f(x^\prime)||_{2}}{||x - x^\prime||_{2}} \leqslant K$$

其中$K$代表一个常数，即利普希茨常数。若$K=1$，则是1-Lipschitz。

在GAN领域，Spectral Normalization有很多应用。在WGAN中，只有满足1-Lipschitz约束时，W距离才能转换成较好求解的对偶问题，使得WGAN更加从容的训练。

如果想让矩阵A映射：$R^{n}\to R^{m}$满足K-Lipschitz连续，K的最小值为$\sqrt{\lambda_{1}}$($\lambda_{1}$是$A_TA$的最大特征值)，那么要想让矩阵A满足1-Lipschitz连续，只需要在A的所有元素上同时除以$\sqrt{\lambda_{1}}$（Spectral norm）。

<font color=DeepSkyBlue>Spectral Normalization实际上在做的事，是将每层的参数矩阵除以自身的最大奇异值，本质上是一个逐层SVD的过程，但是真的去做SVD就太耗时了，所以采用幂迭代的方法求解</font>。过程如下图所示：

![幂迭代法流程](https://files.mdnice.com/user/33499/450732f1-84ad-4bef-a079-d15bb4c8646d.png)

得到谱范数$\sigma_l(W)$后，每个参数矩阵上的参数皆除以它，以达到Normalization的目的。

## 【二十二】激活函数的作用，常用的激活函数有哪些？

### 激活函数的作用

激活函数可以引入非线性因素，提升网络的学习表达能力。

### 常用的激活函数

**Sigmoid 激活函数**

 函数的定义为：
 
 $$f(x) = \frac{1}{1 + e^{-x}}$$

如下图所示，其值域为 $(0,1)$。也就是说，输入的每个神经元、节点都会被缩放到一个介于$0$和$1$之间的值。

当$x$大于零时输出结果会趋近于$1$，而当$x$小于零时，输出结果趋向于$0$，由于函数的特性，<font color=DeepSkyBlue>经常被用作二分类的输出端激活函数</font>。

![](https://files.mdnice.com/user/33499/ef73d59a-0208-4c8d-96ba-16df5e1631d8.png)

Sigmoid的导数:

$$f^{'}(x)=(\frac{1}{1+e^{-x}})^{'}=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))$$

当$x=0$时，$f(x)'=0.25$。

Sigmoid的优点:
1. 平滑
2. 易于求导
3. 可以作为概率，辅助解释模型的输出结果

Sigmoid的缺陷:

1. 当输入数据很大或者很小时，函数的梯度几乎接近于0，这对神经网络在反向传播中的学习非常不利。
2. Sigmoid函数的均值不是0，这使得神经网络的训练过程中只会产生全正或全负的反馈。
3. 导数值恒小于1，反向传播易导致梯度消失。

![Sigmoid导数示意图，两边梯度几乎为0](https://files.mdnice.com/user/33499/b6aa3d37-0d24-40c9-b802-27596d67ec39.png)

**Tanh激活函数**

Tanh函数的定义为：

$$f(x) = Tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

如下图所示，值域为 $(-1,1)$。

![](https://files.mdnice.com/user/33499/e1570597-a9c0-4546-937b-33d3a237fd7e.png)

Tanh的优势：

1. Tanh函数把数据压缩到-1到1的范围，解决了Sigmoid函数均值不为0的问题，所以在实践中通常Tanh函数比Sigmoid函数更容易收敛。在数学形式上其实Tanh只是对Sigmoid的一个缩放形式，公式为$tanh(x) = 2f(2x) -1$（$f(x)$是Sigmoid的函数）。
2. 平滑
3. 易于求导

Tanh的导数:

$$f^{'}(x)=(\frac{e^x - e^{-x}}{e^x + e^{-x}})^{'}=1-(tanh(x))^2$$

当$x=0$时，$f(x)'=1$。

由Tanh和Sigmoid的导数也可以看出Tanh导数更陡，收敛速度比Sigmoid快。

![Tanh导数示意图](https://files.mdnice.com/user/33499/6d4b89a4-2540-4965-bb22-f2c66c2f8245.png)

Tanh的缺点：

导数值恒小于1，反向传播易导致梯度消失。

**Relu激活函数**

Relu激活函数的定义为：

$$f(x) = max(0, x)$$  

如下图所示，值域为 $[0,+∞)$。

![](https://files.mdnice.com/user/33499/b8b05b3a-69d6-4f1d-9133-a188aafb8648.png)

ReLU的优势：

1. 计算公式非常简单，不像上面介绍的两个激活函数那样涉及成本更高的指数运算，大量节约了计算时间。
2. 在随机梯度下降中比Sigmoid和Tanh更加容易使得网络收敛。
3. ReLU进入负半区的时候，梯度为0，神经元此时会训练形成单侧抑制，产生稀疏性，能更好更快地提取稀疏特征。
4. Sigmoid和Tanh激活函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度消失，而ReLU函数大于0部分都为常数保持梯度不衰减，不会产生梯度消失现象。

<font color=DeepSkyBlue>稀疏</font>：在神经网络中，这意味着激活的矩阵含有许多0。这种稀疏性能让我们得到什么？这能提升时间和空间复杂度方面的效率，常数值所需空间更少，计算成本也更低。

ReLU的导数：

$$c(u)=\begin{cases} 0,x<0 \\ 1,x>0 \\ undefined,x=0\end{cases}$$

通常$x=0$时，给定其导数为$1$和$0$。

![ReLU的导数](https://files.mdnice.com/user/33499/09c86709-52f4-4278-8949-d83a41f9aebd.png)

ReLU的不足:

1. 训练中可能会导致出现某些神经元永远无法更新的情况。其中一种对ReLU函数的改进方式是LeakyReLU。
2. ReLU不能避免梯度爆炸问题。

**LeakyReLU激活函数** 

LeakyReLU激活函数定义为： 

$$f(x) =  \left\{
   \begin{aligned}
   ax, \quad x<0 \\
   x, \quad x\ge0
   \end{aligned}
   \right.$$

如下图所示（$a = 0.5$），值域为 $(-∞,+∞)$。 

![](https://files.mdnice.com/user/33499/d475ec3a-0f4d-4154-896a-278f0e87d39e.png)

LeakyReLU的优势:

该方法与ReLU不同的是在$x$小于0的时候取$f(x) = ax$，其中$a$是一个非常小的斜率（比如0.01）。这样的改进可以使得当$x$小于0的时候也不会导致反向传播时的梯度消失现象。

LeakyReLU的不足:

1. 无法避免梯度爆炸的问题。
2. 神经网络不学习$\alpha$值。
3. 在求导的时候，两部分都是线性的。

**SoftPlus激活函数**

SoftPlus激活函数的定义为：

$$f(x) = ln( 1 + e^x)$$

值域为 $(0,+∞)$。

函数图像如下:

![](https://files.mdnice.com/user/33499/bf513661-17d8-4197-87c9-5002f77d7c86.png)

可以把SoftPlus看作是ReLU的平滑。

**ELU激活函数**

ELU激活函数解决了ReLU的一些问题，同时也保留了一些好的方面。这种激活函数要选取一个$\alpha$值，其常见的取值是在0.1到0.3之间。

函数定义如下所示：

$$f(x) =  \left\{
   \begin{aligned}
   a(e^x -1), \quad x<0 \\
   x, \quad x\ge0
   \end{aligned}
   \right.$$

如果我们输入的$x$值大于$0$，则结果与ReLU一样，即$y$值等于$x$值；但如果输入的$x$值小于$0$，则我们会得到一个稍微小于$0$的值，所得到的$y$值取决于输入的$x$值，但还要兼顾参数$\alpha$——可以根据需要来调整这个参数。公式进一步引入了指数运算$e^x$，因此ELU的计算成本比ReLU高。

下面给出了$\alpha$值为0.2时的ELU函数图：

![ELU函数图](https://img-blog.csdnimg.cn/20200401154732541.png)

ELU的导数：

![ELU的导数公式](https://img-blog.csdnimg.cn/20200401155003365.png)

导数图如下所示：

![ELU的导数图](https://img-blog.csdnimg.cn/20200401155309599.png)

ELU的优势：

1. 能避免ReLU中一些神经元无法更新的情况。
2. 能得到负值输出。

ELU的不足：

1. 包含指数运算，计算时间长。
2. 无法避免梯度爆炸问题。
3. 神经网络无法学习$\alpha$值。

## 【二十三】反向传播算法（BP）的概念及简单推导

<font color=DeepSkyBlue>反向传播（Backpropagation，BP）算法是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见算法</font>。BP算法对网络中所有权重计算损失函数的梯度，并将梯度反馈给最优化方法，用来更新权值以最小化损失函数。<font color=DeepSkyBlue>该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数</font>。

接下来我们以全连接层，使用sigmoid激活函数，Softmax+MSE作为损失函数的神经网络为例，推导BP算法逻辑。由于篇幅限制，这里只进行简单推导，后续Rocky将专门写一篇PB算法完整推导流程，大家敬请期待。

首先，我们看看sigmoid激活函数的表达式及其导数：

$$sigmoid表达式：\sigma(x) = \frac{1}{1+e^{-x}}$$
$$sigmoid导数：\frac{d}{dx}\sigma(x) = \sigma(x) - \sigma(x)^2 = \sigma(1- \sigma)$$

可以看到sigmoid激活函数的导数最终可以表达为输出值的简单运算。

我们再看MSE损失函数的表达式及其导数：

$$MSE损失函数的表达式：L = \frac{1}{2}\sum^{K}_{k=1}(y_k - o_k)^2$$

其中$y_k$代表ground truth（gt）值，$o_k$代表网络输出值。

$$MSE损失函数的偏导：\frac{\partial L}{\partial o_i} = (o_i - y_i)$$

由于偏导数中单且仅当$k = i$时才会起作用，故进行了简化。

接下来我们看看全连接层输出的梯度：

![](https://files.mdnice.com/user/33499/4f1b33bf-53c7-440e-811d-644c9956414a.png)

$$MSE损失函数的表达式：L = \frac{1}{2}\sum^{K}_{i=1}(o_i^1 - t_i)^2$$

$$MSE损失函数的偏导：\frac{\partial L}{\partial w_{jk}} = (o_k - t_k)o_k(1-o_k)x_j$$

我们用$\delta_k = (o_k - t_k)o_k(1-o_k)$，则能再次简化：

$$MSE损失函数的偏导：\frac{dL}{dw_{jk}} = \delta_kx_j$$

最后，我们看看那PB算法中每一层的偏导数：

![](https://files.mdnice.com/user/33499/182e5d6f-711b-496f-86af-c86a8f135623.png)

输出层：
$$\frac{\partial L}{\partial w_{jk}} = \delta_k^K o_j$$
$$\delta_k^K = (o_k - t_k)o_k(1-o_k)$$

倒数第二层：
$$\frac{\partial L}{\partial w_{ij}} = \delta_j^J o_i$$
$$\delta_j^J = o_j(1 - o_j) \sum_{k}\delta_k^Kw_{jk}$$

倒数第三层：
$$\frac{\partial L}{\partial w_{ni}} = \delta_i^I o_n$$
$$\delta_i^I = o_i(1 - o_i) \sum_{j}\delta_j^Jw_{ij}$$

像这样依次往回推导，再通过梯度下降算法迭代优化网络参数，即可走完PB算法逻辑。

## 【二十四】分组卷积的相关知识

分组卷积（Group Convolution）最早出现在AlexNet网络中，分组卷积被用来切分网络，使其能在多个GPU上并行运行。

![分组卷积和普通卷积的区别](https://img-blog.csdnimg.cn/20201104211649626.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JvY2t5NjY4OA==,size_16,color_FFFFFF,t_70#pic_center)

普通卷积进行运算的时候，如果输入feature map尺寸是$C\times H \times W$，卷积核有N个，那么输出的feature map与卷积核的数量相同也是N个，每个卷积核的尺寸为$C\times K \times K$，N个卷积核的总参数量为$N \times C \times K \times K$。

分组卷积的主要对输入的feature map进行分组，然后每组分别进行卷积。如果输入feature map尺寸是$C\times H \times W$，输出feature map的数量为$N$个，如果我们设定要分成G个group，则每组的输入feature map数量为$\frac{C}{G}$，则每组的输出feature map数量为$\frac{N}{G}$，每个卷积核的尺寸为$\frac{C}{G} \times K \times K$，卷积核的总数仍为N个，每组的卷积核数量为$\frac{N}{G}$，卷积核只与其同组的输入map进行卷积，卷积核的总参数量为$N \times \frac{C}{G} \times K \times K$，<font color=DeepSkyBlue>易得总的参数量减少为原来的$\frac{1}{G}$</font>。

**分组卷积的作用:**

1. 分组卷积可以减少参数量。
2. 分组卷积可以看成是稀疏操作，有时可以在较少参数量的情况下获得更好的效果（相当于正则化操作）。
3. 当分组数量等于输入feature map通道数量，输出feature map数量也等于输入feature map数量时，分组卷积就成了Depthwise卷积，可以使参数量进一步缩减。
